{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b99e3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel, AutoModelForTokenClassification\n",
    "\n",
    "from crf_layer import CRFLayer\n",
    "from multiLabelTokenClassfication import MultiLabelTokenClassification\n",
    "from stackedModel import MultiLabelStackedClassification\n",
    "\n",
    "from utils import extract_result_multilabel\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore',category=UserWarning,module='torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4af2aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(dict_path):\n",
    "    \"\"\"load_dict\"\"\"\n",
    "    vocab = {}\n",
    "    for line in open(dict_path, 'r', encoding='utf-8'):\n",
    "        value, key = line.strip('\\n').split('\\t')\n",
    "        vocab[key] = int(value)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "474ba276",
   "metadata": {},
   "outputs": [],
   "source": [
    "enum_role = \"环节\"\n",
    "max_seq_len = 512\n",
    "\n",
    "label_vocab = load_dict(dict_path='./dictionary/role_tag.dict')\n",
    "id2label = {val: key for key, val in label_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bedd926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(dataset):\n",
    "    \"\"\"data_process\"\"\"\n",
    "\n",
    "    def label_data(data, start, l, _type):\n",
    "        \"\"\"label_data\"\"\"\n",
    "        for i in range(start, start + l):\n",
    "            suffix = \"B-\" if i == start else \"I-\"\n",
    "            if isinstance(data[i], str):\n",
    "                data[i] = []\n",
    "            solt = \"{}{}\".format(suffix, _type)\n",
    "            if solt not in data[i]:\n",
    "                data[i].append(solt)\n",
    "        return data\n",
    "    \n",
    "    def replace_control_chars(str):\n",
    "        if str == '\\u200b' or str == '\\ufeff' or str == '\\ue601' or str == '\\u3000':\n",
    "            return '[UNK]'\n",
    "        else:\n",
    "            return str\n",
    "\n",
    "    output = []\n",
    "    for d_json in dataset:\n",
    "        _id = d_json[\"id\"]\n",
    "        text_a = [\n",
    "            \"，\" if t == \" \" or t == \"\\n\" or t == \"\\t\" else replace_control_chars(t)\n",
    "            for t in list(d_json[\"text\"].lower())\n",
    "        ]\n",
    "        if len(d_json.get(\"event_list\", [])) == 0:\n",
    "            continue\n",
    "        ### combine same event type\n",
    "        event_type_mapping = {}\n",
    "        for event in d_json.get(\"event_list\", []):\n",
    "            event_type = event['event_type']\n",
    "            trigger = event['trigger']\n",
    "            type_tuple = (event_type, trigger)\n",
    "            if type_tuple not in event_type_mapping:\n",
    "                event_type_mapping[type_tuple] = []\n",
    "            for argument in event[\"arguments\"]:\n",
    "                if argument not in event_type_mapping[type_tuple]:\n",
    "                    event_type_mapping[type_tuple].append(argument)\n",
    "\n",
    "        for type_tuple, arguments in event_type_mapping.items():\n",
    "            event_type = type_tuple[0]\n",
    "            trigger = type_tuple[1]\n",
    "            labels = [\"O\"] * len(text_a)\n",
    "            for arg in arguments:\n",
    "                role_type = arg[\"role\"]\n",
    "                if role_type == enum_role:\n",
    "                    continue\n",
    "                argument = arg[\"argument\"]\n",
    "                start = arg[\"argument_start_index\"]\n",
    "                labels = label_data(labels, start, len(argument), role_type)\n",
    "            text_trigger = [\n",
    "                \"，\" if t == \" \" or t == \"\\n\" or t == \"\\t\" else t\n",
    "                for t in list(event_type+f\"({trigger})：\".lower())\n",
    "            ]\n",
    "            trigger_label = [\"O\"] * len(text_trigger)\n",
    "            output.append({\n",
    "                \"tokens\": text_trigger+text_a, \"labels\": trigger_label+labels\n",
    "            })\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21aa23c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed = 42):\n",
    "    \"\"\"Set the seed for generating random numbers on all GPUs.\n",
    "\n",
    "    It's safe to call this function if CUDA is not available; in that case, it is silently ignored.\n",
    "\n",
    "    Args:\n",
    "        seed (int, optional): random numbers on all GPUs. Defaults to 42.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa8d34f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Tesla V100-PCIE-32GB\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "CUDA Device Count: 3\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    \n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', torch.cuda.memory_allocated(0)/1024**3, 'GB')\n",
    "    print('Cached:   ', torch.cuda.memory_reserved(0)/1024**3, 'GB')\n",
    "    \n",
    "    print('CUDA Device Count:', n_gpu)\n",
    "    \n",
    "set_seed(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87567be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    'ernie-base': AutoTokenizer.from_pretrained(\"nghuyong/ernie-1.0\"),\n",
    "    'roberta-chinese-base': AutoTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext\"),\n",
    "    'roberta-chinese-large': AutoTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext-large\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "870891b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaiduEventDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset_path, label_dict_path, ignore_index=-100):\n",
    "        self.label_vocab = load_dict(label_dict_path)\n",
    "        self.label_num = max(self.label_vocab.values()) + 1\n",
    "        self.examples = []\n",
    "        with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "            dataset = json.loads(f.read())\n",
    "            preprocess_dataset = data_process(dataset)\n",
    "            for d_json in tqdm(preprocess_dataset, total=len(preprocess_dataset)):\n",
    "                tokens = d_json['tokens']\n",
    "                b_input_ids, b_attention_masks, b_token_type_ids = [], [], []\n",
    "                for tokenizer in model_dict.values():\n",
    "                    PADDING = tokenizer.vocab[tokenizer.pad_token]\n",
    "                    SEP = tokenizer.vocab[tokenizer.sep_token]\n",
    "                    input_ids = tokenizer(tokens, is_split_into_words=True, add_special_tokens=True, max_length=max_seq_len, truncation=True)['input_ids']\n",
    "                    tokens_input = input_ids + [PADDING] * (max_seq_len - len(input_ids))\n",
    "                    attention_masks = self._get_attention_mask(input_ids, max_seq_len)\n",
    "                    token_type_ids = self._get_token_type_id(input_ids, max_seq_len, sep_token=SEP)\n",
    "                    b_input_ids.append(tokens_input)\n",
    "                    b_attention_masks.append(attention_masks)\n",
    "                    b_token_type_ids.append(token_type_ids)\n",
    "                example = {\n",
    "                    \"input_ids\": b_input_ids, \"attention_masks\": b_attention_masks,\n",
    "                    \"token_type_ids\": b_token_type_ids, \"seq_lens\": len(tokens)\n",
    "                }\n",
    "                if 'labels' in d_json:\n",
    "                    labels = d_json['labels']\n",
    "                    labels = labels[:(max_seq_len - 2)]\n",
    "                    encoded_label = [\"O\"] + labels + [\"O\"]\n",
    "                    encoded_label = self.to_one_hot_vector(encoded_label, max_seq_len - 2 - len(labels))\n",
    "                    example.update({\"encoded_label\": encoded_label})\n",
    "                self.examples.append(example)\n",
    "\n",
    "    def to_one_hot_vector(self, labels, zero_padding_len = 0):\n",
    "        \"\"\"Convert seq to one hot.\"\"\"\n",
    "        one_hot_vectors = []\n",
    "        for label in labels:\n",
    "            one_hot_vector = np.zeros(self.label_num)\n",
    "            if isinstance(label, str):\n",
    "                one_hot_vector[self.label_vocab.get(label, 0)] = 1\n",
    "            elif isinstance(label, list):\n",
    "                for l in label:\n",
    "                    one_hot_vector[self.label_vocab.get(l, 0)] = 1\n",
    "            one_hot_vectors.append(one_hot_vector)\n",
    "        for _ in range(zero_padding_len):\n",
    "            one_hot_vector = np.zeros(self.label_num)\n",
    "            one_hot_vectors.append(one_hot_vector)\n",
    "        return np.array(one_hot_vectors)\n",
    "\n",
    "    def _get_attention_mask(self, input_ids, max_seq_len):\n",
    "        \"\"\"Mask for padding.\"\"\"\n",
    "        if len(input_ids) > max_seq_len:\n",
    "            raise IndexError(\"Token length more than max seq length!\")\n",
    "        return [1] * len(input_ids) + [0] * (max_seq_len - len(input_ids))\n",
    "\n",
    "    def _get_token_type_id(self, input_ids, max_seq_len, sep_token):\n",
    "        \"\"\"Segments: 0 for the first sequence, 1 for the second.\"\"\"\n",
    "        if len(input_ids) > max_seq_len:\n",
    "            raise IndexError(\"Token length more than max seq length!\")\n",
    "        segments = []\n",
    "        current_segment_id = 0\n",
    "        for input_id in input_ids:\n",
    "            segments.append(current_segment_id)\n",
    "            if input_id == sep_token:\n",
    "                current_segment_id = 1\n",
    "        return segments + [0] * (max_seq_len - len(input_ids))        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, item_idx):\n",
    "        example = {\n",
    "            \"input_ids\": torch.tensor(self.examples[item_idx][\"input_ids\"]).long(),\n",
    "            \"attention_masks\": torch.tensor(self.examples[item_idx][\"attention_masks\"]),\n",
    "            \"token_type_ids\": torch.tensor(self.examples[item_idx][\"token_type_ids\"]),\n",
    "            \"seq_lens\": self.examples[item_idx][\"seq_lens\"]\n",
    "        }\n",
    "        if \"encoded_label\" in self.examples[item_idx]:\n",
    "            example.update({\"encoded_label\": torch.tensor(self.examples[item_idx][\"encoded_label\"], dtype=torch.float)})\n",
    "        return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c573ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11264/11264 [12:54<00:00, 14.53it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = BaiduEventDataset(dataset_path='./resources/duee_fin_train_preprocess.json', label_dict_path='./dictionary/role_tag.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5903c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1803/1803 [02:19<00:00, 12.96it/s]\n"
     ]
    }
   ],
   "source": [
    "dev_dataset = BaiduEventDataset(dataset_path='./resources/duee_fin_dev_preprocess.json', label_dict_path='./dictionary/role_tag.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9cb3ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   1,   53,  230,  ...,    0,    0,    0],\n",
       "         [ 101, 1062, 1385,  ...,    0,    0,    0],\n",
       "         [ 101, 1062, 1385,  ...,    0,    0,    0]]),\n",
       " 'attention_masks': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'seq_lens': 125,\n",
       " 'encoded_label': tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32097756",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    torch.load(f'./models/DuEE_fin/{model_name}/role-multilabel-trick1.bin').to(device)\n",
    "    for model_name in model_dict.keys()\n",
    "]\n",
    "stacked_model = MultiLabelStackedClassification(models=models, num_labels=len(label_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "569b24e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, eval_dataloader):\n",
    "    model.eval()\n",
    "    step = 0\n",
    "    eval_acc = 0.0\n",
    "    eval_f1 = 0.0\n",
    "    eval_precision = 0.0\n",
    "    eval_recall = 0.0\n",
    "    eval_loss = 0.0\n",
    "    for batch in eval_dataloader:\n",
    "        loss, logits = model(\n",
    "            input_ids=batch['input_ids'].to(device),\n",
    "            attention_mask=batch['attention_masks'].to(device),\n",
    "            token_type_ids=batch['token_type_ids'].to(device),\n",
    "            labels=batch['encoded_label'].to(device)\n",
    "        )\n",
    "        \n",
    "        if n_gpu > 1:\n",
    "            loss = loss.mean()\n",
    "        \n",
    "        eval_loss += loss.item()\n",
    "        pred_Y = (torch.sigmoid(logits).data > 0.5).cpu().numpy()\n",
    "        true_Y = batch['encoded_label'].cpu().numpy()\n",
    "        batch_size = true_Y.shape[0]\n",
    "        batch_precision, batch_recall, batch_f1 = 0.0, 0.0, 0.0\n",
    "        for text, t_ids, p_ids, seq_len in zip(batch[\"text\"], true_Y, pred_Y, batch['seq_lens']):\n",
    "            true_label, pred_label = [], []\n",
    "            for pid in p_ids[1: seq_len - 1]:\n",
    "                true_indices = np.argwhere(pid).flatten()\n",
    "                pred_label.append([id2label[true_index] for true_index in true_indices])\n",
    "            p_ret = extract_result_multilabel(text, pred_label)\n",
    "            pred_event_types = set([(p[\"type\"], ''.join(p[\"text\"]), p[\"start\"]) for p in p_ret])\n",
    "            for tid in t_ids[1: seq_len - 1]:\n",
    "                true_indices = np.argwhere(tid).flatten()\n",
    "                true_label.append([id2label[true_index] for true_index in true_indices])\n",
    "            t_ret = extract_result_multilabel(text, true_label)\n",
    "            true_event_types = set([(t[\"type\"], ''.join(t[\"text\"]), t[\"start\"]) for t in t_ret])\n",
    "            count_predict = len(list(pred_event_types))\n",
    "            count_true = len(list(true_event_types))\n",
    "            count_correct = len(list(pred_event_types & true_event_types))\n",
    "            p = count_correct / max(1, count_predict)  # precision\n",
    "            r = count_correct / max(1, count_true)  # recall\n",
    "            batch_precision += p\n",
    "            batch_recall += r\n",
    "            batch_f1 += 2 * r * p / max(1e-9, r + p) # f1 score\n",
    "        eval_acc += accuracy_score(pred_Y.flatten(), true_Y.flatten())\n",
    "        eval_precision += batch_precision / batch_size\n",
    "        eval_recall += batch_recall / batch_size\n",
    "        eval_f1 += batch_f1 / batch_size\n",
    "        step += 1\n",
    "    model.train()\n",
    "    return eval_loss/step, eval_acc/step, eval_precision/step, eval_recall/step, eval_f1/step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57b5dce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### train model\n",
    "\n",
    "def train(model, ds_train, ds_dev = None, n_epochs = 100, learning_rate = 1e-2, weight_decay = 0.01, batch_size = 1, eval_per_epoch = 2):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    train_sampler = RandomSampler(ds_train)\n",
    "    train_dataloader = DataLoader(ds_train, sampler=train_sampler, batch_size=batch_size)\n",
    "    \n",
    "    eval_sampler = SequentialSampler(ds_dev)\n",
    "    eval_dataloader = DataLoader(ds_dev, sampler=eval_sampler, batch_size=batch_size)\n",
    "    \n",
    "    decay_params = [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ]\n",
    "\n",
    "    if n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model, device_ids=[0, 1, 2])\n",
    "\n",
    "    optimizer_grouped_parameters = [{\n",
    "        \"params\": model.parameters(),\n",
    "        \"lr\": learning_rate, \n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"apply_decay_param_fun\": lambda x: x in decay_params\n",
    "    }]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters)\n",
    "#     scheduler = ReduceLROnPlateau(optimizer, \"min\")\n",
    "    \n",
    "    f1 = 0.0\n",
    "    acc = 0.0\n",
    "    precision = 0.0\n",
    "    recall = 0.0\n",
    "    tr_loss = 0.0\n",
    "    global_step = 0\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    postfix = {}\n",
    "    for epoch in range(0, n_epochs):\n",
    "        eval_flag = False\n",
    "        train_iterator = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{n_epochs}\")\n",
    "        for batch in train_iterator:\n",
    "            loss, logits = model(\n",
    "                input_ids=batch['input_ids'].to(device),\n",
    "                attention_mask=batch['attention_masks'].to(device),\n",
    "                token_type_ids=batch['token_type_ids'].to(device),\n",
    "                labels=batch['encoded_label'].to(device)\n",
    "            )\n",
    "            \n",
    "            if n_gpu > 1:\n",
    "                loss = loss.mean()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # scheduler.step(loss)\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            pred_Y = (torch.sigmoid(logits).data > 0.5).cpu().numpy()\n",
    "            true_Y = batch['encoded_label'].cpu().numpy()\n",
    "            batch_size = true_Y.shape[0]\n",
    "            batch_precision, batch_recall, batch_f1 = 0.0, 0.0, 0.0\n",
    "            for text, t_ids, p_ids, seq_len in zip(batch[\"text\"], true_Y, pred_Y, batch['seq_lens']):\n",
    "                true_label, pred_label = [], []\n",
    "                for pid in p_ids[1: seq_len - 1]:\n",
    "                    true_indices = np.argwhere(pid).flatten()\n",
    "                    pred_label.append([id2label[true_index] for true_index in true_indices])\n",
    "                p_ret = extract_result_multilabel(text, pred_label)\n",
    "                pred_event_types = set([(p[\"type\"], ''.join(p[\"text\"]), p[\"start\"]) for p in p_ret])\n",
    "                for tid in t_ids[1: seq_len - 1]:\n",
    "                    true_indices = np.argwhere(tid).flatten()\n",
    "                    true_label.append([id2label[true_index] for true_index in true_indices])\n",
    "                t_ret = extract_result_multilabel(text, true_label)\n",
    "                true_event_types = set([(t[\"type\"], ''.join(t[\"text\"]), t[\"start\"]) for t in t_ret])\n",
    "                count_predict = len(list(pred_event_types))\n",
    "                count_true = len(list(true_event_types))\n",
    "                count_correct = len(list(pred_event_types & true_event_types))\n",
    "                p = count_correct / max(1, count_predict)  # precision\n",
    "                r = count_correct / max(1, count_true)  # recall\n",
    "                batch_precision += p\n",
    "                batch_recall += r\n",
    "                batch_f1 += 2 * r * p / max(1e-9, r + p) # f1 score\n",
    "            acc += accuracy_score(pred_Y.flatten(), true_Y.flatten())\n",
    "            precision += batch_precision / batch_size\n",
    "            recall += batch_recall / batch_size\n",
    "            f1 += batch_f1 / batch_size\n",
    "            model.zero_grad()\n",
    "\n",
    "            postfix.update({\"Avg loss\": f\"{tr_loss / (global_step + 1):.5f}\", \"Avg acc score\": f\"{acc / (global_step + 1):.5f}\", \"Avg precision score\": f\"{precision / (global_step + 1):.5f}\", \"Avg recall score\": f\"{recall / (global_step + 1):.5f}\", \"Avg f1 score\": f\"{f1 / (global_step + 1):.5f}\"})\n",
    "            if (\n",
    "                not eval_flag\n",
    "                and (global_step + 1) % len(train_dataloader) == 0\n",
    "                and (epoch % eval_per_epoch) == 0\n",
    "            ):\n",
    "                if ds_dev is not None:\n",
    "                    eval_loss, eval_acc, eval_precision, eval_recall, eval_f1 = evaluate(model, eval_dataloader)\n",
    "                postfix.update({\"Avg eval loss\": f\"{eval_loss:.5f}\", \"Avg eval acc\": f\"{eval_acc:.5f}\", \"Avg eval precision\": f\"{eval_precision:.5f}\", \"Avg eval recall\": f\"{eval_recall:.5f}\", \"Avg eval f1\": f\"{eval_f1:.5f}\"})\n",
    "                eval_flag = True\n",
    "            train_iterator.set_postfix(postfix)\n",
    "            global_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "213f4b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 118/118 [18:40<00:00,  9.49s/it, Avg loss=0.19971, Avg acc score=0.99188, Avg precision score=0.99350, Avg recall score=0.98306, Avg f1 score=0.98120, Avg eval loss=0.12384, Avg eval acc=0.99949, Avg eval precision=0.98205, Avg eval recall=0.98672, Avg eval f1=0.98438]\n",
      "Epoch 2/20: 100%|██████████| 118/118 [15:50<00:00,  8.06s/it, Avg loss=0.14638, Avg acc score=0.99591, Avg precision score=0.99558, Avg recall score=0.99120, Avg f1 score=0.98985, Avg eval loss=0.12384, Avg eval acc=0.99949, Avg eval precision=0.98205, Avg eval recall=0.98672, Avg eval f1=0.98438]\n",
      "Epoch 3/20: 100%|██████████| 118/118 [18:09<00:00,  9.24s/it, Avg loss=0.11588, Avg acc score=0.99726, Avg precision score=0.99621, Avg recall score=0.99393, Avg f1 score=0.99271, Avg eval loss=0.04447, Avg eval acc=0.99949, Avg eval precision=0.98123, Avg eval recall=0.98728, Avg eval f1=0.98424]\n",
      "Epoch 4/20: 100%|██████████| 118/118 [15:49<00:00,  8.05s/it, Avg loss=0.09596, Avg acc score=0.99793, Avg precision score=0.99652, Avg recall score=0.99531, Avg f1 score=0.99414, Avg eval loss=0.04447, Avg eval acc=0.99949, Avg eval precision=0.98123, Avg eval recall=0.98728, Avg eval f1=0.98424]\n",
      "Epoch 5/20: 100%|██████████| 118/118 [18:16<00:00,  9.29s/it, Avg loss=0.08194, Avg acc score=0.99833, Avg precision score=0.99670, Avg recall score=0.99613, Avg f1 score=0.99500, Avg eval loss=0.02349, Avg eval acc=0.99949, Avg eval precision=0.98137, Avg eval recall=0.98712, Avg eval f1=0.98423]\n",
      "Epoch 6/20: 100%|██████████| 118/118 [15:52<00:00,  8.07s/it, Avg loss=0.07155, Avg acc score=0.99860, Avg precision score=0.99682, Avg recall score=0.99668, Avg f1 score=0.99557, Avg eval loss=0.02349, Avg eval acc=0.99949, Avg eval precision=0.98137, Avg eval recall=0.98712, Avg eval f1=0.98423]\n",
      "Epoch 7/20: 100%|██████████| 118/118 [18:17<00:00,  9.30s/it, Avg loss=0.06353, Avg acc score=0.99880, Avg precision score=0.99691, Avg recall score=0.99707, Avg f1 score=0.99598, Avg eval loss=0.01526, Avg eval acc=0.99949, Avg eval precision=0.98161, Avg eval recall=0.98689, Avg eval f1=0.98424]\n",
      "Epoch 8/20: 100%|██████████| 118/118 [15:55<00:00,  8.10s/it, Avg loss=0.05716, Avg acc score=0.99894, Avg precision score=0.99698, Avg recall score=0.99737, Avg f1 score=0.99628, Avg eval loss=0.01526, Avg eval acc=0.99949, Avg eval precision=0.98161, Avg eval recall=0.98689, Avg eval f1=0.98424]\n",
      "Epoch 9/20: 100%|██████████| 118/118 [18:16<00:00,  9.30s/it, Avg loss=0.05198, Avg acc score=0.99905, Avg precision score=0.99704, Avg recall score=0.99759, Avg f1 score=0.99653, Avg eval loss=0.01117, Avg eval acc=0.99949, Avg eval precision=0.98182, Avg eval recall=0.98673, Avg eval f1=0.98426]\n",
      "Epoch 10/20: 100%|██████████| 118/118 [15:52<00:00,  8.07s/it, Avg loss=0.04767, Avg acc score=0.99914, Avg precision score=0.99709, Avg recall score=0.99777, Avg f1 score=0.99672, Avg eval loss=0.01117, Avg eval acc=0.99949, Avg eval precision=0.98182, Avg eval recall=0.98673, Avg eval f1=0.98426]\n",
      "Epoch 11/20: 100%|██████████| 118/118 [18:15<00:00,  9.28s/it, Avg loss=0.04404, Avg acc score=0.99922, Avg precision score=0.99713, Avg recall score=0.99792, Avg f1 score=0.99688, Avg eval loss=0.00883, Avg eval acc=0.99949, Avg eval precision=0.98200, Avg eval recall=0.98652, Avg eval f1=0.98425]\n",
      "Epoch 12/20: 100%|██████████| 118/118 [15:49<00:00,  8.05s/it, Avg loss=0.04093, Avg acc score=0.99928, Avg precision score=0.99717, Avg recall score=0.99805, Avg f1 score=0.99701, Avg eval loss=0.00883, Avg eval acc=0.99949, Avg eval precision=0.98200, Avg eval recall=0.98652, Avg eval f1=0.98425]\n",
      "Epoch 13/20: 100%|██████████| 118/118 [18:19<00:00,  9.32s/it, Avg loss=0.03824, Avg acc score=0.99933, Avg precision score=0.99720, Avg recall score=0.99815, Avg f1 score=0.99713, Avg eval loss=0.00734, Avg eval acc=0.99949, Avg eval precision=0.98214, Avg eval recall=0.98641, Avg eval f1=0.98427]\n",
      "Epoch 14/20: 100%|██████████| 118/118 [15:51<00:00,  8.06s/it, Avg loss=0.03589, Avg acc score=0.99937, Avg precision score=0.99723, Avg recall score=0.99824, Avg f1 score=0.99723, Avg eval loss=0.00734, Avg eval acc=0.99949, Avg eval precision=0.98214, Avg eval recall=0.98641, Avg eval f1=0.98427]\n",
      "Epoch 15/20: 100%|██████████| 118/118 [18:19<00:00,  9.32s/it, Avg loss=0.03382, Avg acc score=0.99941, Avg precision score=0.99726, Avg recall score=0.99831, Avg f1 score=0.99731, Avg eval loss=0.00633, Avg eval acc=0.99949, Avg eval precision=0.98231, Avg eval recall=0.98628, Avg eval f1=0.98429]\n",
      "Epoch 16/20: 100%|██████████| 118/118 [15:48<00:00,  8.04s/it, Avg loss=0.03197, Avg acc score=0.99945, Avg precision score=0.99729, Avg recall score=0.99838, Avg f1 score=0.99739, Avg eval loss=0.00633, Avg eval acc=0.99949, Avg eval precision=0.98231, Avg eval recall=0.98628, Avg eval f1=0.98429]\n",
      "Epoch 17/20: 100%|██████████| 118/118 [18:17<00:00,  9.30s/it, Avg loss=0.03033, Avg acc score=0.99948, Avg precision score=0.99731, Avg recall score=0.99844, Avg f1 score=0.99746, Avg eval loss=0.00561, Avg eval acc=0.99949, Avg eval precision=0.98240, Avg eval recall=0.98623, Avg eval f1=0.98431]\n",
      "Epoch 18/20: 100%|██████████| 118/118 [15:54<00:00,  8.09s/it, Avg loss=0.02884, Avg acc score=0.99950, Avg precision score=0.99733, Avg recall score=0.99849, Avg f1 score=0.99752, Avg eval loss=0.00561, Avg eval acc=0.99949, Avg eval precision=0.98240, Avg eval recall=0.98623, Avg eval f1=0.98431]\n",
      "Epoch 19/20: 100%|██████████| 118/118 [18:13<00:00,  9.27s/it, Avg loss=0.02750, Avg acc score=0.99953, Avg precision score=0.99735, Avg recall score=0.99854, Avg f1 score=0.99757, Avg eval loss=0.00509, Avg eval acc=0.99949, Avg eval precision=0.98254, Avg eval recall=0.98608, Avg eval f1=0.98431]\n",
      "Epoch 20/20: 100%|██████████| 118/118 [15:50<00:00,  8.06s/it, Avg loss=0.02628, Avg acc score=0.99955, Avg precision score=0.99737, Avg recall score=0.99858, Avg f1 score=0.99762, Avg eval loss=0.00509, Avg eval acc=0.99949, Avg eval precision=0.98254, Avg eval recall=0.98608, Avg eval f1=0.98431]\n"
     ]
    }
   ],
   "source": [
    "train(stacked_model, train_dataset, ds_dev=dev_dataset, n_epochs=20, batch_size=32*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62fe2a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(stacked_model.state_dict(), './models/DuEE_fin/stacked/stacked_role-multilabel-trick1.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "458f733c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c25bbf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "069d4b83295243b0bedfebb442b10c5b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_c4fd9ccefe95434db2e7df930aaac118",
       "style": "IPY_MODEL_e1c3e571d2b54106bf20fac3e33b5012",
       "value": " 130/274 [02:12&lt;02:26,  1.02s/it, Avg loss=1.23, Avg acc score=0.92, Avg f1 score=0.92]"
      }
     },
     "60f889e1229a4154aa318d4c46d16fe2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "80336ec9357f4abd8c4c3ba264963949": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_8e3273f660b642669d155147b757f335",
        "IPY_MODEL_069d4b83295243b0bedfebb442b10c5b"
       ],
       "layout": "IPY_MODEL_f399e7b310cd4d1fb83bf06a20fc5d56"
      }
     },
     "8e3273f660b642669d155147b757f335": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "description": "Epoch 1/20:  47%",
       "layout": "IPY_MODEL_60f889e1229a4154aa318d4c46d16fe2",
       "max": 274,
       "style": "IPY_MODEL_c2f754305c5341929c539cca7dd9a9d2",
       "value": 130
      }
     },
     "c2f754305c5341929c539cca7dd9a9d2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "c4fd9ccefe95434db2e7df930aaac118": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e1c3e571d2b54106bf20fac3e33b5012": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "f399e7b310cd4d1fb83bf06a20fc5d56": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
