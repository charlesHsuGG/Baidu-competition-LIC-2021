{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e5446d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel\n",
    "\n",
    "from crf_layer import CRFLayer\n",
    "from multiLabelTokenClassfication import MultiLabelTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77f33187",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_model = \"nghuyong/ernie-1.0\"\n",
    "\n",
    "shema_path = './dictionary/event_schema.json'\n",
    "enerm_dict_path = './dictionary/enum_tag.dict'\n",
    "trigger_dict_path = './dictionary/trigger_tag.dict'\n",
    "role_dict_path = './dictionary/role_tag.dict'\n",
    "\n",
    "enerm_model_path = './models/DuEE_fin/ernie-base/enum.bin'\n",
    "tigger_model_path = './models/DuEE_fin/ernie-base/trigger-multilabel.bin'\n",
    "role_model_path = './models/DuEE_fin/ernie-base/role-multilabel.bin'\n",
    "\n",
    "duee_fin_dev_path = './resources/duee_fin_dev.json'\n",
    "duee_fin_dev_preprocess_path = './resources/duee_fin_dev_preprocess.json'\n",
    "\n",
    "enum_role = \"环节\"\n",
    "enum_event_type = \"公司上市\"\n",
    "max_seq_len = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfbd1a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b2db8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(dataset, model=\"trigger\"):\n",
    "    \"\"\"data_process\"\"\"\n",
    "    \n",
    "    def label_data(data, start, l, _type):\n",
    "        \"\"\"label_data\"\"\"\n",
    "        for i in range(start, start + l):\n",
    "            suffix = \"B-\" if i == start else \"I-\"\n",
    "            if isinstance(data[i], str):\n",
    "                data[i] = []\n",
    "            solt = \"{}{}\".format(suffix, _type)\n",
    "            if solt not in data[i]:\n",
    "                data[i].append(solt)\n",
    "        return data\n",
    "    \n",
    "    def remove_control_chars(str):\n",
    "        return str.lower().replace('\\u200b', '').replace('\\ufeff', '').replace('\\ue601', '').replace('\\u3000', '')\n",
    "\n",
    "    output = []\n",
    "    for d_json in dataset:\n",
    "        _id = d_json[\"id\"]\n",
    "        d_json[\"text\"] = remove_control_chars(d_json[\"text\"])\n",
    "        text_a = [\n",
    "            \"，\" if t == \" \" or t == \"\\n\" or t == \"\\t\" else str(t)\n",
    "            for t in list(d_json[\"text\"].lower())\n",
    "        ]\n",
    "        if model == \"trigger\":\n",
    "            labels = [\"O\"] * len(text_a)\n",
    "            for event in d_json.get(\"event_list\", []):\n",
    "                event_type = event[\"event_type\"]\n",
    "                start = event[\"trigger_start_index\"]\n",
    "                trigger = event[\"trigger\"]\n",
    "                labels = label_data(labels, start, len(trigger), event_type)\n",
    "            output.append({\n",
    "                \"id\": d_json[\"id\"],\n",
    "                \"sent_id\": d_json[\"sent_id\"],\n",
    "                \"text\": d_json[\"text\"],\n",
    "                \"tokens\": text_a,\n",
    "                \"labels\": labels\n",
    "            })\n",
    "        elif model == \"role\":\n",
    "            labels = [\"O\"] * len(text_a)\n",
    "            for event in d_json.get(\"event_list\", []):\n",
    "                for arg in event[\"arguments\"]:\n",
    "                    role_type = arg[\"role\"]\n",
    "                    if role_type == enum_role:\n",
    "                        continue\n",
    "                    argument = arg[\"argument\"]\n",
    "                    start = arg[\"argument_start_index\"]\n",
    "                    labels = label_data(labels, start, len(argument), role_type)\n",
    "            output.append({\n",
    "                \"id\": d_json[\"id\"],\n",
    "                \"sent_id\": d_json[\"sent_id\"],\n",
    "                \"text\": d_json[\"text\"],\n",
    "                \"tokens\": text_a,\n",
    "                \"labels\": labels\n",
    "            })\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7433cdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(duee_fin_dev_preprocess_path, 'r', encoding='utf-8') as f:\n",
    "    dataset = json.loads(f.read())\n",
    "    preprocess_dataset = data_process(dataset, model='role')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85b282d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enum_data_process(dataset):\n",
    "    \"\"\"enum_data_process\"\"\"\n",
    "    output = []\n",
    "    for d_json in dataset:\n",
    "        text = d_json[\"text\"].lower().replace(\"\\t\", \" \")\n",
    "#         if len(d_json.get(\"event_list\", [])) == 0:\n",
    "#             continue\n",
    "        label = 'ABS'\n",
    "        for event in d_json.get(\"event_list\", []):\n",
    "            if event[\"event_type\"] != \"公司上市\":\n",
    "                continue\n",
    "            for argument in event[\"arguments\"]:\n",
    "                role_type = argument[\"role\"]\n",
    "                if role_type == enum_role:\n",
    "                    label = argument[\"argument\"]\n",
    "        output.append({\n",
    "            \"id\": d_json[\"id\"],\n",
    "            \"sent_id\": d_json[\"sent_id\"],\n",
    "            \"text\": text,\n",
    "            \"label\": label\n",
    "        })\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab43c840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(dict_path):\n",
    "    \"\"\"load_dict\"\"\"\n",
    "    vocab = {}\n",
    "    for line in open(dict_path, 'r', encoding='utf-8'):\n",
    "        value, key = line.strip('\\n').split('\\t')\n",
    "        vocab[key] = int(value)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec0d3b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_enum_vocab = load_dict(dict_path=enerm_dict_path)\n",
    "id2enumlabel = {val: key for key, val in label_enum_vocab.items()}\n",
    "label_trigger_vocab = load_dict(dict_path=trigger_dict_path)\n",
    "id2triggerlabel = {val: key for key, val in label_trigger_vocab.items()}\n",
    "label_role_vocab = load_dict(dict_path=role_dict_path)\n",
    "id2rolelabel = {val: key for key, val in label_role_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01b5ccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PADDING = tokenizer.vocab[tokenizer.pad_token]\n",
    "SEP = tokenizer.vocab[tokenizer.sep_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b609aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaiduEnermDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset_path, label_dict_path):\n",
    "        self.label_vocab = load_dict(label_dict_path)\n",
    "        self.label_num = max(self.label_vocab.values()) + 1\n",
    "        self.examples = []\n",
    "        with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "            dataset = json.loads(f.read())\n",
    "            preprocess_dataset = enum_data_process(dataset)\n",
    "            for d_json in preprocess_dataset:\n",
    "                text = d_json['text']\n",
    "                input_ids = tokenizer(text, is_split_into_words=False, add_special_tokens=True, max_length=max_seq_len, truncation=True)['input_ids']\n",
    "                tokens_input = input_ids + [PADDING] * (max_seq_len - len(input_ids))\n",
    "                attention_masks = self._get_attention_mask(input_ids, max_seq_len)\n",
    "                token_type_ids = self._get_token_type_id(input_ids, max_seq_len)\n",
    "                example = {\n",
    "                    \"input_ids\": tokens_input, \"attention_masks\": attention_masks, \"token_type_ids\": token_type_ids\n",
    "                }\n",
    "                example.update(d_json)\n",
    "                if 'label' in d_json:\n",
    "                    label = d_json['label']\n",
    "                    example.update({\"encoded_label\": self.label_vocab.get(label, -1)})\n",
    "                self.examples.append(example)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, item_idx):\n",
    "        example = {\n",
    "            \"id\": self.examples[item_idx][\"id\"],\n",
    "            \"sent_id\": self.examples[item_idx][\"sent_id\"],\n",
    "            \"text\": self.examples[item_idx][\"text\"],\n",
    "            \"input_ids\": torch.tensor(self.examples[item_idx][\"input_ids\"]).long(),\n",
    "            \"attention_masks\": torch.tensor(self.examples[item_idx][\"attention_masks\"]),\n",
    "            \"token_type_ids\": torch.tensor(self.examples[item_idx][\"token_type_ids\"])\n",
    "        }\n",
    "        if \"encoded_label\" in self.examples[item_idx]:\n",
    "            example.update({\"encoded_label\": torch.tensor(self.examples[item_idx][\"encoded_label\"], dtype=torch.long)})\n",
    "        return example\n",
    "\n",
    "    def _get_attention_mask(self, input_ids, max_seq_len):\n",
    "        \"\"\"Mask for padding.\"\"\"\n",
    "        if len(input_ids) > max_seq_len:\n",
    "            raise IndexError(\"Token length more than max seq length!\")\n",
    "        return [1] * len(input_ids) + [0] * (max_seq_len - len(input_ids))\n",
    "\n",
    "    def _get_token_type_id(self, input_ids, max_seq_len):\n",
    "        \"\"\"Segments: 0 for the first sequence, 1 for the second.\"\"\"\n",
    "        if len(input_ids) > max_seq_len:\n",
    "            raise IndexError(\"Token length more than max seq length!\")\n",
    "        segments = []\n",
    "        current_segment_id = 0\n",
    "        for input_id in input_ids:\n",
    "            segments.append(current_segment_id)\n",
    "            if input_id == SEP:\n",
    "                current_segment_id = 1\n",
    "        return segments + [0] * (max_seq_len - len(input_ids)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7ff1c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaiduEventDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset_path, label_dict_path, model=\"trigger\", ignore_index=-100):\n",
    "        self.label_vocab = load_dict(label_dict_path)\n",
    "        self.label_num = max(self.label_vocab.values()) + 1\n",
    "        self.examples = []\n",
    "        with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "            dataset = json.loads(f.read())\n",
    "            preprocess_dataset = data_process(dataset, model=model)\n",
    "            for d_json in preprocess_dataset:\n",
    "                tokens = d_json['tokens']\n",
    "                input_ids = tokenizer(tokens, is_split_into_words=True, add_special_tokens=True, max_length=max_seq_len, truncation=True)['input_ids']\n",
    "                tokens_input = input_ids + [PADDING] * (max_seq_len - len(input_ids))\n",
    "                attention_masks = self._get_attention_mask(input_ids, max_seq_len)\n",
    "                token_type_ids = self._get_token_type_id(input_ids, max_seq_len)\n",
    "                example = {\n",
    "                    \"input_ids\": tokens_input, \"attention_masks\": attention_masks, \"token_type_ids\": token_type_ids,\n",
    "                    \"seq_lens\": len(tokens)\n",
    "                }\n",
    "                example.update(d_json)\n",
    "                if 'labels' in d_json:\n",
    "                    labels = d_json['labels']\n",
    "                    labels = labels[:(max_seq_len - 2)]\n",
    "                    encoded_label = [\"O\"] + labels + [\"O\"]\n",
    "                    encoded_label = self.to_one_hot_vector(encoded_label, max_seq_len - 2 - len(labels))\n",
    "                    example.update({\"encoded_label\": encoded_label})\n",
    "                self.examples.append(example)\n",
    "                \n",
    "    def to_one_hot_vector(self, labels, zero_padding_len = 0):\n",
    "        \"\"\"Convert seq to one hot.\"\"\"\n",
    "        one_hot_vectors = []\n",
    "        for label in labels:\n",
    "            one_hot_vector = np.zeros(self.label_num)\n",
    "            if isinstance(label, str):\n",
    "                one_hot_vector[self.label_vocab.get(label, 0)] = 1\n",
    "            elif isinstance(label, list):\n",
    "                for l in label:\n",
    "                    one_hot_vector[self.label_vocab.get(l, 0)] = 1\n",
    "            one_hot_vectors.append(one_hot_vector)\n",
    "        for _ in range(zero_padding_len):\n",
    "            one_hot_vector = np.zeros(self.label_num)\n",
    "            one_hot_vectors.append(one_hot_vector)\n",
    "        return np.array(one_hot_vectors)\n",
    "\n",
    "    def _get_attention_mask(self, input_ids, max_seq_len):\n",
    "        \"\"\"Mask for padding.\"\"\"\n",
    "        if len(input_ids) > max_seq_len:\n",
    "            raise IndexError(\"Token length more than max seq length!\")\n",
    "        return [1] * len(input_ids) + [0] * (max_seq_len - len(input_ids))\n",
    "\n",
    "    def _get_token_type_id(self, input_ids, max_seq_len):\n",
    "        \"\"\"Segments: 0 for the first sequence, 1 for the second.\"\"\"\n",
    "        if len(input_ids) > max_seq_len:\n",
    "            raise IndexError(\"Token length more than max seq length!\")\n",
    "        segments = []\n",
    "        current_segment_id = 0\n",
    "        for input_id in input_ids:\n",
    "            segments.append(current_segment_id)\n",
    "            if input_id == SEP:\n",
    "                current_segment_id = 1\n",
    "        return segments + [0] * (max_seq_len - len(input_ids))        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, item_idx):\n",
    "        example = {\n",
    "            \"id\": self.examples[item_idx][\"id\"],\n",
    "            \"sent_id\": self.examples[item_idx][\"sent_id\"],\n",
    "            \"text\": self.examples[item_idx][\"text\"],\n",
    "            \"input_ids\": torch.tensor(self.examples[item_idx][\"input_ids\"]).long(),\n",
    "            \"attention_masks\": torch.tensor(self.examples[item_idx][\"attention_masks\"]),\n",
    "            \"token_type_ids\": torch.tensor(self.examples[item_idx][\"token_type_ids\"]),\n",
    "            \"seq_lens\": self.examples[item_idx][\"seq_lens\"]\n",
    "        }\n",
    "        if \"encoded_label\" in self.examples[item_idx]:\n",
    "            example.update({\"encoded_label\": torch.tensor(self.examples[item_idx][\"encoded_label\"], dtype=torch.long)})\n",
    "        return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd9bf5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_enerm_dataset = BaiduEnermDataset(dataset_path=duee_fin_dev_preprocess_path, label_dict_path=enerm_dict_path)\n",
    "dev_trigger_dataset = BaiduEventDataset(dataset_path=duee_fin_dev_preprocess_path, label_dict_path=trigger_dict_path, model=\"trigger\")\n",
    "dev_role_dataset = BaiduEventDataset(dataset_path=duee_fin_dev_preprocess_path, label_dict_path=role_dict_path, model=\"role\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3e91ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2995"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_enerm_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45593b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2995"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_trigger_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "567b9a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2995"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_role_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0599b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed = 42):\n",
    "    \"\"\"Set the seed for generating random numbers on all GPUs.\n",
    "\n",
    "    It's safe to call this function if CUDA is not available; in that case, it is silently ignored.\n",
    "\n",
    "    Args:\n",
    "        seed (int, optional): random numbers on all GPUs. Defaults to 42.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f796fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "\n",
      "Tesla V100-PCIE-32GB\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "CUDA Device Count: 3\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    \n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', torch.cuda.memory_allocated(0)/1024**3, 'GB')\n",
    "    print('Cached:   ', torch.cuda.memory_reserved(0)/1024**3, 'GB')\n",
    "    \n",
    "    print('CUDA Device Count:', n_gpu)\n",
    "    \n",
    "set_seed(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32467dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_enerm(model, test_dataloader):\n",
    "    from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
    "    \n",
    "    model.eval()\n",
    "    step = 0\n",
    "    eval_acc = 0.0\n",
    "    eval_f1 = 0.0\n",
    "    eval_precision = 0.0\n",
    "    eval_recall = 0.0\n",
    "    results = []\n",
    "    test_iterator = tqdm(test_dataloader)\n",
    "    for batch in test_iterator:\n",
    "        outputs = model(\n",
    "            input_ids=batch['input_ids'].to(device),\n",
    "            attention_mask=batch['attention_masks'].to(device),\n",
    "            token_type_ids=batch['token_type_ids'].to(device)\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "\n",
    "        probs = F.softmax(logits, dim=1).cpu()\n",
    "        probs_ids = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "        true_label = batch.get(\"encoded_label\", None).cpu().numpy()\n",
    "        pred_label = copy.deepcopy(probs_ids)\n",
    "        ignore_indices = np.argwhere(true_label == -1)\n",
    "        pred_label[ignore_indices] = -1\n",
    "        probs = probs.numpy()\n",
    "        eval_acc += accuracy_score(true_label.flatten(), pred_label.flatten())\n",
    "        eval_precision += precision_score(true_label.flatten(), pred_label.flatten(), average=\"macro\", zero_division=1)\n",
    "        eval_recall += recall_score(true_label.flatten(), pred_label.flatten(), average=\"macro\", zero_division=1)\n",
    "        eval_f1 += f1_score(true_label.flatten(), pred_label.flatten(), average=\"macro\")\n",
    "        for id_, sent_id, text, prob_one, p_id in zip(batch['id'], batch['sent_id'], batch['text'], probs.tolist(), probs_ids.tolist()):\n",
    "            label_probs = {}\n",
    "            for idx, p in enumerate(prob_one):\n",
    "                label_probs[id2enumlabel[idx]] = p\n",
    "            results.append({\"id\": id_, \"sent_id\": sent_id, \"text\": text, \"pred\":{\"probs\": label_probs, \"label\": id2enumlabel[p_id]}})\n",
    "        step += 1\n",
    "    print({\"Avg eval acc\": f\"{eval_acc/step:.2f}\", \"Avg eval precision\": f\"{eval_precision/step:.2f}\", \"Avg eval recall\": f\"{eval_recall/step:.2f}\", \"Avg eval f1\": f\"{eval_f1/step:.2f}\"})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81aa096e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a303c24e76b44e183fe6146dbdbd56f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'Avg eval acc': '0.98', 'Avg eval precision': '0.66', 'Avg eval recall': '0.72', 'Avg eval f1': '0.61'}\n"
     ]
    }
   ],
   "source": [
    "enum_model = torch.load(enerm_model_path).to(device)\n",
    "\n",
    "test_enerm_sampler = SequentialSampler(dev_enerm_dataset)\n",
    "test_enerm_dataloader = DataLoader(dev_enerm_dataset, sampler=test_enerm_sampler, batch_size = 512)\n",
    "    \n",
    "sentences_enum_data = test_enerm(enum_model, test_enerm_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "268c4b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_trigger(model, test_dataloader):\n",
    "    from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
    "\n",
    "    model.eval()\n",
    "    step = 0\n",
    "    eval_acc = 0.0\n",
    "    eval_f1 = 0.0\n",
    "    eval_precision = 0.0\n",
    "    eval_recall = 0.0\n",
    "    results = []\n",
    "    test_iterator = tqdm(test_dataloader)\n",
    "    for batch in test_iterator:\n",
    "        _, logits = model(\n",
    "            input_ids=batch['input_ids'].to(device),\n",
    "            attention_mask=batch['attention_masks'].to(device),\n",
    "            token_type_ids=batch['token_type_ids'].to(device)\n",
    "        )\n",
    "\n",
    "        probs = torch.sigmoid(logits).data.cpu()\n",
    "        probs_ids = (probs > 0.5).numpy()\n",
    "        probs = probs.numpy()\n",
    "        true_Y = batch['encoded_label'].cpu().numpy()\n",
    "        pred, true = [], []\n",
    "        for t_ids, p_ids, seq_len in zip(true_Y, probs_ids, batch['seq_lens']):\n",
    "            pred.extend(p_ids[1: seq_len - 1])\n",
    "            true.extend(t_ids[1: seq_len - 1])\n",
    "        pred_Y = np.array(pred).flatten()\n",
    "        true_Y = np.array(true).flatten()\n",
    "        eval_acc += accuracy_score(pred_Y, true_Y)\n",
    "        eval_precision += precision_score(pred_Y, true_Y, average='macro', zero_division=1)\n",
    "        eval_recall += recall_score(pred_Y, true_Y, average='macro', zero_division=1)\n",
    "        eval_f1 += f1_score(pred_Y, true_Y, average='macro', zero_division=1)\n",
    "        for id_, sent_id, text, p_list, p_ids, seq_len in zip(batch['id'], batch['sent_id'], batch['text'], probs.tolist(), probs_ids.tolist(), batch['seq_lens']):\n",
    "            prob_multi, label_multi = [], []\n",
    "            for index, pid in enumerate(p_ids[1: seq_len - 1]):\n",
    "                true_indices = np.argwhere(pid).flatten()\n",
    "                prob_multi.append(p_list[index])\n",
    "                label_multi.append([id2triggerlabel[true_index] for true_index in true_indices])\n",
    "            results.append({\"id\": id_, \"sent_id\":sent_id, \"text\": text, \"pred\": {\"probs\": prob_multi, \"labels\": label_multi}})\n",
    "        step += 1\n",
    "    print({\"Avg eval acc\": f\"{eval_acc/step:.2f}\", \"Avg eval precision\": f\"{eval_precision/step:.2f}\", \"Avg eval recall\": f\"{eval_recall/step:.2f}\", \"Avg eval f1\": f\"{eval_f1/step:.2f}\"})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a60a5b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b9e3deae56442ca7e9ab412e2a9d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'Avg eval acc': '1.00', 'Avg eval precision': '1.00', 'Avg eval recall': '1.00', 'Avg eval f1': '1.00'}\n"
     ]
    }
   ],
   "source": [
    "tigger_model = torch.load(tigger_model_path).to(device)\n",
    "\n",
    "test_trigger_sampler = SequentialSampler(dev_trigger_dataset)\n",
    "test_trigger_dataloader = DataLoader(dev_trigger_dataset, sampler=test_trigger_sampler, batch_size = 512)\n",
    "    \n",
    "sentences_tigger_data = test_trigger(tigger_model, test_trigger_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4700f5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_role(model, test_dataloader):\n",
    "    from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
    "\n",
    "    model.eval()\n",
    "    step = 0\n",
    "    eval_acc = 0.0\n",
    "    eval_f1 = 0.0\n",
    "    eval_precision = 0.0\n",
    "    eval_recall = 0.0\n",
    "    results = []\n",
    "    test_iterator = tqdm(test_dataloader)\n",
    "    for batch in test_iterator:\n",
    "        _, logits = model(\n",
    "            input_ids=batch['input_ids'].to(device),\n",
    "            attention_mask=batch['attention_masks'].to(device),\n",
    "            token_type_ids=batch['token_type_ids'].to(device)\n",
    "        )\n",
    "\n",
    "        probs = torch.sigmoid(logits).data.cpu()\n",
    "        probs_ids = (probs > 0.5).numpy()\n",
    "        probs = probs.numpy()\n",
    "        true_Y = batch['encoded_label'].cpu().numpy()\n",
    "        pred, true = [], []\n",
    "        for t_ids, p_ids, seq_len in zip(true_Y, probs_ids, batch['seq_lens']):\n",
    "            pred.extend(p_ids[1: seq_len - 1])\n",
    "            true.extend(t_ids[1: seq_len - 1])\n",
    "        pred_Y = np.array(pred).flatten()\n",
    "        true_Y = np.array(true).flatten()\n",
    "        eval_acc += accuracy_score(pred_Y, true_Y)\n",
    "        eval_precision += precision_score(pred_Y, true_Y, average='macro', zero_division=1)\n",
    "        eval_recall += recall_score(pred_Y, true_Y, average='macro', zero_division=1)\n",
    "        eval_f1 += f1_score(pred_Y, true_Y, average='macro', zero_division=1)\n",
    "        for id_, sent_id, text, input_ids, p_list, p_ids, seq_len in zip(batch['id'], batch['sent_id'], batch['text'], batch['input_ids'], probs.tolist(), probs_ids.tolist(), batch['seq_lens']):\n",
    "            prob_multi, label_multi = [], []\n",
    "            for index, pid in enumerate(p_ids[1: seq_len - 1]):\n",
    "                true_indices = np.argwhere(pid).flatten()\n",
    "                prob_multi.append(p_list[index])\n",
    "                label_multi.append([id2rolelabel[true_index] for true_index in true_indices])\n",
    "            results.append({\"id\": id_, \"sent_id\":sent_id, \"text\": text, \"tokens\": tokenizer.convert_ids_to_tokens(input_ids), \"pred\": {\"probs\": prob_multi, \"labels\": label_multi}})\n",
    "        step += 1\n",
    "    print({\"Avg eval acc\": f\"{eval_acc/step:.2f}\", \"Avg eval precision\": f\"{eval_precision/step:.2f}\", \"Avg eval recall\": f\"{eval_recall/step:.2f}\", \"Avg eval f1\": f\"{eval_f1/step:.2f}\"})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4e3e3e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27828c76094d4616bb1ec71660565fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'Avg eval acc': '1.00', 'Avg eval precision': '0.97', 'Avg eval recall': '0.98', 'Avg eval f1': '0.98'}\n"
     ]
    }
   ],
   "source": [
    "role_model = torch.load(role_model_path).to(device)\n",
    "\n",
    "test_role_sampler = SequentialSampler(dev_role_dataset)\n",
    "test_role_dataloader = DataLoader(dev_role_dataset, sampler=test_role_sampler, batch_size = 512)\n",
    "    \n",
    "sentences_role_data = test_role(role_model, test_role_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "472ac616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_by_lines, extract_result_multilabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea3a6ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_normalization(doc):\n",
    "    \"\"\"event_merge\"\"\"\n",
    "    for event in doc.get(\"event_list\", []):\n",
    "        argument_list = []\n",
    "        argument_set = set()\n",
    "        for arg in event[\"arguments\"]:\n",
    "            arg_str = \"{}-{}\".format(arg[\"role\"], arg[\"argument\"])\n",
    "            if arg_str not in argument_set:\n",
    "                argument_list.append(arg)\n",
    "            argument_set.add(arg_str)\n",
    "        event[\"arguments\"] = argument_list\n",
    "\n",
    "    event_list = sorted(\n",
    "        doc.get(\"event_list\", []),\n",
    "        key=lambda x: len(x[\"arguments\"]),\n",
    "        reverse=True)\n",
    "    new_event_list = []\n",
    "    for event in event_list:\n",
    "        event_type = event[\"event_type\"]\n",
    "        event_argument_set = set()\n",
    "        for arg in event[\"arguments\"]:\n",
    "            event_argument_set.add(\"{}-{}\".format(arg[\"role\"], arg[\"argument\"]))\n",
    "        flag = True\n",
    "        for new_event in new_event_list:\n",
    "            if event_type != new_event[\"event_type\"]:\n",
    "                continue\n",
    "            new_event_argument_set = set()\n",
    "            for arg in new_event[\"arguments\"]:\n",
    "                new_event_argument_set.add(\"{}-{}\".format(arg[\"role\"], arg[\n",
    "                    \"argument\"]))\n",
    "            if len(event_argument_set & new_event_argument_set) == len(\n",
    "                    new_event_argument_set):\n",
    "                flag = False\n",
    "        if flag:\n",
    "            new_event_list.append(event)\n",
    "    doc[\"event_list\"] = new_event_list\n",
    "    return doc\n",
    "\n",
    "def predict_data_process(trigger_data, role_data, enum_data, schema_file):\n",
    "    \"\"\"predict_data_process\"\"\"\n",
    "    pred_ret = []\n",
    "    schema_data = read_by_lines(schema_file)\n",
    "    print(\"trigger predict {} load.\".format(len(trigger_data)))\n",
    "    print(\"role predict {} load\".format(len(role_data)))\n",
    "    print(\"enum predict {} load\".format(len(enum_data)))\n",
    "    print(\"schema {} load from {}\".format(len(schema_data), schema_file))\n",
    "\n",
    "    schema, sent_role_mapping, sent_enum_mapping = {}, {}, {}\n",
    "    for s in schema_data:\n",
    "        d_json = json.loads(s)\n",
    "        schema[d_json[\"event_type\"]] = [r[\"role\"] for r in d_json[\"role_list\"]]\n",
    "\n",
    "    # role depends on id and sent_id \n",
    "    for d_json in role_data:\n",
    "        r_ret = extract_result_multilabel(d_json[\"text\"], d_json[\"pred\"][\"labels\"])\n",
    "        role_ret = {}\n",
    "        for r in r_ret:\n",
    "            role_type = r[\"type\"]\n",
    "            if role_type not in role_ret:\n",
    "                role_ret[role_type] = []\n",
    "            role_ret[role_type].append(\"\".join(r[\"text\"]))\n",
    "        _id = \"{}\\t{}\".format(d_json[\"id\"], d_json[\"sent_id\"])\n",
    "        if _id not in sent_role_mapping:\n",
    "            sent_role_mapping[_id] = role_ret\n",
    "        else:\n",
    "            for role_type, vals in role_ret.items():\n",
    "                if role_type in sent_role_mapping[_id]:\n",
    "                    sent_role_mapping[_id][role_type].extend(vals)\n",
    "                else:\n",
    "                    sent_role_mapping[_id][role_type] = vals\n",
    "\n",
    "    # process the enum_role data\n",
    "    for d_json in enum_data:\n",
    "        _id = \"{}\\t{}\".format(d_json[\"id\"], d_json[\"sent_id\"])\n",
    "        label = d_json[\"pred\"][\"label\"]\n",
    "        sent_enum_mapping[_id] = label\n",
    "\n",
    "    # process trigger data\n",
    "    for d_json in trigger_data:\n",
    "        t_ret = extract_result_multilabel(d_json[\"text\"], d_json[\"pred\"][\"labels\"])\n",
    "        pred_event_types = list(set([t[\"type\"] for t in t_ret]))\n",
    "        event_list = []\n",
    "        _id = \"{}\\t{}\".format(d_json[\"id\"], d_json[\"sent_id\"])\n",
    "        for event_type in pred_event_types:\n",
    "            role_list = schema[event_type]\n",
    "            arguments = []\n",
    "            for role_type, ags in sent_role_mapping[_id].items():\n",
    "                if role_type not in role_list:\n",
    "                    continue\n",
    "                for arg in ags:\n",
    "                    out = {\"role\": role_type, \"argument\": arg}\n",
    "                    if out not in arguments:\n",
    "                        arguments.append(out)\n",
    "            # 特殊处理环节\n",
    "            if event_type == enum_event_type:\n",
    "                arguments.append({\n",
    "                    \"role\": enum_role,\n",
    "                    \"argument\": sent_enum_mapping[_id]\n",
    "                })\n",
    "            event = {\n",
    "                \"event_type\": event_type,\n",
    "                \"arguments\": arguments,\n",
    "                \"text\": d_json[\"text\"]\n",
    "            }\n",
    "            event_list.append(event)\n",
    "        pred_ret.append({\n",
    "            \"id\": d_json[\"id\"],\n",
    "            \"sent_id\": d_json[\"sent_id\"],\n",
    "            \"text\": d_json[\"text\"],\n",
    "            \"event_list\": event_list\n",
    "        })\n",
    "    doc_pred = {}\n",
    "    for d in pred_ret:\n",
    "        if d[\"id\"] not in doc_pred:\n",
    "            doc_pred[d[\"id\"]] = {\"id\": d[\"id\"], \"event_list\": []}\n",
    "        doc_pred[d[\"id\"]][\"event_list\"].extend(d[\"event_list\"])\n",
    "\n",
    "    # unfiy the all prediction results and save them\n",
    "    doc_pred = [\n",
    "        event_normalization(r)\n",
    "        for r in doc_pred.values()\n",
    "    ]\n",
    "    print(\"submit data {} save\".format(len(doc_pred)))\n",
    "    return doc_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09818e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trigger predict 2995 load.\n",
      "role predict 2995 load\n",
      "enum predict 2995 load\n",
      "schema 13 load from ./dictionary/event_schema.json\n",
      "submit data 1174 save\n"
     ]
    }
   ],
   "source": [
    "doc_pred = predict_data_process(sentences_tigger_data, sentences_role_data, sentences_enum_data, shema_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f56e5585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_sent_enum_data, true_sent_tigger_data, true_sent_role_data  = [], [], []\n",
    "# with open(duee_fin_dev_preprocess_path, 'r', encoding='utf-8') as f:\n",
    "#     dataset = json.loads(f.read())\n",
    "#     preprocess_enum_dataset = enum_data_process(dataset)\n",
    "#     preprocess_trigger_dataset = data_process(dataset, model=\"trigger\")\n",
    "#     preprocess_role_dataset = data_process(dataset, model=\"role\")\n",
    "#     for d_json in preprocess_enum_dataset:\n",
    "#         true_sent_enum_data.append({\"id\": d_json['id'], \"sent_id\":d_json['sent_id'], \"text\": d_json['text'], \"pred\": {\"label\": d_json['label']}})\n",
    "#     for d_json in preprocess_trigger_dataset:\n",
    "#         labels = d_json['labels']\n",
    "#         labels = labels[:(max_seq_len - 2)]\n",
    "#         encoded_label = [[l] if isinstance(l , str) else l for l in labels]\n",
    "#         true_sent_tigger_data.append({\"id\": d_json['id'], \"sent_id\":d_json['sent_id'], \"text\": d_json['text'], \"pred\": {\"labels\": encoded_label}})\n",
    "#     for d_json in preprocess_role_dataset:\n",
    "#         labels = d_json['labels']\n",
    "#         labels = labels[:(max_seq_len - 2)]\n",
    "#         encoded_label = [[l] if isinstance(l , str) else l for l in labels]\n",
    "#         true_sent_role_data.append({\"id\": d_json['id'], \"sent_id\":d_json['sent_id'], \"text\": d_json['text'], \"pred\": {\"labels\": encoded_label}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53b73e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_true = predict_data_process(true_sent_tigger_data, true_sent_role_data, true_sent_enum_data, shema_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d08df807",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_data_list = []\n",
    "with open(duee_fin_dev_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        json_data = json.loads(line)\n",
    "        true_data_list.append(json_data)\n",
    "pred_mapping_dict = {}\n",
    "for doc in doc_pred:\n",
    "    pred_mapping_dict[doc['id']] = doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "96d7eb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '据\\u200b北京市市场监督管理局消息，近日，北京市市场监管局召集美团、饿了么、京东、微店、抖音、快手等6家互联网企业负责人，召开落实“长江禁捕打非断链”工作电商平台行政约谈会。\\n会上，市市场监管局要求电商平台立即开展自查自纠，严禁销售来自长江禁捕水域或来历不明的水产品；将“长江江鲜”“长江野生”等纳入平台禁售商品名录和违规词库，加强对平台用户的资质审核和内容审查；外卖订餐平台要重点检查商户店名、菜单、菜品、宣传图片、餐具等是否含有违规字样，一旦发现立即删除、屏蔽或暂停服务；各平台要对网站、APP、小程序、公众号等进行全面排查，加强对直播、短视频的内容管理，不得有与长江禁捕政策相抵触的宣传。\\n同时，会议还要求各电商平台利用互联网优势积极宣传长江禁捕政策，通过公益广告引导社会公众不买、不卖、不食、不运输、不发布违法广告。发现问题线索，及时上报市场监管部门。', 'event_list': [{'trigger': '约谈', 'event_type': '被约谈', 'arguments': [{'role': '约谈机构', 'argument': '北京市监局'}, {'role': '公司名称', 'argument': '美团'}, {'role': '被约谈时间', 'argument': '近日'}]}, {'trigger': '约谈', 'event_type': '被约谈', 'arguments': [{'role': '约谈机构', 'argument': '北京市监局'}, {'role': '公司名称', 'argument': '京东'}, {'role': '被约谈时间', 'argument': '近日'}]}, {'trigger': '约谈', 'event_type': '被约谈', 'arguments': [{'role': '约谈机构', 'argument': '北京市监局'}, {'role': '公司名称', 'argument': '微店'}, {'role': '被约谈时间', 'argument': '近日'}]}, {'trigger': '约谈', 'event_type': '被约谈', 'arguments': [{'role': '约谈机构', 'argument': '北京市监局'}, {'role': '公司名称', 'argument': '抖音'}, {'role': '被约谈时间', 'argument': '近日'}]}, {'trigger': '约谈', 'event_type': '被约谈', 'arguments': [{'role': '约谈机构', 'argument': '北京市监局'}, {'role': '公司名称', 'argument': '快手'}, {'role': '被约谈时间', 'argument': '近日'}]}, {'trigger': '约谈', 'event_type': '被约谈', 'arguments': [{'role': '约谈机构', 'argument': '北京市监局'}, {'role': '公司名称', 'argument': '饿了么'}, {'role': '被约谈时间', 'argument': '近日'}]}], 'id': '10be7f956da35f15fa4a9ad2a4556960', 'title': '北京市监局约谈美团、饿了么等6家网络平台，严禁销售长江禁捕水域水产品'}\n",
      "{'id': '10be7f956da35f15fa4a9ad2a4556960', 'event_list': [{'event_type': '被约谈', 'arguments': [{'role': '约谈机构', 'argument': '北京市监局'}, {'role': '公司名称', 'argument': '美团'}, {'role': '公司名称', 'argument': '饿了么'}, {'role': '公司名称', 'argument': '京东'}, {'role': '公司名称', 'argument': '微店'}, {'role': '公司名称', 'argument': '抖音'}, {'role': '公司名称', 'argument': '快手'}], 'text': '北京市监局约谈美团、饿了么等6家网络平台，严禁销售长江禁捕水域水产品 据北京市市场监督管理局消息，近日，北京市市场监管局召集美团、饿了么、京东、微店、抖音、快手等6家互联网企业负责人，召开落实“长江禁捕打非断链”工作电商平台行政约谈会。 会上，市市场监管局要求电商平台立即开展自查自纠，严禁销售来自长江禁捕水域或来历不明的水产品；将“长江江鲜”“长江野生”等纳入平台禁售商品名录和违规词库，加强对平台用户的资质审核和内容审查；外卖订餐平台要重点检查商户店名、菜单、菜品、宣传图片、餐具等是否含有违规字样，一旦发现立即删除、屏蔽或暂停服务；各平台要对网站、app、小程序、公众号等进行全面排查，加强对直播、短视频的内容管理，不得有与长江禁捕政策相抵触的宣传。 同时，会议还要求各电商平台利用互联网优势积极宣传长江禁捕政策，通过公益广告引导社会公众不买、不卖、不食、不运输、不发布违法广告。发现问题线索，及时上报市场监管部门。'}]}\n"
     ]
    }
   ],
   "source": [
    "index = 1\n",
    "\n",
    "print(true_data_list[index])\n",
    "print(pred_mapping_dict[doc_pred[index]['id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f18a18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "北京市监局约谈美团、饿了么等6家网络平台，严禁销售长江禁捕水域水产品 据北京市市场监督管理局消息，近日，北京市市场监管局召集美团、饿了么、京东、微店、抖音、快手等6家互联网企业负责人，召开落实“长江禁捕打非断链”工作电商平台行政约谈会。 会上，市市场监管局要求电商平台立即开展自查自纠，严禁销售来自长江禁捕水域或来历不明的水产品；将“长江江鲜”“长江野生”等纳入平台禁售商品名录和违规词库，加强对平台用户的资质审核和内容审查；外卖订餐平台要重点检查商户店名、菜单、菜品、宣传图片、餐具等是否含有违规字样，一旦发现立即删除、屏蔽或暂停服务；各平台要对网站、app、小程序、公众号等进行全面排查，加强对直播、短视频的内容管理，不得有与长江禁捕政策相抵触的宣传。 同时，会议还要求各电商平台利用互联网优势积极宣传长江禁捕政策，通过公益广告引导社会公众不买、不卖、不食、不运输、不发布违法广告。发现问题线索，及时上报市场监管部门。\n",
      "['[CLS]', '北', '京', '市', '监', '局', '约', '谈', '美', '团', '、', '饿', '了', '么', '等', '6', '家', '网', '络', '平', '台', '，', '严', '禁', '销', '售', '长', '江', '禁', '捕', '水', '域', '水', '产', '品', '，', '据', '北', '京', '市', '市', '场', '监', '督', '管', '理', '局', '消', '息', '，', '近', '日', '，', '北', '京', '市', '市', '场', '监', '管', '局', '召', '集', '美', '团', '、', '饿', '了', '么', '、', '京', '东', '、', '微', '店', '、', '抖', '音', '、', '快', '手', '等', '6', '家', '互', '联', '网', '企', '业', '负', '责', '人', '，', '召', '开', '落', '实', '“', '长', '江', '禁', '捕', '打', '非', '断', '链', '”', '工', '作', '电', '商', '平', '台', '行', '政', '约', '谈', '会', '。', '，', '会', '上', '，', '市', '市', '场', '监', '管', '局', '要', '求', '电', '商', '平', '台', '立', '即', '开', '展', '自', '查', '自', '纠', '，', '严', '禁', '销', '售', '来', '自', '长', '江', '禁', '捕', '水', '域', '或', '来', '历', '不', '明', '的', '水', '产', '品', '；', '将', '“', '长', '江', '江', '鲜', '”', '“', '长', '江', '野', '生', '”', '等', '纳', '入', '平', '台', '禁', '售', '商', '品', '名', '录', '和', '违', '规', '词', '库', '，', '加', '强', '对', '平', '台', '用', '户', '的', '资', '质', '审', '核', '和', '内', '容', '审', '查', '；', '外', '卖', '订', '餐', '平', '台', '要', '重', '点', '检', '查', '商', '户', '店', '名', '、', '菜', '单', '、', '菜', '品', '、', '宣', '传', '图', '片', '、', '餐', '具', '等', '是', '否', '含', '有', '违', '规', '字', '样', '，', '一', '旦', '发', '现', '立', '即', '删', '除', '、', '屏', '蔽', '或', '暂', '停', '服', '务', '；', '各', '平', '台', '要', '对', '网', '站', '、', 'a', 'p', 'p', '、', '小', '程', '序', '、', '公', '众', '号', '等', '进', '行', '全', '面', '排', '查', '，', '加', '强', '对', '直', '播', '、', '短', '视', '频', '的', '内', '容', '管', '理', '，', '不', '得', '有', '与', '长', '江', '禁', '捕', '政', '策', '相', '抵', '触', '的', '宣', '传', '。', '，', '同', '时', '，', '会', '议', '还', '要', '求', '各', '电', '商', '平', '台', '利', '用', '互', '联', '网', '优', '势', '积', '极', '宣', '传', '长', '江', '禁', '捕', '政', '策', '，', '通', '过', '公', '益', '广', '告', '引', '导', '社', '会', '公', '众', '不', '买', '、', '不', '卖', '、', '不', '食', '、', '不', '运', '输', '、', '不', '发', '布', '违', '法', '广', '告', '。', '发', '现', '问', '题', '线', '索', '，', '及', '时', '上', '报', '市', '场', '监', '管', '部', '门', '。', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "[['B-约谈机构'], ['I-约谈机构'], ['I-约谈机构'], ['I-约谈机构'], ['I-约谈机构'], ['O'], ['O'], ['B-公司名称'], ['I-公司名称'], ['O'], ['B-公司名称'], ['I-公司名称'], ['I-公司名称'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], [], ['I-被约谈时间'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['B-公司名称'], ['I-公司名称'], ['O'], ['B-公司名称'], ['I-公司名称'], ['O'], ['B-公司名称'], ['I-公司名称'], ['O'], ['B-公司名称'], ['I-公司名称'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O']]\n",
      "[{'start': 0, 'text': ['北', '京', '市', '监', '局'], 'type': '约谈机构'}, {'start': 7, 'text': ['美', '团'], 'type': '公司名称'}, {'start': 10, 'text': ['饿', '了', '么'], 'type': '公司名称'}, {'start': 69, 'text': ['京', '东'], 'type': '公司名称'}, {'start': 72, 'text': ['微', '店'], 'type': '公司名称'}, {'start': 75, 'text': ['抖', '音'], 'type': '公司名称'}, {'start': 78, 'text': ['快', '手'], 'type': '公司名称'}]\n",
      "现立即删除、屏蔽或暂停服务；各平台要对网站、app、小程序、公众号等进行全面排查，加强对直播、短视频的内容管理，不得有与长江禁捕政策相抵触的宣传。 同时，会议还要求各电商平台利用互联网优势积极宣传长江禁捕政策，通过公益广告引导社会公众不买、不卖、不食、不运输、不发布违法广告。发现问题线索，及时上报市场监管部门。\n",
      "['[CLS]', '现', '立', '即', '删', '除', '、', '屏', '蔽', '或', '暂', '停', '服', '务', '；', '各', '平', '台', '要', '对', '网', '站', '、', 'a', 'p', 'p', '、', '小', '程', '序', '、', '公', '众', '号', '等', '进', '行', '全', '面', '排', '查', '，', '加', '强', '对', '直', '播', '、', '短', '视', '频', '的', '内', '容', '管', '理', '，', '不', '得', '有', '与', '长', '江', '禁', '捕', '政', '策', '相', '抵', '触', '的', '宣', '传', '。', '，', '同', '时', '，', '会', '议', '还', '要', '求', '各', '电', '商', '平', '台', '利', '用', '互', '联', '网', '优', '势', '积', '极', '宣', '传', '长', '江', '禁', '捕', '政', '策', '，', '通', '过', '公', '益', '广', '告', '引', '导', '社', '会', '公', '众', '不', '买', '、', '不', '卖', '、', '不', '食', '、', '不', '运', '输', '、', '不', '发', '布', '违', '法', '广', '告', '。', '发', '现', '问', '题', '线', '索', '，', '及', '时', '上', '报', '市', '场', '监', '管', '部', '门', '。', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "[['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], [], [], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O'], ['O']]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for d_json in sentences_role_data:\n",
    "    if d_json['id'] == '10be7f956da35f15fa4a9ad2a4556960':\n",
    "        r_ret = extract_result_multilabel(d_json[\"text\"], d_json[\"pred\"][\"labels\"])\n",
    "        print(d_json[\"text\"])\n",
    "        print(d_json[\"tokens\"])\n",
    "        print(d_json[\"pred\"][\"labels\"])\n",
    "        print(r_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d38e5f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mergedata(predict_doc, true_merge_dataset_path):\n",
    "    true_data_list = []\n",
    "    with open(true_merge_dataset_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            json_data = json.loads(line)\n",
    "            true_data_list.append(json_data)\n",
    "    predict_mapping_dict = {}\n",
    "    for doc in predict_doc:\n",
    "        predict_mapping_dict[doc['id']] = doc\n",
    "    count_predict = 0\n",
    "    count_true = 0\n",
    "    count_correct = 0\n",
    "    for true_data in true_data_list:\n",
    "        if true_data['id'] not in predict_mapping_dict:\n",
    "            if 'event_list' in true_data:\n",
    "                print('error: ', true_data)\n",
    "        else:\n",
    "            predict_doc = predict_mapping_dict[true_data['id']]\n",
    "            pred_data_set = set([(pred_event['event_type'], argument['role'], argument['argument']) for pred_event in predict_doc.get('event_list', []) for argument in pred_event.get('arguments')])\n",
    "            true_data_set = set([(true_event['event_type'], argument['role'], argument['argument']) for true_event in true_data.get('event_list', []) for argument in true_event.get('arguments')])\n",
    "            count_predict += len(list(pred_data_set))\n",
    "            count_true += len(list(true_data_set))\n",
    "            count_correct += len(list(pred_data_set & true_data_set))\n",
    "    p = count_correct / max(1, count_predict)  # precision\n",
    "    r = count_correct / max(1, count_true)  # recall\n",
    "    f1 = 2 * r * p / max(1e-9, r + p) # f1 score\n",
    "    s = count_true  # support\n",
    "\n",
    "    print(\"{:>10}{:>10}{:>10}{:>10}\\n\".format(\"precision\", \"recall\", \"f1-score\", \"support\"))\n",
    "    formatter = \"{:>10.3f}{:>10.3f}{:>10.3f}{:>10d}\".format\n",
    "    print(formatter(p, r, f1, s))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29fb40e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " precision    recall  f1-score   support\n",
      "\n",
      "     0.508     0.740     0.602      7061\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_mergedata(doc_pred, duee_fin_dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60d567c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
