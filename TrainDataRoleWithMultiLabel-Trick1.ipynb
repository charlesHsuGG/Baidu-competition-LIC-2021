{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b99e3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel, AutoModelForTokenClassification\n",
    "\n",
    "from crf_layer import CRFLayer\n",
    "from multiLabelTokenClassfication import MultiLabelTokenClassification\n",
    "\n",
    "from utils import extract_result_multilabel\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore',category=UserWarning,module='torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "474ba276",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"hfl/chinese-roberta-wwm-ext\"\n",
    "\n",
    "save_model_path = \"roberta-chinese-base\"\n",
    "\n",
    "use_n_gpu = False\n",
    "\n",
    "enum_role = \"环节\"\n",
    "max_seq_len = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bedd926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(dataset):\n",
    "    \"\"\"data_process\"\"\"\n",
    "\n",
    "    def label_data(data, start, l, _type):\n",
    "        \"\"\"label_data\"\"\"\n",
    "        for i in range(start, start + l):\n",
    "            suffix = \"B-\" if i == start else \"I-\"\n",
    "            if isinstance(data[i], str):\n",
    "                data[i] = []\n",
    "            solt = \"{}{}\".format(suffix, _type)\n",
    "            if solt not in data[i]:\n",
    "                data[i].append(solt)\n",
    "        return data\n",
    "    \n",
    "    def replace_control_chars(str):\n",
    "        if str == '\\u200b' or str == '\\ufeff' or str == '\\ue601' or str == '\\u3000':\n",
    "            return '[UNK]'\n",
    "        else:\n",
    "            return str\n",
    "\n",
    "    output = []\n",
    "    for d_json in dataset:\n",
    "        _id = d_json[\"id\"]\n",
    "        text_a = [\n",
    "            \"，\" if t == \" \" or t == \"\\n\" or t == \"\\t\" else replace_control_chars(t)\n",
    "            for t in list(d_json[\"text\"].lower())\n",
    "        ]\n",
    "        if len(d_json.get(\"event_list\", [])) == 0:\n",
    "            continue\n",
    "        ### combine same event type\n",
    "        event_type_mapping = {}\n",
    "        for event in d_json.get(\"event_list\", []):\n",
    "            event_type = event['event_type']\n",
    "            trigger = event['trigger']\n",
    "            type_tuple = (event_type, trigger)\n",
    "            if type_tuple not in event_type_mapping:\n",
    "                event_type_mapping[type_tuple] = []\n",
    "            for argument in event[\"arguments\"]:\n",
    "                if argument not in event_type_mapping[type_tuple]:\n",
    "                    event_type_mapping[type_tuple].append(argument)\n",
    "\n",
    "        for type_tuple, arguments in event_type_mapping.items():\n",
    "            event_type = type_tuple[0]\n",
    "            trigger = type_tuple[1]\n",
    "            labels = [\"O\"] * len(text_a)\n",
    "            for arg in arguments:\n",
    "                role_type = arg[\"role\"]\n",
    "                if role_type == enum_role:\n",
    "                    continue\n",
    "                argument = arg[\"argument\"]\n",
    "                start = arg[\"argument_start_index\"]\n",
    "                labels = label_data(labels, start, len(argument), role_type)\n",
    "            text_trigger = [\n",
    "                \"，\" if t == \" \" or t == \"\\n\" or t == \"\\t\" else t\n",
    "                for t in list(event_type+f\"({trigger})：\".lower())\n",
    "            ]\n",
    "            trigger_label = [\"O\"] * len(text_trigger)\n",
    "            output.append({\n",
    "               \"text\": event_type + f\"({trigger})：\" + d_json[\"text\"], \"tokens\": text_trigger+text_a, \"labels\": trigger_label+labels\n",
    "            })\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4af2aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(dict_path):\n",
    "    \"\"\"load_dict\"\"\"\n",
    "    vocab = {}\n",
    "    for line in open(dict_path, 'r', encoding='utf-8'):\n",
    "        value, key = line.strip('\\n').split('\\t')\n",
    "        vocab[key] = int(value)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09abf029",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_vocab = load_dict(dict_path='./dictionary/role_tag.dict')\n",
    "id2label = {val: key for key, val in label_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba347d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.num_labels = len(label_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87567be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44aa16b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PADDING = tokenizer.vocab[tokenizer.pad_token]\n",
    "SEP = tokenizer.vocab[tokenizer.sep_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "870891b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaiduEventDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset_path, label_dict_path, ignore_index=-100):\n",
    "        self.label_vocab = load_dict(label_dict_path)\n",
    "        self.label_num = max(self.label_vocab.values()) + 1\n",
    "        self.examples = []\n",
    "        with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "            dataset = json.loads(f.read())\n",
    "            preprocess_dataset = data_process(dataset)\n",
    "            for d_json in preprocess_dataset:\n",
    "                tokens = d_json['tokens']\n",
    "                input_ids = tokenizer(tokens, is_split_into_words=True, add_special_tokens=True, max_length=max_seq_len, truncation=True)['input_ids']\n",
    "                tokens_input = input_ids + [PADDING] * (max_seq_len - len(input_ids))\n",
    "                attention_masks = self._get_attention_mask(input_ids, max_seq_len)\n",
    "                token_type_ids = self._get_token_type_id(input_ids, max_seq_len)\n",
    "                example = {\n",
    "                    \"text\": d_json['text'], \n",
    "                    \"input_ids\": tokens_input, \"attention_masks\": attention_masks,\n",
    "                    \"token_type_ids\": token_type_ids, \"seq_lens\": len(tokens)\n",
    "                }\n",
    "                if 'labels' in d_json:\n",
    "                    labels = d_json['labels']\n",
    "                    labels = labels[:(max_seq_len - 2)]\n",
    "                    encoded_label = [\"O\"] + labels + [\"O\"]\n",
    "                    encoded_label = self.to_one_hot_vector(encoded_label, max_seq_len - 2 - len(labels))\n",
    "                    example.update({\"encoded_label\": encoded_label})\n",
    "                self.examples.append(example)\n",
    "\n",
    "    def to_one_hot_vector(self, labels, zero_padding_len = 0):\n",
    "        \"\"\"Convert seq to one hot.\"\"\"\n",
    "        one_hot_vectors = []\n",
    "        for label in labels:\n",
    "            one_hot_vector = np.zeros(self.label_num)\n",
    "            if isinstance(label, str):\n",
    "                one_hot_vector[self.label_vocab.get(label, 0)] = 1\n",
    "            elif isinstance(label, list):\n",
    "                for l in label:\n",
    "                    one_hot_vector[self.label_vocab.get(l, 0)] = 1\n",
    "            one_hot_vectors.append(one_hot_vector)\n",
    "        for _ in range(zero_padding_len):\n",
    "            one_hot_vector = np.zeros(self.label_num)\n",
    "            one_hot_vectors.append(one_hot_vector)\n",
    "        return np.array(one_hot_vectors)\n",
    "\n",
    "    def _get_attention_mask(self, input_ids, max_seq_len):\n",
    "        \"\"\"Mask for padding.\"\"\"\n",
    "        if len(input_ids) > max_seq_len:\n",
    "            raise IndexError(\"Token length more than max seq length!\")\n",
    "        return [1] * len(input_ids) + [0] * (max_seq_len - len(input_ids))\n",
    "\n",
    "    def _get_token_type_id(self, input_ids, max_seq_len):\n",
    "        \"\"\"Segments: 0 for the first sequence, 1 for the second.\"\"\"\n",
    "        if len(input_ids) > max_seq_len:\n",
    "            raise IndexError(\"Token length more than max seq length!\")\n",
    "        segments = []\n",
    "        current_segment_id = 0\n",
    "        for input_id in input_ids:\n",
    "            segments.append(current_segment_id)\n",
    "            if input_id == SEP:\n",
    "                current_segment_id = 1\n",
    "        return segments + [0] * (max_seq_len - len(input_ids))        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, item_idx):\n",
    "        example = {\n",
    "            \"text\": self.examples[item_idx][\"text\"],\n",
    "            \"input_ids\": torch.tensor(self.examples[item_idx][\"input_ids\"]).long(),\n",
    "            \"attention_masks\": torch.tensor(self.examples[item_idx][\"attention_masks\"]),\n",
    "            \"token_type_ids\": torch.tensor(self.examples[item_idx][\"token_type_ids\"]),\n",
    "            \"seq_lens\": self.examples[item_idx][\"seq_lens\"]\n",
    "        }\n",
    "        if \"encoded_label\" in self.examples[item_idx]:\n",
    "            example.update({\"encoded_label\": torch.tensor(self.examples[item_idx][\"encoded_label\"], dtype=torch.float)})\n",
    "        return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5903c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BaiduEventDataset(dataset_path='./resources/duee_fin_train_preprocess.json', label_dict_path='./dictionary/role_tag.dict')\n",
    "dev_dataset = BaiduEventDataset(dataset_path='./resources/duee_fin_dev_preprocess.json', label_dict_path='./dictionary/role_tag.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9cb3ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '公司上市(IPO)：理想汽车拟将美国IPO定价在招股区间顶端或更高水平据腾讯美股30日消息，据知情人士透露，理想汽车告诉潜在投资者，计划把美国首次公开募股（IPO）发行价定在招股区间顶端，甚至更高水平。该公司正以每股8-10美元发行9500万股股票。',\n",
       " 'input_ids': tensor([ 101, 1062, 1385,  677, 2356,  113,  151,  158,  157,  114, 8038, 4415,\n",
       "         2682, 3749, 6756, 2877, 2199, 5401, 1744,  151,  158,  157, 2137,  817,\n",
       "         1762, 2875, 5500, 1277, 7313, 7553, 4999, 2772, 3291, 7770, 3717, 2398,\n",
       "         2945, 5596, 6380, 5401, 5500,  124,  121, 3189, 3867, 2622, 8024, 2945,\n",
       "         4761, 2658,  782, 1894, 6851, 7463, 8024, 4415, 2682, 3749, 6756, 1440,\n",
       "         6401, 4052, 1762, 2832, 6598, 5442, 8024, 6369, 1153, 2828, 5401, 1744,\n",
       "         7674, 3613, 1062, 2458, 1247, 5500, 8020,  151,  158,  157, 8021, 1355,\n",
       "         6121,  817, 2137, 1762, 2875, 5500, 1277, 7313, 7553, 4999, 8024, 4493,\n",
       "         5635, 3291, 7770, 3717, 2398,  511, 6421, 1062, 1385, 3633,  809, 3680,\n",
       "         5500,  129,  118,  122,  121, 5401, 1039, 1355, 6121,  130,  126,  121,\n",
       "          121,  674, 5500, 5500, 4873,  511,  102,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]),\n",
       " 'attention_masks': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'seq_lens': 125,\n",
       " 'encoded_label': tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32097756",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = MultiLabelTokenClassification(model_name, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21aa23c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed = 42):\n",
    "    \"\"\"Set the seed for generating random numbers on all GPUs.\n",
    "\n",
    "    It's safe to call this function if CUDA is not available; in that case, it is silently ignored.\n",
    "\n",
    "    Args:\n",
    "        seed (int, optional): random numbers on all GPUs. Defaults to 42.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa8d34f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:2\n",
      "\n",
      "Tesla V100-PCIE-32GB\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "CUDA Device Count: 3\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "if use_n_gpu:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "else:\n",
    "    device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    \n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', torch.cuda.memory_allocated(0)/1024**3, 'GB')\n",
    "    print('Cached:   ', torch.cuda.memory_reserved(0)/1024**3, 'GB')\n",
    "    \n",
    "    print('CUDA Device Count:', n_gpu)\n",
    "    \n",
    "set_seed(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "569b24e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, eval_dataloader):\n",
    "    model.eval()\n",
    "    step = 0\n",
    "    eval_acc = 0.0\n",
    "    eval_f1 = 0.0\n",
    "    eval_precision = 0.0\n",
    "    eval_recall = 0.0\n",
    "    eval_loss = 0.0\n",
    "    for batch in eval_dataloader:\n",
    "        loss, logits = model(\n",
    "            input_ids=batch['input_ids'].to(device),\n",
    "            attention_mask=batch['attention_masks'].to(device),\n",
    "            token_type_ids=batch['token_type_ids'].to(device),\n",
    "            labels=batch['encoded_label'].to(device)\n",
    "        )\n",
    "        \n",
    "        if use_n_gpu and n_gpu > 1:\n",
    "            loss = loss.mean()\n",
    "        \n",
    "        eval_loss += loss.item()\n",
    "        pred_Y = (torch.sigmoid(logits).data > 0.5).cpu().numpy()\n",
    "        true_Y = batch['encoded_label'].cpu().numpy()\n",
    "        batch_size = true_Y.shape[0]\n",
    "        batch_precision, batch_recall, batch_f1 = 0.0, 0.0, 0.0\n",
    "        for text, t_ids, p_ids, seq_len in zip(batch[\"text\"], true_Y, pred_Y, batch['seq_lens']):\n",
    "            true_label, pred_label = [], []\n",
    "            for pid in p_ids[1: seq_len - 1]:\n",
    "                true_indices = np.argwhere(pid).flatten()\n",
    "                pred_label.append([id2label[true_index] for true_index in true_indices])\n",
    "            p_ret = extract_result_multilabel(text, pred_label)\n",
    "            pred_event_types = set([(p[\"type\"], ''.join(p[\"text\"]), p[\"start\"]) for p in p_ret])\n",
    "            for tid in t_ids[1: seq_len - 1]:\n",
    "                true_indices = np.argwhere(tid).flatten()\n",
    "                true_label.append([id2label[true_index] for true_index in true_indices])\n",
    "            t_ret = extract_result_multilabel(text, true_label)\n",
    "            true_event_types = set([(t[\"type\"], ''.join(t[\"text\"]), t[\"start\"]) for t in t_ret])\n",
    "            count_predict = len(list(pred_event_types))\n",
    "            count_true = len(list(true_event_types))\n",
    "            count_correct = len(list(pred_event_types & true_event_types))\n",
    "            p = count_correct / max(1, count_predict)  # precision\n",
    "            r = count_correct / max(1, count_true)  # recall\n",
    "            batch_precision += p\n",
    "            batch_recall += r\n",
    "            batch_f1 += 2 * r * p / max(1e-9, r + p) # f1 score\n",
    "        eval_acc += accuracy_score(pred_Y.flatten(), true_Y.flatten())\n",
    "        eval_precision += batch_precision / batch_size\n",
    "        eval_recall += batch_recall / batch_size\n",
    "        eval_f1 += batch_f1 / batch_size\n",
    "        step += 1\n",
    "    model.train()\n",
    "    return eval_loss/step, eval_acc/step, eval_precision/step, eval_recall/step, eval_f1/step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57b5dce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### train model\n",
    "\n",
    "def train(model, ds_train, ds_dev = None, n_epochs = 100, learning_rate = 5e-5, weight_decay = 0.01, batch_size = 1, eval_per_epoch = 2):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    train_sampler = RandomSampler(ds_train)\n",
    "    train_dataloader = DataLoader(ds_train, sampler=train_sampler, batch_size=batch_size)\n",
    "    \n",
    "    eval_sampler = SequentialSampler(ds_dev)\n",
    "    eval_dataloader = DataLoader(ds_dev, sampler=eval_sampler, batch_size=batch_size)\n",
    "    \n",
    "    decay_params = [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ]\n",
    "\n",
    "    if use_n_gpu and n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model, device_ids=[0, 1])\n",
    "\n",
    "    optimizer_grouped_parameters = [{\n",
    "        \"params\": model.parameters(),\n",
    "        \"lr\": learning_rate, \n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"apply_decay_param_fun\": lambda x: x in decay_params\n",
    "    }]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters)\n",
    "#     scheduler = ReduceLROnPlateau(optimizer, \"min\")\n",
    "    \n",
    "    f1 = 0.0\n",
    "    acc = 0.0\n",
    "    precision = 0.0\n",
    "    recall = 0.0\n",
    "    tr_loss = 0.0\n",
    "    global_step = 0\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    postfix = {}\n",
    "    for epoch in range(0, n_epochs):\n",
    "        eval_flag = False\n",
    "        train_iterator = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{n_epochs}\")\n",
    "        for batch in train_iterator:\n",
    "            loss, logits = model(\n",
    "                input_ids=batch['input_ids'].to(device),\n",
    "                attention_mask=batch['attention_masks'].to(device),\n",
    "                token_type_ids=batch['token_type_ids'].to(device),\n",
    "                labels=batch['encoded_label'].to(device)\n",
    "            )\n",
    "            \n",
    "            if use_n_gpu and n_gpu > 1:\n",
    "                loss = loss.mean()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # scheduler.step(loss)\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            pred_Y = (torch.sigmoid(logits).data > 0.5).cpu().numpy()\n",
    "            true_Y = batch['encoded_label'].cpu().numpy()\n",
    "            batch_size = true_Y.shape[0]\n",
    "            batch_precision, batch_recall, batch_f1 = 0.0, 0.0, 0.0\n",
    "            for text, t_ids, p_ids, seq_len in zip(batch[\"text\"], true_Y, pred_Y, batch['seq_lens']):\n",
    "                true_label, pred_label = [], []\n",
    "                for pid in p_ids[1: seq_len - 1]:\n",
    "                    true_indices = np.argwhere(pid).flatten()\n",
    "                    pred_label.append([id2label[true_index] for true_index in true_indices])\n",
    "                p_ret = extract_result_multilabel(text, pred_label)\n",
    "                pred_event_types = set([(p[\"type\"], ''.join(p[\"text\"]), p[\"start\"]) for p in p_ret])\n",
    "                for tid in t_ids[1: seq_len - 1]:\n",
    "                    true_indices = np.argwhere(tid).flatten()\n",
    "                    true_label.append([id2label[true_index] for true_index in true_indices])\n",
    "                t_ret = extract_result_multilabel(text, true_label)\n",
    "                true_event_types = set([(t[\"type\"], ''.join(t[\"text\"]), t[\"start\"]) for t in t_ret])\n",
    "                count_predict = len(list(pred_event_types))\n",
    "                count_true = len(list(true_event_types))\n",
    "                count_correct = len(list(pred_event_types & true_event_types))\n",
    "                p = count_correct / max(1, count_predict)  # precision\n",
    "                r = count_correct / max(1, count_true)  # recall\n",
    "                batch_precision += p\n",
    "                batch_recall += r\n",
    "                batch_f1 += 2 * r * p / max(1e-9, r + p) # f1 score\n",
    "            acc += accuracy_score(pred_Y.flatten(), true_Y.flatten())\n",
    "            precision += batch_precision / batch_size\n",
    "            recall += batch_recall / batch_size\n",
    "            f1 += batch_f1 / batch_size\n",
    "            model.zero_grad()\n",
    "\n",
    "            postfix.update({\"Avg loss\": f\"{tr_loss / (global_step + 1):.5f}\", \"Avg acc score\": f\"{acc / (global_step + 1):.5f}\", \"Avg precision score\": f\"{precision / (global_step + 1):.5f}\", \"Avg recall score\": f\"{recall / (global_step + 1):.5f}\", \"Avg f1 score\": f\"{f1 / (global_step + 1):.5f}\"})\n",
    "            if (\n",
    "                not eval_flag\n",
    "                and (global_step + 1) % len(train_dataloader) == 0\n",
    "                and (epoch % eval_per_epoch) == 0\n",
    "            ):\n",
    "                if ds_dev is not None:\n",
    "                    eval_loss, eval_acc, eval_precision, eval_recall, eval_f1 = evaluate(model, eval_dataloader)\n",
    "                postfix.update({\"Avg eval loss\": f\"{eval_loss:.5f}\", \"Avg eval acc\": f\"{eval_acc:.5f}\", \"Avg eval precision\": f\"{eval_precision:.5f}\", \"Avg eval recall\": f\"{eval_recall:.5f}\", \"Avg eval f1\": f\"{eval_f1:.5f}\"})\n",
    "                eval_flag = True\n",
    "            train_iterator.set_postfix(postfix)\n",
    "            global_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "213f4b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 352/352 [23:06<00:00,  3.94s/it, Avg loss=0.07204, Avg acc score=0.99358, Avg precision score=0.00000, Avg recall score=0.00000, Avg f1 score=0.00000, Avg eval loss=0.01239, Avg eval acc=0.99905, Avg eval precision=0.00000, Avg eval recall=0.00000, Avg eval f1=0.00000]\n",
      "Epoch 2/20: 100%|██████████| 352/352 [08:46<00:00,  1.50s/it, Avg loss=0.04042, Avg acc score=0.99638, Avg precision score=0.00000, Avg recall score=0.00000, Avg f1 score=0.00000, Avg eval loss=0.01239, Avg eval acc=0.99905, Avg eval precision=0.00000, Avg eval recall=0.00000, Avg eval f1=0.00000]\n",
      "Epoch 3/20: 100%|██████████| 352/352 [09:30<00:00,  1.62s/it, Avg loss=0.02857, Avg acc score=0.99734, Avg precision score=0.00000, Avg recall score=0.00000, Avg f1 score=0.00000, Avg eval loss=0.00375, Avg eval acc=0.99932, Avg eval precision=0.00000, Avg eval recall=0.00000, Avg eval f1=0.00000]\n",
      "Epoch 4/20: 100%|██████████| 352/352 [08:49<00:00,  1.50s/it, Avg loss=0.02221, Avg acc score=0.99786, Avg precision score=0.00343, Avg recall score=0.00070, Avg f1 score=0.00111, Avg eval loss=0.00375, Avg eval acc=0.99932, Avg eval precision=0.00000, Avg eval recall=0.00000, Avg eval f1=0.00000]\n",
      "Epoch 5/20: 100%|██████████| 352/352 [09:32<00:00,  1.63s/it, Avg loss=0.01821, Avg acc score=0.99820, Avg precision score=0.03499, Avg recall score=0.00848, Avg f1 score=0.01289, Avg eval loss=0.00194, Avg eval acc=0.99956, Avg eval precision=0.48798, Avg eval recall=0.25670, Avg eval f1=0.31248]\n",
      "Epoch 6/20: 100%|██████████| 352/352 [08:48<00:00,  1.50s/it, Avg loss=0.01545, Avg acc score=0.99843, Avg precision score=0.09762, Avg recall score=0.03657, Avg f1 score=0.04900, Avg eval loss=0.00194, Avg eval acc=0.99956, Avg eval precision=0.48798, Avg eval recall=0.25670, Avg eval f1=0.31248]\n",
      "Epoch 7/20: 100%|██████████| 352/352 [09:32<00:00,  1.63s/it, Avg loss=0.01343, Avg acc score=0.99861, Avg precision score=0.16431, Avg recall score=0.07898, Avg f1 score=0.09825, Avg eval loss=0.00152, Avg eval acc=0.99959, Avg eval precision=0.67305, Avg eval recall=0.61579, Avg eval f1=0.62072]\n",
      "Epoch 8/20: 100%|██████████| 352/352 [08:48<00:00,  1.50s/it, Avg loss=0.01189, Avg acc score=0.99875, Avg precision score=0.22707, Avg recall score=0.12709, Avg f1 score=0.15098, Avg eval loss=0.00152, Avg eval acc=0.99959, Avg eval precision=0.67305, Avg eval recall=0.61579, Avg eval f1=0.62072]\n",
      "Epoch 9/20: 100%|██████████| 352/352 [09:33<00:00,  1.63s/it, Avg loss=0.01067, Avg acc score=0.99887, Avg precision score=0.28232, Avg recall score=0.17599, Avg f1 score=0.20229, Avg eval loss=0.00142, Avg eval acc=0.99960, Avg eval precision=0.66933, Avg eval recall=0.70161, Avg eval f1=0.66910]\n",
      "Epoch 10/20: 100%|██████████| 352/352 [08:50<00:00,  1.51s/it, Avg loss=0.00968, Avg acc score=0.99896, Avg precision score=0.32976, Avg recall score=0.22209, Avg f1 score=0.24929, Avg eval loss=0.00142, Avg eval acc=0.99960, Avg eval precision=0.66933, Avg eval recall=0.70161, Avg eval f1=0.66910]\n",
      "Epoch 11/20: 100%|██████████| 352/352 [09:34<00:00,  1.63s/it, Avg loss=0.00886, Avg acc score=0.99904, Avg precision score=0.37045, Avg recall score=0.26408, Avg f1 score=0.29122, Avg eval loss=0.00142, Avg eval acc=0.99960, Avg eval precision=0.66029, Avg eval recall=0.71957, Avg eval f1=0.67309]\n",
      "Epoch 12/20: 100%|██████████| 352/352 [08:50<00:00,  1.51s/it, Avg loss=0.00817, Avg acc score=0.99911, Avg precision score=0.40595, Avg recall score=0.30244, Avg f1 score=0.32897, Avg eval loss=0.00142, Avg eval acc=0.99960, Avg eval precision=0.66029, Avg eval recall=0.71957, Avg eval f1=0.67309]\n",
      "Epoch 13/20: 100%|██████████| 352/352 [09:36<00:00,  1.64s/it, Avg loss=0.00759, Avg acc score=0.99917, Avg precision score=0.43687, Avg recall score=0.33686, Avg f1 score=0.36250, Avg eval loss=0.00143, Avg eval acc=0.99962, Avg eval precision=0.67055, Avg eval recall=0.71587, Avg eval f1=0.67780]\n",
      "Epoch 14/20: 100%|██████████| 352/352 [08:52<00:00,  1.51s/it, Avg loss=0.00708, Avg acc score=0.99922, Avg precision score=0.46416, Avg recall score=0.36792, Avg f1 score=0.39256, Avg eval loss=0.00143, Avg eval acc=0.99962, Avg eval precision=0.67055, Avg eval recall=0.71587, Avg eval f1=0.67780]\n",
      "Epoch 15/20: 100%|██████████| 352/352 [09:36<00:00,  1.64s/it, Avg loss=0.00663, Avg acc score=0.99926, Avg precision score=0.48865, Avg recall score=0.39635, Avg f1 score=0.41989, Avg eval loss=0.00146, Avg eval acc=0.99962, Avg eval precision=0.66246, Avg eval recall=0.73068, Avg eval f1=0.68030]\n",
      "Epoch 16/20: 100%|██████████| 352/352 [08:52<00:00,  1.51s/it, Avg loss=0.00624, Avg acc score=0.99930, Avg precision score=0.51021, Avg recall score=0.42184, Avg f1 score=0.44422, Avg eval loss=0.00146, Avg eval acc=0.99962, Avg eval precision=0.66246, Avg eval recall=0.73068, Avg eval f1=0.68030]\n",
      "Epoch 17/20: 100%|██████████| 352/352 [09:36<00:00,  1.64s/it, Avg loss=0.00590, Avg acc score=0.99934, Avg precision score=0.52970, Avg recall score=0.44506, Avg f1 score=0.46638, Avg eval loss=0.00150, Avg eval acc=0.99962, Avg eval precision=0.67410, Avg eval recall=0.72007, Avg eval f1=0.68254]\n",
      "Epoch 18/20: 100%|██████████| 352/352 [08:50<00:00,  1.51s/it, Avg loss=0.00559, Avg acc score=0.99937, Avg precision score=0.54759, Avg recall score=0.46659, Avg f1 score=0.48686, Avg eval loss=0.00150, Avg eval acc=0.99962, Avg eval precision=0.67410, Avg eval recall=0.72007, Avg eval f1=0.68254]\n",
      "Epoch 19/20: 100%|██████████| 352/352 [09:36<00:00,  1.64s/it, Avg loss=0.00531, Avg acc score=0.99940, Avg precision score=0.56375, Avg recall score=0.48626, Avg f1 score=0.50548, Avg eval loss=0.00156, Avg eval acc=0.99962, Avg eval precision=0.67101, Avg eval recall=0.74218, Avg eval f1=0.68940]\n",
      "Epoch 20/20: 100%|██████████| 352/352 [08:50<00:00,  1.51s/it, Avg loss=0.00506, Avg acc score=0.99942, Avg precision score=0.57864, Avg recall score=0.50437, Avg f1 score=0.52263, Avg eval loss=0.00156, Avg eval acc=0.99962, Avg eval precision=0.67101, Avg eval recall=0.74218, Avg eval f1=0.68940]\n"
     ]
    }
   ],
   "source": [
    "if use_n_gpu:\n",
    "    train(model, train_dataset, ds_dev=dev_dataset, n_epochs=20, batch_size=12*2)\n",
    "else:\n",
    "    train(model, train_dataset, ds_dev=dev_dataset, n_epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62fe2a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.cpu(), f'./models/DuEE_fin/{save_model_path}/role-multilabel-trick1.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "458f733c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-18bf08814031>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/cuda/memory.py\u001b[0m in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m     \"\"\"\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_emptyCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c25bbf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "069d4b83295243b0bedfebb442b10c5b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_c4fd9ccefe95434db2e7df930aaac118",
       "style": "IPY_MODEL_e1c3e571d2b54106bf20fac3e33b5012",
       "value": " 130/274 [02:12&lt;02:26,  1.02s/it, Avg loss=1.23, Avg acc score=0.92, Avg f1 score=0.92]"
      }
     },
     "60f889e1229a4154aa318d4c46d16fe2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "80336ec9357f4abd8c4c3ba264963949": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_8e3273f660b642669d155147b757f335",
        "IPY_MODEL_069d4b83295243b0bedfebb442b10c5b"
       ],
       "layout": "IPY_MODEL_f399e7b310cd4d1fb83bf06a20fc5d56"
      }
     },
     "8e3273f660b642669d155147b757f335": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "description": "Epoch 1/20:  47%",
       "layout": "IPY_MODEL_60f889e1229a4154aa318d4c46d16fe2",
       "max": 274,
       "style": "IPY_MODEL_c2f754305c5341929c539cca7dd9a9d2",
       "value": 130
      }
     },
     "c2f754305c5341929c539cca7dd9a9d2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "c4fd9ccefe95434db2e7df930aaac118": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e1c3e571d2b54106bf20fac3e33b5012": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "f399e7b310cd4d1fb83bf06a20fc5d56": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
