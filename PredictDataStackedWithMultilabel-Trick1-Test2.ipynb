{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5761e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel\n",
    "\n",
    "from crf_layer import CRFLayer\n",
    "from multiLabelTokenClassfication import MultiLabelTokenClassification\n",
    "\n",
    "from utils import read_by_lines, extract_result_multilabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04abdc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_Id = \"2\"\n",
    "\n",
    "shema_path = './dictionary/event_schema.json'\n",
    "enerm_dict_path = './dictionary/enum_tag.dict'\n",
    "trigger_dict_path = './dictionary/trigger_tag.dict'\n",
    "role_dict_path = './dictionary/role_tag.dict'\n",
    "\n",
    "enerm_stacked_model_path = './models/DuEE_fin/stacked/stacked_enum.dict'\n",
    "tigger_stacked_model_path = './models/DuEE_fin/stacked/stacked_trigger-multilabel.dict'\n",
    "role_stacked_model_path = './models/DuEE_fin/stacked/stacked_role-multilabel-trick1.dict'\n",
    "\n",
    "duee_fin_test_preprocess_path = './resources/duee_fin_test2_preprocess.json'\n",
    "\n",
    "enum_role = \"环节\"\n",
    "enum_event_type = \"公司上市\"\n",
    "max_seq_len = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5b25c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(dict_path):\n",
    "    \"\"\"load_dict\"\"\"\n",
    "    vocab = {}\n",
    "    for line in open(dict_path, 'r', encoding='utf-8'):\n",
    "        value, key = line.strip('\\n').split('\\t')\n",
    "        vocab[key] = int(value)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df86af26",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_enum_vocab = load_dict(dict_path=enerm_dict_path)\n",
    "id2enumlabel = {val: key for key, val in label_enum_vocab.items()}\n",
    "label_trigger_vocab = load_dict(dict_path=trigger_dict_path)\n",
    "id2triggerlabel = {val: key for key, val in label_trigger_vocab.items()}\n",
    "label_role_vocab = load_dict(dict_path=role_dict_path)\n",
    "id2rolelabel = {val: key for key, val in label_role_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16fb478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enum_data_process(dataset):\n",
    "    \"\"\"enum_data_process\"\"\"\n",
    "    output = []\n",
    "    for d_json in dataset:\n",
    "        text = d_json[\"text\"].lower().replace(\"\\t\", \" \")\n",
    "        output.append({\n",
    "            \"id\": d_json[\"id\"],\n",
    "            \"sent_id\": d_json[\"sent_id\"],\n",
    "            \"text\": text\n",
    "        })\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec34649c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigger_data_process(dataset):\n",
    "    \"\"\"data_process\"\"\"\n",
    "    \n",
    "    def replace_control_chars(str):\n",
    "        if str == '\\u200b' or str == '\\ufeff' or str == '\\ue601' or str == '\\u3000':\n",
    "            return '[UNK]'\n",
    "        else:\n",
    "            return str\n",
    "\n",
    "    output = []\n",
    "    for d_json in dataset:\n",
    "        _id = d_json[\"id\"]\n",
    "        text_a = [\n",
    "            \"，\" if t == \" \" or t == \"\\n\" or t == \"\\t\" else replace_control_chars(t)\n",
    "            for t in list(d_json[\"text\"].lower())\n",
    "        ]\n",
    "        output.append({\n",
    "            \"id\": d_json[\"id\"],\n",
    "            \"sent_id\": d_json[\"sent_id\"],\n",
    "            \"text\": d_json[\"text\"],\n",
    "            \"tokens\": text_a\n",
    "        })\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74074759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def role_data_process(dataset, trigger_file_path):\n",
    "    \"\"\"data_process\"\"\"\n",
    "    \n",
    "    def replace_control_chars(str):\n",
    "        if str == '\\u200b' or str == '\\ufeff' or str == '\\ue601' or str == '\\u3000':\n",
    "            return '[UNK]'\n",
    "        else:\n",
    "            return str\n",
    "    \n",
    "    trigger_data = read_by_lines(trigger_file_path)\n",
    "    # process trigger data\n",
    "    sent_trigger_mapping = {}\n",
    "    for d in tqdm(trigger_data, total=len(trigger_data)):\n",
    "        d_json = json.loads(d)\n",
    "        t_ret = extract_result_multilabel(d_json[\"text\"], d_json[\"pred\"][\"labels\"])\n",
    "        pred_event_types = list(set([(t[\"type\"], ''.join(t[\"text\"])) for t in t_ret]))\n",
    "        if d_json[\"id\"] not in sent_trigger_mapping:\n",
    "            sent_trigger_mapping[d_json[\"id\"]] = []\n",
    "        for pred_event_type in pred_event_types:\n",
    "            if pred_event_type not in sent_trigger_mapping[d_json[\"id\"]]:\n",
    "                sent_trigger_mapping[d_json[\"id\"]].append(pred_event_type)\n",
    "\n",
    "    output = []\n",
    "    for d_json in dataset:\n",
    "        _id = d_json[\"id\"]\n",
    "        text_a = [\n",
    "            \"，\" if t == \" \" or t == \"\\n\" or t == \"\\t\" else replace_control_chars(t)\n",
    "            for t in list(d_json[\"text\"].lower())\n",
    "        ]\n",
    "        for pred_event_type in sent_trigger_mapping.get(d_json[\"id\"], []):\n",
    "            trigger_text = pred_event_type[0] + f\"({pred_event_type[1]})：\"\n",
    "            text_trigger = [\n",
    "                \"，\" if t == \" \" or t == \"\\n\" or t == \"\\t\" else t\n",
    "                for t in list(trigger_text.lower())\n",
    "            ]\n",
    "            output.append({\n",
    "                \"id\": d_json[\"id\"],\n",
    "                \"sent_id\": d_json[\"sent_id\"],\n",
    "                \"org_text\": d_json[\"text\"],\n",
    "                \"text\": trigger_text + d_json[\"text\"],\n",
    "                \"event_type\": pred_event_type[0],\n",
    "                \"trigger\": pred_event_type[1],\n",
    "                \"tokens\": text_trigger+text_a\n",
    "            })\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784c7b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    'ernie-base': AutoTokenizer.from_pretrained(\"nghuyong/ernie-1.0\"),\n",
    "    'roberta-chinese-base': AutoTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext\"),\n",
    "    'roberta-chinese-large': AutoTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext-large\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0411af8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaiduEnermDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset_path):\n",
    "        self.examples = []\n",
    "        with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "            dataset = json.loads(f.read())\n",
    "            preprocess_dataset = enum_data_process(dataset)\n",
    "            for d_json in tqdm(preprocess_dataset, total=len(preprocess_dataset)):\n",
    "                text = d_json['text']\n",
    "                b_input_ids, b_attention_masks, b_token_type_ids = [], [], []\n",
    "                for tokenizer in model_dict.values():\n",
    "                    PADDING = tokenizer.vocab[tokenizer.pad_token]\n",
    "                    SEP = tokenizer.vocab[tokenizer.sep_token]\n",
    "                    input_ids = tokenizer(text, is_split_into_words=False, add_special_tokens=True, max_length=max_seq_len, truncation=True)['input_ids']\n",
    "                    tokens_input = input_ids + [PADDING] * (max_seq_len - len(input_ids))\n",
    "                    attention_masks = self._get_attention_mask(input_ids, max_seq_len)\n",
    "                    token_type_ids = self._get_token_type_id(input_ids, max_seq_len, sep_token=SEP)\n",
    "                    b_input_ids.append(tokens_input)\n",
    "                    b_attention_masks.append(attention_masks)\n",
    "                    b_token_type_ids.append(token_type_ids)\n",
    "                example = {\n",
    "                    \"input_ids\": b_input_ids, \"attention_masks\": b_attention_masks, \"token_type_ids\": b_token_type_ids\n",
    "                }\n",
    "                example.update(d_json)\n",
    "                self.examples.append(example)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, item_idx):\n",
    "        example = {\n",
    "            \"id\": self.examples[item_idx][\"id\"],\n",
    "            \"sent_id\": self.examples[item_idx][\"sent_id\"],\n",
    "            \"text\": self.examples[item_idx][\"text\"],\n",
    "            \"input_ids\": torch.tensor(self.examples[item_idx][\"input_ids\"]).long(),\n",
    "            \"attention_masks\": torch.tensor(self.examples[item_idx][\"attention_masks\"]),\n",
    "            \"token_type_ids\": torch.tensor(self.examples[item_idx][\"token_type_ids\"]),\n",
    "        }\n",
    "        return example\n",
    "\n",
    "    def _get_attention_mask(self, input_ids, max_seq_len):\n",
    "        \"\"\"Mask for padding.\"\"\"\n",
    "        if len(input_ids) > max_seq_len:\n",
    "            raise IndexError(\"Token length more than max seq length!\")\n",
    "        return [1] * len(input_ids) + [0] * (max_seq_len - len(input_ids))\n",
    "\n",
    "    def _get_token_type_id(self, input_ids, max_seq_len, sep_token):\n",
    "        \"\"\"Segments: 0 for the first sequence, 1 for the second.\"\"\"\n",
    "        if len(input_ids) > max_seq_len:\n",
    "            raise IndexError(\"Token length more than max seq length!\")\n",
    "        segments = []\n",
    "        current_segment_id = 0\n",
    "        for input_id in input_ids:\n",
    "            segments.append(current_segment_id)\n",
    "            if input_id == sep_token:\n",
    "                current_segment_id = 1\n",
    "        return segments + [0] * (max_seq_len - len(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486a6b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaiduTriggerDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset_path):\n",
    "        self.examples = []\n",
    "        with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "            dataset = json.loads(f.read())\n",
    "            preprocess_dataset = trigger_data_process(dataset)\n",
    "            for d_json in tqdm(preprocess_dataset, total=len(preprocess_dataset)):\n",
    "                tokens = d_json['tokens']\n",
    "                b_input_ids, b_attention_masks, b_token_type_ids = [], [], []\n",
    "                for tokenizer in model_dict.values():\n",
    "                    PADDING = tokenizer.vocab[tokenizer.pad_token]\n",
    "                    SEP = tokenizer.vocab[tokenizer.sep_token]\n",
    "                    input_ids = tokenizer(tokens, is_split_into_words=True, add_special_tokens=True, max_length=max_seq_len, truncation=True)['input_ids']\n",
    "                    tokens_input = input_ids + [PADDING] * (max_seq_len - len(input_ids))\n",
    "                    attention_masks = self._get_attention_mask(input_ids, max_seq_len)\n",
    "                    token_type_ids = self._get_token_type_id(input_ids, max_seq_len, sep_token=SEP)\n",
    "                    b_input_ids.append(tokens_input)\n",
    "                    b_attention_masks.append(attention_masks)\n",
    "                    b_token_type_ids.append(token_type_ids)\n",
    "                example = {\n",
    "                    \"input_ids\": b_input_ids, \"attention_masks\": b_attention_masks, \"token_type_ids\": b_token_type_ids, \"seq_lens\": len(tokens)\n",
    "                }\n",
    "                example.update(d_json)\n",
    "                self.examples.append(example)\n",
    "\n",
    "    def _get_attention_mask(self, input_ids, max_seq_len):\n",
    "        \"\"\"Mask for padding.\"\"\"\n",
    "        if len(input_ids) > max_seq_len:\n",
    "            raise IndexError(\"Token length more than max seq length!\")\n",
    "        return [1] * len(input_ids) + [0] * (max_seq_len - len(input_ids))\n",
    "\n",
    "    def _get_token_type_id(self, input_ids, max_seq_len, sep_token):\n",
    "        \"\"\"Segments: 0 for the first sequence, 1 for the second.\"\"\"\n",
    "        if len(input_ids) > max_seq_len:\n",
    "            raise IndexError(\"Token length more than max seq length!\")\n",
    "        segments = []\n",
    "        current_segment_id = 0\n",
    "        for input_id in input_ids:\n",
    "            segments.append(current_segment_id)\n",
    "            if input_id == sep_token:\n",
    "                current_segment_id = 1\n",
    "        return segments + [0] * (max_seq_len - len(input_ids))       \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, item_idx):\n",
    "        example = {\n",
    "            \"id\": self.examples[item_idx][\"id\"],\n",
    "            \"sent_id\": self.examples[item_idx][\"sent_id\"],\n",
    "            \"text\": self.examples[item_idx][\"text\"],\n",
    "            \"input_ids\": torch.tensor(self.examples[item_idx][\"input_ids\"]).long(),\n",
    "            \"attention_masks\": torch.tensor(self.examples[item_idx][\"attention_masks\"]),\n",
    "            \"token_type_ids\": torch.tensor(self.examples[item_idx][\"token_type_ids\"]),\n",
    "            \"seq_lens\": self.examples[item_idx][\"seq_lens\"],\n",
    "        }\n",
    "        return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d34e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaiduRoleDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset_path, trigger_file_path):\n",
    "        self.examples = []\n",
    "        with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "            dataset = json.loads(f.read())\n",
    "            preprocess_dataset = role_data_process(dataset, trigger_file_path=trigger_file_path)\n",
    "            for d_json in tqdm(preprocess_dataset, total=len(preprocess_dataset)):\n",
    "                tokens = d_json['tokens']\n",
    "                b_input_ids, b_attention_masks, b_token_type_ids = [], [], []\n",
    "                for tokenizer in model_dict.values():\n",
    "                    PADDING = tokenizer.vocab[tokenizer.pad_token]\n",
    "                    SEP = tokenizer.vocab[tokenizer.sep_token]\n",
    "                    input_ids = tokenizer(tokens, is_split_into_words=True, add_special_tokens=True, max_length=max_seq_len, truncation=True)['input_ids']\n",
    "                    tokens_input = input_ids + [PADDING] * (max_seq_len - len(input_ids))\n",
    "                    attention_masks = self._get_attention_mask(input_ids, max_seq_len)\n",
    "                    token_type_ids = self._get_token_type_id(input_ids, max_seq_len, sep_token=SEP)\n",
    "                    b_input_ids.append(tokens_input)\n",
    "                    b_attention_masks.append(attention_masks)\n",
    "                    b_token_type_ids.append(token_type_ids)\n",
    "                example = {\n",
    "                    \"input_ids\": b_input_ids, \"attention_masks\": b_attention_masks, \"token_type_ids\": b_token_type_ids, \"seq_lens\": len(tokens)\n",
    "                }\n",
    "                example.update(d_json)\n",
    "                self.examples.append(example)\n",
    "\n",
    "    def _get_attention_mask(self, input_ids, max_seq_len):\n",
    "        \"\"\"Mask for padding.\"\"\"\n",
    "        if len(input_ids) > max_seq_len:\n",
    "            raise IndexError(\"Token length more than max seq length!\")\n",
    "        return [1] * len(input_ids) + [0] * (max_seq_len - len(input_ids))\n",
    "\n",
    "    def _get_token_type_id(self, input_ids, max_seq_len, sep_token):\n",
    "        \"\"\"Segments: 0 for the first sequence, 1 for the second.\"\"\"\n",
    "        if len(input_ids) > max_seq_len:\n",
    "            raise IndexError(\"Token length more than max seq length!\")\n",
    "        segments = []\n",
    "        current_segment_id = 0\n",
    "        for input_id in input_ids:\n",
    "            segments.append(current_segment_id)\n",
    "            if input_id == sep_token:\n",
    "                current_segment_id = 1\n",
    "        return segments + [0] * (max_seq_len - len(input_ids))        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, item_idx):\n",
    "        example = {\n",
    "            \"id\": self.examples[item_idx][\"id\"],\n",
    "            \"sent_id\": self.examples[item_idx][\"sent_id\"],\n",
    "            \"event_type\": self.examples[item_idx][\"event_type\"],\n",
    "            \"trigger\": self.examples[item_idx][\"trigger\"],\n",
    "            \"text\": self.examples[item_idx][\"text\"],\n",
    "            \"input_ids\": torch.tensor(self.examples[item_idx][\"input_ids\"]).long(),\n",
    "            \"attention_masks\": torch.tensor(self.examples[item_idx][\"attention_masks\"]),\n",
    "            \"token_type_ids\": torch.tensor(self.examples[item_idx][\"token_type_ids\"]),\n",
    "            \"seq_lens\": self.examples[item_idx][\"seq_lens\"],\n",
    "        }\n",
    "        return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83d75c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_enerm_dataset = BaiduEnermDataset(dataset_path=duee_fin_test_preprocess_path)\n",
    "test_trigger_dataset = BaiduTriggerDataset(dataset_path=duee_fin_test_preprocess_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d4256b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_enerm_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d250f8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_trigger_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126fbe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed = 42):\n",
    "    \"\"\"Set the seed for generating random numbers on all GPUs.\n",
    "\n",
    "    It's safe to call this function if CUDA is not available; in that case, it is silently ignored.\n",
    "\n",
    "    Args:\n",
    "        seed (int, optional): random numbers on all GPUs. Defaults to 42.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5b9fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device(f'cuda:{device_Id}' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    \n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', torch.cuda.memory_allocated(0)/1024**3, 'GB')\n",
    "    print('Cached:   ', torch.cuda.memory_reserved(0)/1024**3, 'GB')\n",
    "    \n",
    "    print('CUDA Device Count:', n_gpu)\n",
    "    \n",
    "set_seed(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e7449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_by_lines(path, data):\n",
    "    \"\"\"write the data\"\"\"\n",
    "    with open(path, \"w\") as outfile:\n",
    "        [outfile.write(d + \"\\n\") for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb751c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_enerm(model, test_dataloader):\n",
    "    model.eval()\n",
    "    step = 0\n",
    "    eval_acc = 0.0\n",
    "    eval_f1 = 0.0\n",
    "    eval_loss = 0.0\n",
    "    results = []\n",
    "    test_iterator = tqdm(test_dataloader)\n",
    "    for batch in test_iterator:\n",
    "        _, logits = model(\n",
    "            input_ids=batch['input_ids'].to(device),\n",
    "            attention_mask=batch['attention_masks'].to(device),\n",
    "            token_type_ids=batch['token_type_ids'].to(device)\n",
    "        )\n",
    "\n",
    "        probs = torch.sigmoid(logits).data.cpu()\n",
    "        probs_ids = (probs > 0.5).numpy()\n",
    "        probs = probs.numpy()\n",
    "        for id_, sent_id, text, label_probs, p_id in zip(batch['id'], batch['sent_id'], batch['text'], probs.tolist(), probs_ids.tolist()):\n",
    "            true_indices = np.argwhere(p_id).flatten()\n",
    "            labels = [id2enumlabel[true_index] for true_index in true_indices]\n",
    "            results.append({\"id\": id_, \"sent_id\": sent_id, \"text\": text, \"pred\":{\"probs\": label_probs, \"label\": labels}})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e822ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_trigger(model, test_dataloader):\n",
    "    model.eval()\n",
    "    step = 0\n",
    "    eval_acc = 0.0\n",
    "    eval_f1 = 0.0\n",
    "    eval_loss = 0.0\n",
    "    results = []\n",
    "    test_iterator = tqdm(test_dataloader)\n",
    "    for batch in test_iterator:\n",
    "        _, logits = model(\n",
    "            input_ids=batch['input_ids'].to(device),\n",
    "            attention_mask=batch['attention_masks'].to(device),\n",
    "            token_type_ids=batch['token_type_ids'].to(device)\n",
    "        )\n",
    "\n",
    "        probs = torch.sigmoid(logits).data.cpu()\n",
    "        probs_ids = (probs > 0.1).numpy()\n",
    "        probs = probs.numpy()\n",
    "        for id_, sent_id, text, p_list, p_ids, seq_len in zip(batch['id'], batch['sent_id'], batch['text'], probs.tolist(), probs_ids.tolist(), batch['seq_lens']):\n",
    "            prob_multi, label_multi = [], []\n",
    "            for index, pid in enumerate(p_ids[1: seq_len - 1]):\n",
    "                true_indices = np.argwhere(pid).flatten()\n",
    "                prob_multi.append(p_list[index])\n",
    "                label_multi.append([id2triggerlabel[true_index] for true_index in true_indices])\n",
    "            results.append({\"id\": id_, \"sent_id\":sent_id, \"text\": text, \"pred\": {\"probs\": prob_multi, \"labels\": label_multi}})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44ffadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_role(model, test_dataloader):\n",
    "    model.eval()\n",
    "    step = 0\n",
    "    eval_acc = 0.0\n",
    "    eval_f1 = 0.0\n",
    "    eval_loss = 0.0\n",
    "    results = []\n",
    "    test_iterator = tqdm(test_dataloader)\n",
    "    for batch in test_iterator:\n",
    "        _, logits = model(\n",
    "            input_ids=batch['input_ids'].to(device),\n",
    "            attention_mask=batch['attention_masks'].to(device),\n",
    "            token_type_ids=batch['token_type_ids'].to(device)\n",
    "        )\n",
    "\n",
    "        probs = torch.sigmoid(logits).data.cpu()\n",
    "        probs_ids = (probs > 0.1).numpy()\n",
    "        probs = probs.numpy()\n",
    "        for id_, sent_id, text, event_type, trigger, p_list, p_ids, seq_len in zip(batch['id'], batch['sent_id'], batch['text'], batch['event_type'], batch['trigger'], probs.tolist(), probs_ids.tolist(), batch['seq_lens']):\n",
    "            prob_multi, label_multi = [], []\n",
    "            for index, pid in enumerate(p_ids[1: seq_len - 1]):\n",
    "                true_indices = np.argwhere(pid).flatten()\n",
    "                prob_multi.append(p_list[index])\n",
    "                label_multi.append([id2rolelabel[true_index] for true_index in true_indices])\n",
    "            results.append({\"id\": id_, \"sent_id\":sent_id, \"event_type\": event_type, \"trigger\": trigger, \"text\": text, \"pred\": {\"probs\": prob_multi, \"labels\": label_multi}})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37388d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_enerm_sampler = SequentialSampler(test_enerm_dataset)\n",
    "test_enerm_dataloader = DataLoader(test_enerm_dataset, sampler=test_enerm_sampler, batch_size = 64)\n",
    "\n",
    "test_trigger_sampler = SequentialSampler(test_trigger_dataset)\n",
    "test_trigger_dataloader = DataLoader(test_trigger_dataset, sampler=test_trigger_sampler, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e5e932",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stackedModel import MultiLabelStackedClassification\n",
    "\n",
    "models = [\n",
    "    torch.load(f'./models/DuEE_fin/{model_name}/enum.bin').to(device)\n",
    "    for model_name in model_dict.keys()\n",
    "]\n",
    "enum_model = MultiLabelStackedClassification(models=models, num_labels=len(label_enum_vocab))\n",
    "enum_model.load_state_dict(torch.load(enerm_stacked_model_path))\n",
    "\n",
    "enum_model = enum_model.to(device)\n",
    "\n",
    "models = [\n",
    "    torch.load(f'./models/DuEE_fin/{model_name}/trigger-multilabel.bin').to(device)\n",
    "    for model_name in model_dict.keys()\n",
    "]\n",
    "tigger_model = MultiLabelStackedClassification(models=models, num_labels=len(label_trigger_vocab))\n",
    "tigger_model.load_state_dict(torch.load(tigger_stacked_model_path))\n",
    "\n",
    "tigger_model = tigger_model.to(device)\n",
    "\n",
    "\n",
    "models = [\n",
    "    torch.load(f'./models/DuEE_fin/{model_name}/role-multilabel-trick1.bin').to(device)\n",
    "    for model_name in model_dict.keys()\n",
    "]\n",
    "role_model = MultiLabelStackedClassification(models=models, num_labels=len(label_role_vocab))\n",
    "role_model.load_state_dict(torch.load(role_stacked_model_path))\n",
    "\n",
    "role_model = role_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7e770f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_enerm = test_enerm(enum_model, test_enerm_dataloader)\n",
    "sentences_enerm = [json.dumps(sent_enerm, ensure_ascii=False) for sent_enerm in sentences_enerm]\n",
    "write_by_lines('./predict/DuEE_fin_test2/enerm/test_pred.json', sentences_enerm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f89af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_tigger = test_trigger(tigger_model, test_trigger_dataloader)\n",
    "sentences_tigger = [json.dumps(sent_tigger, ensure_ascii=False) for sent_tigger in sentences_tigger]\n",
    "write_by_lines('./predict/DuEE_fin_test2/trigger/test_pred.json', sentences_tigger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0ac0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_role_dataset = BaiduRoleDataset(dataset_path=duee_fin_test_preprocess_path, trigger_file_path='./predict/DuEE_fin_test2/trigger/test_pred.json')\n",
    "\n",
    "test_role_sampler = SequentialSampler(test_role_dataset)\n",
    "test_role_dataloader = DataLoader(test_role_dataset, sampler=test_role_sampler, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51ab620",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_role_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cfb008",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_role = test_role(role_model, test_role_dataloader)\n",
    "sentences_role = [json.dumps(sent_role, ensure_ascii=False) for sent_role in sentences_role]\n",
    "write_by_lines('./predict/DuEE_fin_test2/role/test_pred.json', sentences_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1052a74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### multilabel classfication with header trick use \n",
    "\n",
    "!python duee_fin_postprocess_trick1.py --trigger_file ./predict/DuEE_fin_test2/trigger/test_pred.json \\\n",
    "    --role_file ./predict/DuEE_fin_test2/role/test_pred.json \\\n",
    "    --enum_file ./predict/DuEE_fin_test2/enerm/test_pred.json --schema_file ./dictionary/event_schema.json \\\n",
    "    --save_path ./submit/DuEE_fin_test2/test_duee_fin_erine_multilabel-trick1-fix.json --multilabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b19e23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
