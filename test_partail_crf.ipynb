{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9898ccaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "from abc import abstractmethod\n",
    "\n",
    "IMPOSSIBLE_SCORE = -10000000.0\n",
    "\n",
    "def log_sum_exp(tensor: torch.Tensor, dim: int = -1, keepdim: bool = False) -> torch.Tensor:\n",
    "    max_score, _ = tensor.max(dim, keepdim=keepdim)\n",
    "    if keepdim:\n",
    "        stable_vec = tensor - max_score\n",
    "    else:\n",
    "        stable_vec = tensor - max_score.unsqueeze(dim)\n",
    "    return max_score + (stable_vec.exp().sum(dim, keepdim=keepdim)).log()\n",
    "\n",
    "class BaseCRF(nn.Module):\n",
    "    \"\"\"BaseCRF\n",
    "    \"\"\"\n",
    "    def __init__(self, num_tags, padding_idx = None) -> None:\n",
    "        super().__init__()\n",
    "        self.num_tags = num_tags\n",
    "        self.start_transitions = nn.Parameter(torch.randn(num_tags))\n",
    "        self.end_transitions = nn.Parameter(torch.randn(num_tags))\n",
    "        init_transition = torch.randn(num_tags, num_tags)\n",
    "        if padding_idx is not None:\n",
    "            init_transition[:, padding_idx] = IMPOSSIBLE_SCORE\n",
    "            init_transition[padding_idx, :] = IMPOSSIBLE_SCORE\n",
    "        self.transitions = nn.Parameter(init_transition)\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, emissions, tags,mask = None) -> torch.Tensor:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def marginal_probabilities(self, emissions, mask = None) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            emissions: (batch_size, sequence_length, num_tags)\n",
    "            mask:  Show padding tags. 0 don't calculate score. (batch_size, sequence_length)\n",
    "        Returns:\n",
    "            marginal_probabilities: (sequence_length, sequence_length, num_tags)\n",
    "        \"\"\"\n",
    "        if mask is None:\n",
    "            batch_size, sequence_length, _ = emissions.data.shape\n",
    "            mask = torch.ones([batch_size, sequence_length], dtype=torch.uint8, device=emissions.device)\n",
    "\n",
    "        alpha = self._forward_algorithm(emissions, \n",
    "                                        mask, \n",
    "                                        reverse_direction = False)\n",
    "        beta = self._forward_algorithm(emissions, \n",
    "                                        mask, \n",
    "                                        reverse_direction = True)\n",
    "        z = log_sum_exp(alpha[alpha.size(0) - 1] + self.end_transitions, dim = 1)\n",
    "\n",
    "        proba = alpha + beta - z.view(1, -1, 1)\n",
    "        return torch.exp(proba)\n",
    "\n",
    "    def _forward_algorithm(self, emissions, mask, reverse_direction = False) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            emissions: (batch_size, sequence_length, num_tags)\n",
    "            mask:  Show padding tags. 0 don't calculate score. (batch_size, sequence_length)\n",
    "            reverse_direction: This parameter decide algorithm direction.\n",
    "        Returns:\n",
    "            log_probabilities: (sequence_length, batch_size, num_tags)\n",
    "        \"\"\"\n",
    "        batch_size, sequence_length, num_tags = emissions.data.shape\n",
    "\n",
    "        broadcast_emissions = emissions.transpose(0, 1).unsqueeze(2).contiguous() # (sequence_length, batch_size, 1, num_tags)\n",
    "        mask = mask.float().transpose(0, 1).contiguous()                          # (sequence_length, batch_size)\n",
    "        broadcast_transitions = self.transitions.unsqueeze(0)                     # (1, num_tags, num_tags)\n",
    "        sequence_iter = range(1, sequence_length)\n",
    "\n",
    "        # backward algorithm\n",
    "        if reverse_direction:\n",
    "            # Transpose transitions matrix and emissions\n",
    "            broadcast_transitions = broadcast_transitions.transpose(1, 2)         # (1, num_tags, num_tags)\n",
    "            broadcast_emissions = broadcast_emissions.transpose(2, 3)             # (sequence_length, batch_size, num_tags, 1)\n",
    "            sequence_iter = reversed(sequence_iter)\n",
    "\n",
    "            # It is beta\n",
    "            log_proba = [self.end_transitions.expand(batch_size, num_tags)]\n",
    "        # forward algorithm\n",
    "        else:\n",
    "            # It is alpha\n",
    "            log_proba = [emissions.transpose(0, 1)[0] + self.start_transitions.view(1, -1)]\n",
    "\n",
    "        for i in sequence_iter:\n",
    "            # Broadcast log probability\n",
    "            broadcast_log_proba = log_proba[-1].unsqueeze(2) # (batch_size, num_tags, 1)\n",
    "\n",
    "            # Add all scores\n",
    "            # inner: (batch_size, num_tags, num_tags)\n",
    "            # broadcast_log_proba:   (batch_size, num_tags, 1)\n",
    "            # broadcast_transitions: (1, num_tags, num_tags)\n",
    "            # broadcast_emissions:   (batch_size, 1, num_tags)\n",
    "            inner = broadcast_log_proba \\\n",
    "                    + broadcast_transitions \\\n",
    "                    + broadcast_emissions[i]\n",
    "\n",
    "            # Append log proba\n",
    "            log_proba.append((log_sum_exp(inner, 1) * mask[i].view(batch_size, 1) +\n",
    "                     log_proba[-1] * (1 - mask[i]).view(batch_size, 1)))\n",
    "\n",
    "        if reverse_direction:\n",
    "            log_proba.reverse()\n",
    "\n",
    "        return torch.stack(log_proba)\n",
    "\n",
    "    def viterbi_decode(self, emissions, mask = None) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            emissions: (batch_size, sequence_length, num_tags)\n",
    "            mask:  Show padding tags. 0 don't calculate score. (batch_size, sequence_length)\n",
    "        Returns:\n",
    "            tags: (batch_size)\n",
    "        \"\"\"\n",
    "        batch_size, sequence_length, _ = emissions.shape\n",
    "        if mask is None:\n",
    "            mask = torch.ones([batch_size, sequence_length], dtype=torch.uint8, device=emissions.device)\n",
    "\n",
    "        emissions = emissions.transpose(0, 1).contiguous()\n",
    "        mask = mask.transpose(0, 1).contiguous()\n",
    "\n",
    "        # Start transition and first emission score\n",
    "        score = self.start_transitions + emissions[0]\n",
    "        history = []\n",
    "\n",
    "        for i in range(1, sequence_length):\n",
    "            broadcast_score = score.unsqueeze(2)\n",
    "            broadcast_emissions = emissions[i].unsqueeze(1)\n",
    "\n",
    "            next_score = broadcast_score + self.transitions + broadcast_emissions\n",
    "            next_score, indices = next_score.max(dim = 1)\n",
    "\n",
    "            score = torch.where(mask[i].unsqueeze(1), next_score, score)\n",
    "            history.append(indices)\n",
    "\n",
    "        # Add end transition score\n",
    "        score += self.end_transitions\n",
    "\n",
    "        # Compute the best path\n",
    "        seq_ends = mask.long().sum(dim = 0) - 1\n",
    "\n",
    "        best_tags_list = []\n",
    "        for i in range(batch_size):\n",
    "            _, best_last_tag = score[i].max(dim = 0)\n",
    "            best_tags = [best_last_tag.item()]\n",
    "\n",
    "            for hist in reversed(history[:seq_ends[i]]):\n",
    "                best_last_tag = hist[i][best_tags[-1]]\n",
    "                best_tags.append(best_last_tag.item())\n",
    "\n",
    "            best_tags.reverse()\n",
    "            best_tags_list.append(best_tags)\n",
    "\n",
    "        return best_tags_list\n",
    "\n",
    "    def restricted_viterbi_decode(self, emissions, possible_tags, mask = None) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            emissions: (batch_size, sequence_length, num_tags)\n",
    "            possible_tags: (batch_size, sequence_length, num_tags)\n",
    "            mask: Show padding tags. 0 don't calculate score. (batch_size, sequence_length)\n",
    "        Returns:\n",
    "            tags: (batch_size)\n",
    "        \"\"\"\n",
    "        batch_size, sequence_length, num_tags = emissions.data.shape\n",
    "        if mask is None:\n",
    "            mask = torch.ones([batch_size, sequence_length], dtype=torch.uint8, device=emissions.device)\n",
    "\n",
    "        emissions = emissions.transpose(0, 1).contiguous()\n",
    "        mask = mask.transpose(0, 1).contiguous()\n",
    "        possible_tags = possible_tags.float().transpose(0, 1).contiguous()\n",
    "\n",
    "        # Start transition score and first emission\n",
    "        first_possible_tag = possible_tags[0]\n",
    "\n",
    "        score = self.start_transitions + emissions[0]      # (batch_size, num_tags)\n",
    "        score[(first_possible_tag == 0)] = IMPOSSIBLE_SCORE\n",
    "\n",
    "        history = []\n",
    "\n",
    "        for i in range(1, sequence_length):\n",
    "            current_possible_tags = possible_tags[i-1]\n",
    "            next_possible_tags = possible_tags[i]\n",
    "            \n",
    "            # Feature score\n",
    "            emissions_score = emissions[i]\n",
    "            emissions_score[(next_possible_tags == 0)] = IMPOSSIBLE_SCORE\n",
    "            emissions_score = emissions_score.view(batch_size, 1, num_tags)\n",
    "\n",
    "            # Transition score\n",
    "            transition_scores = self.transitions.view(1, num_tags, num_tags).expand(batch_size, num_tags, num_tags).clone()\n",
    "            transition_scores[(current_possible_tags == 0)] = IMPOSSIBLE_SCORE\n",
    "            transition_scores.transpose(1, 2)[(next_possible_tags == 0)] = IMPOSSIBLE_SCORE\n",
    "\n",
    "            broadcast_score = score.view(batch_size, num_tags, 1)\n",
    "            next_score = broadcast_score + transition_scores + emissions_score\n",
    "            next_score, indices = next_score.max(dim=1)\n",
    "\n",
    "            score = torch.where(mask[i].unsqueeze(1), next_score, score)\n",
    "            history.append(indices)\n",
    "\n",
    "        # Add end transition score\n",
    "        score += self.end_transitions\n",
    "\n",
    "        # Compute the best path for each sample\n",
    "        seq_ends = mask.long().sum(dim=0) - 1\n",
    "        max_len = int(seq_ends[0])\n",
    "        best_tags_list = []\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            _, best_last_tag = score[idx].max(dim=0)\n",
    "            best_tags = [best_last_tag.item()]\n",
    "\n",
    "            for hist in reversed(history[:seq_ends[idx]]):\n",
    "                best_last_tag = hist[idx][best_tags[-1]]\n",
    "                best_tags.append(best_last_tag.item())\n",
    "\n",
    "            best_tags.reverse()\n",
    "            best_tags_list.append(best_tags)\n",
    "\n",
    "        return best_tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec87a7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartialCRF(BaseCRF):\n",
    "    \"\"\"Partial/Fuzzy Conditional random field.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_tags: int, padding_idx: int = None) -> None:\n",
    "        super().__init__(num_tags, padding_idx)\n",
    "\n",
    "    def _reset_parameters(self) -> None:\n",
    "        nn.init.uniform_(self.start_transitions, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.end_transitions, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.transitions, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, emissions, possible_tags, mask = None) -> torch.Tensor:\n",
    "        batch_size, sequence_length, num_tags = emissions.data.shape\n",
    "        \n",
    "        if mask is None:\n",
    "            mask = torch.ones([batch_size, sequence_length], dtype=torch.uint8, device=emissions.device)\n",
    "\n",
    "        gold_score = self._numerator_score(emissions, mask, possible_tags)\n",
    "        forward_score = self._denominator_score(emissions, mask)\n",
    "        return torch.sum(forward_score - gold_score)\n",
    "\n",
    "    def _denominator_score(self, emissions, mask) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            emissions: (batch_size, sequence_length, num_tags)\n",
    "            mask: Show padding tags. 0 don't calculate score. (batch_size, sequence_length)\n",
    "        Returns:\n",
    "            scores: (batch_size)\n",
    "        \"\"\"\n",
    "        batch_size, sequence_length, num_tags = emissions.data.shape\n",
    "\n",
    "        emissions = emissions.transpose(0, 1).contiguous()\n",
    "        mask = mask.float().transpose(0, 1).contiguous()\n",
    "\n",
    "        # Start transition score and first emissions score\n",
    "        alpha = self.start_transitions.view(1, num_tags) + emissions[0]\n",
    "\n",
    "        for i in range(1, sequence_length):\n",
    "\n",
    "            emissions_score = emissions[i].view(batch_size, 1, num_tags)      # (batch_size, 1, num_tags)\n",
    "            transition_scores = self.transitions.view(1, num_tags, num_tags)  # (1, num_tags, num_tags)\n",
    "            broadcast_alpha = alpha.view(batch_size, num_tags, 1)             # (batch_size, num_tags, 1)\n",
    "\n",
    "            inner = broadcast_alpha + emissions_score + transition_scores     # (batch_size, num_tags, num_tags)\n",
    "\n",
    "            alpha = (log_sum_exp(inner, 1) * mask[i].view(batch_size, 1) +\n",
    "                     alpha * (1 - mask[i]).view(batch_size, 1))\n",
    "\n",
    "        # Add end transition score\n",
    "        stops = alpha + self.end_transitions.view(1, num_tags)\n",
    "\n",
    "        return log_sum_exp(stops) # (batch_size,)\n",
    "\n",
    "    def _numerator_score(self, emissions, mask, possible_tags) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            emissions: (batch_size, sequence_length, num_tags)\n",
    "            tags:  (batch_size, sequence_length)\n",
    "            mask:  Show padding tags. 0 don't calculate score. (batch_size, sequence_length)\n",
    "        Returns:\n",
    "            scores: (batch_size)\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, sequence_length, num_tags = emissions.data.shape\n",
    "\n",
    "        emissions = emissions.transpose(0, 1).contiguous()\n",
    "        mask = mask.float().transpose(0, 1).contiguous()\n",
    "        possible_tags = possible_tags.float().transpose(0, 1)\n",
    "\n",
    "        # Start transition score and first emission\n",
    "        first_possible_tag = possible_tags[0]\n",
    "        \n",
    "        alpha = self.start_transitions + emissions[0]      # (batch_size, num_tags)\n",
    "        alpha[(first_possible_tag == 0)] = IMPOSSIBLE_SCORE\n",
    "\n",
    "        for i in range(1, sequence_length):\n",
    "            current_possible_tags = possible_tags[i-1] # (batch_size, num_tags)\n",
    "            next_possible_tags = possible_tags[i]      # (batch_size, num_tags)\n",
    "\n",
    "            # Emissions scores\n",
    "            emissions_score = emissions[i]\n",
    "            emissions_score[(next_possible_tags == 0)] = IMPOSSIBLE_SCORE\n",
    "            emissions_score = emissions_score.view(batch_size, 1, num_tags)\n",
    "\n",
    "            # Transition scores\n",
    "            transition_scores = self.transitions.view(1, num_tags, num_tags).expand(batch_size, num_tags, num_tags).clone()\n",
    "            transition_scores[(current_possible_tags == 0)] = IMPOSSIBLE_SCORE\n",
    "            transition_scores.transpose(1, 2)[(next_possible_tags == 0)] = IMPOSSIBLE_SCORE\n",
    "\n",
    "            # Broadcast alpha\n",
    "            broadcast_alpha = alpha.view(batch_size, num_tags, 1)\n",
    "\n",
    "            # Add all scores\n",
    "            inner = broadcast_alpha + emissions_score + transition_scores # (batch_size, num_tags, num_tags)\n",
    "            print(mask[i])\n",
    "            alpha = (log_sum_exp(inner, 1) * mask[i].view(batch_size, 1) +\n",
    "                     alpha * (1 - mask[i]).view(batch_size, 1))\n",
    "\n",
    "        # Add end transition score\n",
    "        last_tag_indexes = mask.sum(0).long() - 1\n",
    "        end_transitions = self.end_transitions.expand(batch_size, num_tags) \\\n",
    "                            * possible_tags.transpose(0, 1).view(sequence_length * batch_size, num_tags)[last_tag_indexes + torch.arange(batch_size, device=possible_tags.device) * sequence_length]\n",
    "        end_transitions[(end_transitions == 0)] = IMPOSSIBLE_SCORE\n",
    "        stops = alpha + end_transitions\n",
    "\n",
    "        return log_sum_exp(stops) # (batch_size,)\n",
    "\n",
    "    def _forward_algorithm(self, emissions, mask, reverse_direction = False) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            emissions: (batch_size, sequence_length, num_tags)\n",
    "            tags:  (batch_size, sequence_length)\n",
    "            mask:  Show padding tags. 0 don't calculate score. (batch_size, sequence_length)\n",
    "            reverse: This parameter decide algorithm direction.\n",
    "        Returns:\n",
    "            log_probabilities: (sequence_length, batch_size, num_tags)\n",
    "        \"\"\"\n",
    "        batch_size, sequence_length, num_tags = emissions.data.shape\n",
    "\n",
    "        broadcast_emissions = emissions.transpose(0, 1).unsqueeze(2).contiguous() # (sequence_length, batch_size, 1, num_tags)\n",
    "        mask = mask.float().transpose(0, 1).contiguous()                          # (sequence_length, batch_size)\n",
    "        broadcast_transitions = self.transitions.unsqueeze(0)                     # (1, num_tags, num_tags)\n",
    "        sequence_iter = range(1, sequence_length)\n",
    "\n",
    "        # backward algorithm\n",
    "        if reverse_direction:\n",
    "            # Transpose transitions matrix and emissions\n",
    "            broadcast_transitions = broadcast_transitions.transpose(1, 2)         # (1, num_tags, num_tags)\n",
    "            broadcast_emissions = broadcast_emissions.transpose(2, 3)             # (sequence_length, batch_size, num_tags, 1)\n",
    "            sequence_iter = reversed(sequence_iter)\n",
    "\n",
    "            # It is beta\n",
    "            log_proba = [self.end_transitions.expand(batch_size, num_tags)]\n",
    "        # forward algorithm\n",
    "        else:\n",
    "            # It is alpha\n",
    "            log_proba = [emissions.transpose(0, 1)[0] + self.start_transitions.view(1, -1)]\n",
    "\n",
    "        for i in sequence_iter:\n",
    "            # Broadcast log probability\n",
    "            broadcast_log_proba = log_proba[-1].unsqueeze(2) # (batch_size, num_tags, 1)\n",
    "\n",
    "            # Add all scores\n",
    "            # inner: (batch_size, num_tags, num_tags)\n",
    "            # broadcast_log_proba:   (batch_size, num_tags, 1)\n",
    "            # broadcast_transitions: (1, num_tags, num_tags)\n",
    "            # broadcast_emissions:   (batch_size, 1, num_tags)\n",
    "            inner = broadcast_log_proba \\\n",
    "                    + broadcast_transitions \\\n",
    "                    + broadcast_emissions[i]\n",
    "\n",
    "            # Append log proba\n",
    "            log_proba.append((log_sum_exp(inner, 1) * mask[i].view(batch_size, 1) +\n",
    "                     log_proba[-1] * (1 - mask[i]).view(batch_size, 1)))\n",
    "\n",
    "        if reverse_direction:\n",
    "            log_proba.reverse()\n",
    "\n",
    "        return torch.stack(log_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eaffaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot_vector(labels, num_tags):\n",
    "    \"\"\"Convert seq to one hot.\"\"\"\n",
    "    one_hot_vectors = []\n",
    "    for label in labels:\n",
    "        one_hot_vector = np.zeros(num_tags)\n",
    "        if isinstance(label, int):\n",
    "            one_hot_vector[label] = 1\n",
    "        elif isinstance(label, list):\n",
    "            for l in label:\n",
    "                one_hot_vector[label] = 1\n",
    "        one_hot_vectors.append(one_hot_vector)\n",
    "    return np.array(one_hot_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d867300a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tags = 6\n",
    "batch_size, sequence_length = 3, 5\n",
    "emissions = torch.randn(batch_size, sequence_length, num_tags)\n",
    "\n",
    "tags = [\n",
    "    [1, 2, [1, 3], 3, 5],\n",
    "    [1, 3, -1, 2, 1],\n",
    "    [1, 0, 2, 4, 4],\n",
    "]\n",
    "\n",
    "possible_tags = []\n",
    "for tag in tags:\n",
    "    possible_tags.append(to_one_hot_vector(tag, num_tags))\n",
    "    \n",
    "possible_tags = torch.ByteTensor(possible_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "220f2011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.])\n",
      "tensor([1., 1., 1.])\n",
      "tensor([1., 1., 1.])\n",
      "tensor([1., 1., 1.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(31.0716, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PartialCRF(num_tags)\n",
    "\n",
    "model(emissions, possible_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cbb7735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4338, 0.2106, 0.0220, 0.2537, 0.0390, 0.0408],\n",
       "         [0.0526, 0.0272, 0.0217, 0.2958, 0.0068, 0.5959],\n",
       "         [0.5227, 0.0515, 0.0119, 0.1838, 0.0144, 0.2158]],\n",
       "\n",
       "        [[0.1523, 0.1030, 0.0903, 0.0471, 0.0414, 0.5660],\n",
       "         [0.0828, 0.0106, 0.0349, 0.6984, 0.0834, 0.0899],\n",
       "         [0.0349, 0.0243, 0.1356, 0.1067, 0.0171, 0.6814]],\n",
       "\n",
       "        [[0.1409, 0.0588, 0.0807, 0.5712, 0.0510, 0.0974],\n",
       "         [0.0654, 0.0116, 0.2265, 0.1768, 0.1516, 0.3682],\n",
       "         [0.0547, 0.0037, 0.6617, 0.2276, 0.0102, 0.0420]],\n",
       "\n",
       "        [[0.0081, 0.0440, 0.0938, 0.0063, 0.0164, 0.8315],\n",
       "         [0.0863, 0.0099, 0.2116, 0.4349, 0.2094, 0.0480],\n",
       "         [0.0834, 0.1637, 0.0348, 0.4550, 0.2271, 0.0360]],\n",
       "\n",
       "        [[0.0728, 0.0618, 0.0203, 0.7128, 0.0757, 0.0565],\n",
       "         [0.0284, 0.2669, 0.0191, 0.1860, 0.4128, 0.0869],\n",
       "         [0.0351, 0.3993, 0.0483, 0.0671, 0.0950, 0.3552]]],\n",
       "       grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.marginal_probabilities(emissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d6973c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
