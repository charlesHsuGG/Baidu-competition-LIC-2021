{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e5761e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel\n",
    "\n",
    "from crf_layer import CRFLayer\n",
    "from multiLabelTokenClassfication import MultiLabelTokenClassification\n",
    "\n",
    "from utils import read_by_lines, extract_result_multilabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04abdc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = \"roberta-chinese-base\"\n",
    "tokenizer_model = \"hfl/chinese-roberta-wwm-ext\"\n",
    "\n",
    "shema_path = './dictionary/event_schema.json'\n",
    "enerm_dict_path = './dictionary/enum_tag.dict'\n",
    "trigger_dict_path = './dictionary/trigger_tag.dict'\n",
    "role_dict_path = './dictionary/role_tag.dict'\n",
    "\n",
    "enerm_model_path = f'./models/DuEE_fin/{folder_name}/enum.bin'\n",
    "tigger_model_path = f'./models/DuEE_fin/{folder_name}/trigger-multilabel.bin'\n",
    "role_model_path = f'./models/DuEE_fin/{folder_name}/role-multilabel-trick1.bin'\n",
    "\n",
    "duee_fin_test_preprocess_path = './resources/duee_fin_test_preprocess.json'\n",
    "\n",
    "enum_role = \"环节\"\n",
    "enum_event_type = \"公司上市\"\n",
    "max_seq_len = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f5b25c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(dict_path):\n",
    "    \"\"\"load_dict\"\"\"\n",
    "    vocab = {}\n",
    "    for line in open(dict_path, 'r', encoding='utf-8'):\n",
    "        value, key = line.strip('\\n').split('\\t')\n",
    "        vocab[key] = int(value)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df86af26",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_enum_vocab = load_dict(dict_path=enerm_dict_path)\n",
    "id2enumlabel = {val: key for key, val in label_enum_vocab.items()}\n",
    "label_trigger_vocab = load_dict(dict_path=trigger_dict_path)\n",
    "id2triggerlabel = {val: key for key, val in label_trigger_vocab.items()}\n",
    "label_role_vocab = load_dict(dict_path=role_dict_path)\n",
    "id2rolelabel = {val: key for key, val in label_role_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a16fb478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enum_data_process(dataset):\n",
    "    \"\"\"enum_data_process\"\"\"\n",
    "    output = []\n",
    "    for d_json in dataset:\n",
    "        text = d_json[\"text\"].lower().replace(\"\\t\", \" \")\n",
    "        output.append({\n",
    "            \"id\": d_json[\"id\"],\n",
    "            \"sent_id\": d_json[\"sent_id\"],\n",
    "            \"text\": text\n",
    "        })\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec34649c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigger_data_process(dataset):\n",
    "    \"\"\"data_process\"\"\"\n",
    "    \n",
    "    def replace_control_chars(str):\n",
    "        if str == '\\u200b' or str == '\\ufeff' or str == '\\ue601' or str == '\\u3000':\n",
    "            return '[UNK]'\n",
    "        else:\n",
    "            return str\n",
    "\n",
    "    output = []\n",
    "    for d_json in dataset:\n",
    "        _id = d_json[\"id\"]\n",
    "        text_a = [\n",
    "            \"，\" if t == \" \" or t == \"\\n\" or t == \"\\t\" else replace_control_chars(t)\n",
    "            for t in list(d_json[\"text\"].lower())\n",
    "        ]\n",
    "        output.append({\n",
    "            \"id\": d_json[\"id\"],\n",
    "            \"sent_id\": d_json[\"sent_id\"],\n",
    "            \"text\": d_json[\"text\"],\n",
    "            \"tokens\": text_a\n",
    "        })\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74074759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def role_data_process(dataset, trigger_file_path):\n",
    "    \"\"\"data_process\"\"\"\n",
    "    \n",
    "    def replace_control_chars(str):\n",
    "        if str == '\\u200b' or str == '\\ufeff' or str == '\\ue601' or str == '\\u3000':\n",
    "            return '[UNK]'\n",
    "        else:\n",
    "            return str\n",
    "    \n",
    "    trigger_data = read_by_lines(trigger_file_path)\n",
    "    # process trigger data\n",
    "    sent_trigger_mapping = {}\n",
    "    for d in tqdm(trigger_data, total=len(trigger_data)):\n",
    "        d_json = json.loads(d)\n",
    "        t_ret = extract_result_multilabel(d_json[\"text\"], d_json[\"pred\"][\"labels\"])\n",
    "        pred_event_types = list(set([(t[\"type\"], ''.join(t[\"text\"])) for t in t_ret]))\n",
    "        if t_ret:\n",
    "            print(pred_event_types)\n",
    "            break\n",
    "        if d_json[\"id\"] not in sent_trigger_mapping:\n",
    "            sent_trigger_mapping[d_json[\"id\"]] = []\n",
    "        for pred_event_type in pred_event_types:\n",
    "            if pred_event_type not in sent_trigger_mapping[d_json[\"id\"]]:\n",
    "                sent_trigger_mapping[d_json[\"id\"]].append(pred_event_type)\n",
    "\n",
    "    output = []\n",
    "    for d_json in dataset:\n",
    "        _id = d_json[\"id\"]\n",
    "        text_a = [\n",
    "            \"，\" if t == \" \" or t == \"\\n\" or t == \"\\t\" else replace_control_chars(t)\n",
    "            for t in list(d_json[\"text\"].lower())\n",
    "        ]\n",
    "        for pred_event_type in sent_trigger_mapping[d_json[\"id\"]]:\n",
    "            trigger_text = pred_event_type[0] + f\"({pred_event_type[1]})：\"\n",
    "            text_trigger = [\n",
    "                \"，\" if t == \" \" or t == \"\\n\" or t == \"\\t\" else t\n",
    "                for t in list(trigger_text.lower())\n",
    "            ]\n",
    "            output.append({\n",
    "                \"id\": d_json[\"id\"],\n",
    "                \"sent_id\": d_json[\"sent_id\"],\n",
    "                \"org_text\": d_json[\"text\"],\n",
    "                \"text\": trigger_text + d_json[\"text\"],\n",
    "                \"event_type\": pred_event_type[0],\n",
    "                \"trigger\": pred_event_type[1],\n",
    "                \"tokens\": text_trigger+text_a\n",
    "            })\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0f29dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d0e27901894cc282d4e21fd4cb3cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=30000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(duee_fin_test_preprocess_path, 'r', encoding='utf-8') as f:\n",
    "    dataset = json.loads(f.read())\n",
    "    preprocess_dataset = role_data_process(dataset, trigger_file_path='./predict/DuEE_fin/trigger/test_pred.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "784c7b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43e4f95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PADDING = tokenizer.vocab[tokenizer.pad_token]\n",
    "SEP = tokenizer.vocab[tokenizer.sep_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0411af8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaiduEnermDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset_path):\n",
    "        self.examples = []\n",
    "        with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "            dataset = json.loads(f.read())\n",
    "            preprocess_dataset = enum_data_process(dataset)\n",
    "            for d_json in preprocess_dataset:\n",
    "                text = d_json['text']\n",
    "                input_ids = tokenizer(text, is_split_into_words=False, add_special_tokens=True, max_length=max_seq_len, truncation=True)['input_ids']\n",
    "                tokens_input = input_ids + [PADDING] * (max_seq_len - len(input_ids))\n",
    "                attention_masks = self._get_attention_mask(input_ids, max_seq_len)\n",
    "                token_type_ids = self._get_token_type_id(input_ids, max_seq_len)\n",
    "                example = {\n",
    "                    \"input_ids\": tokens_input, \"attention_masks\": attention_masks, \"token_type_ids\": token_type_ids,\n",
    "                }\n",
    "                example.update(d_json)\n",
    "                self.examples.append(example)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, item_idx):\n",
    "        example = {\n",
    "            \"id\": self.examples[item_idx][\"id\"],\n",
    "            \"sent_id\": self.examples[item_idx][\"sent_id\"],\n",
    "            \"text\": self.examples[item_idx][\"text\"],\n",
    "            \"input_ids\": torch.tensor(self.examples[item_idx][\"input_ids\"]).long(),\n",
    "            \"attention_masks\": torch.tensor(self.examples[item_idx][\"attention_masks\"]),\n",
    "            \"token_type_ids\": torch.tensor(self.examples[item_idx][\"token_type_ids\"]),\n",
    "        }\n",
    "        return example\n",
    "\n",
    "    def _get_attention_mask(self, input_ids, max_seq_len):\n",
    "        \"\"\"Mask for padding.\"\"\"\n",
    "        if len(input_ids) > max_seq_len:\n",
    "            raise IndexError(\"Token length more than max seq length!\")\n",
    "        return [1] * len(input_ids) + [0] * (max_seq_len - len(input_ids))\n",
    "\n",
    "    def _get_token_type_id(self, input_ids, max_seq_len):\n",
    "        \"\"\"Segments: 0 for the first sequence, 1 for the second.\"\"\"\n",
    "        if len(input_ids) > max_seq_len:\n",
    "            raise IndexError(\"Token length more than max seq length!\")\n",
    "        segments = []\n",
    "        current_segment_id = 0\n",
    "        for input_id in input_ids:\n",
    "            segments.append(current_segment_id)\n",
    "            if input_id == SEP:\n",
    "                current_segment_id = 1\n",
    "        return segments + [0] * (max_seq_len - len(input_ids)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "486a6b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaiduTriggerDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset_path):\n",
    "        self.examples = []\n",
    "        with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "            dataset = json.loads(f.read())\n",
    "            preprocess_dataset = trigger_data_process(dataset)\n",
    "            for d_json in preprocess_dataset:\n",
    "                tokens = d_json['tokens']\n",
    "                input_ids = tokenizer(tokens, is_split_into_words=True, add_special_tokens=True, max_length=max_seq_len, truncation=True)['input_ids']\n",
    "                tokens_input = input_ids + [PADDING] * (max_seq_len - len(input_ids))\n",
    "                attention_masks = self._get_attention_mask(input_ids, max_seq_len)\n",
    "                token_type_ids = self._get_token_type_id(input_ids, max_seq_len)\n",
    "                example = {\n",
    "                    \"input_ids\": tokens_input,\n",
    "                    \"attention_masks\": attention_masks,\n",
    "                    \"token_type_ids\": token_type_ids,\n",
    "                    \"seq_lens\": len(tokens)\n",
    "                }\n",
    "                example.update(d_json)\n",
    "                self.examples.append(example)\n",
    "\n",
    "    def _get_attention_mask(self, input_ids, max_seq_len):\n",
    "        \"\"\"Mask for padding.\"\"\"\n",
    "        if len(input_ids) > max_seq_len:\n",
    "            raise IndexError(\"Token length more than max seq length!\")\n",
    "        return [1] * len(input_ids) + [0] * (max_seq_len - len(input_ids))\n",
    "\n",
    "    def _get_token_type_id(self, input_ids, max_seq_len):\n",
    "        \"\"\"Segments: 0 for the first sequence, 1 for the second.\"\"\"\n",
    "        if len(input_ids) > max_seq_len:\n",
    "            raise IndexError(\"Token length more than max seq length!\")\n",
    "        segments = []\n",
    "        current_segment_id = 0\n",
    "        for input_id in input_ids:\n",
    "            segments.append(current_segment_id)\n",
    "            if input_id == SEP:\n",
    "                current_segment_id = 1\n",
    "        return segments + [0] * (max_seq_len - len(input_ids))        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, item_idx):\n",
    "        example = {\n",
    "            \"id\": self.examples[item_idx][\"id\"],\n",
    "            \"sent_id\": self.examples[item_idx][\"sent_id\"],\n",
    "            \"text\": self.examples[item_idx][\"text\"],\n",
    "            \"input_ids\": torch.tensor(self.examples[item_idx][\"input_ids\"]).long(),\n",
    "            \"attention_masks\": torch.tensor(self.examples[item_idx][\"attention_masks\"]),\n",
    "            \"token_type_ids\": torch.tensor(self.examples[item_idx][\"token_type_ids\"]),\n",
    "            \"seq_lens\": self.examples[item_idx][\"seq_lens\"],\n",
    "        }\n",
    "        return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67d34e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaiduRoleDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset_path, trigger_file_path):\n",
    "        self.examples = []\n",
    "        with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "            dataset = json.loads(f.read())\n",
    "            preprocess_dataset = role_data_process(dataset, trigger_file_path=trigger_file_path)\n",
    "            for d_json in preprocess_dataset:\n",
    "                tokens = d_json['tokens']\n",
    "                input_ids = tokenizer(tokens, is_split_into_words=True, add_special_tokens=True, max_length=max_seq_len, truncation=True)['input_ids']\n",
    "                tokens_input = input_ids + [PADDING] * (max_seq_len - len(input_ids))\n",
    "                attention_masks = self._get_attention_mask(input_ids, max_seq_len)\n",
    "                token_type_ids = self._get_token_type_id(input_ids, max_seq_len)\n",
    "                example = {\n",
    "                    \"input_ids\": tokens_input,\n",
    "                    \"attention_masks\": attention_masks,\n",
    "                    \"token_type_ids\": token_type_ids,\n",
    "                    \"seq_lens\": len(tokens)\n",
    "                }\n",
    "                example.update(d_json)\n",
    "                self.examples.append(example)\n",
    "\n",
    "    def _get_attention_mask(self, input_ids, max_seq_len):\n",
    "        \"\"\"Mask for padding.\"\"\"\n",
    "        if len(input_ids) > max_seq_len:\n",
    "            raise IndexError(\"Token length more than max seq length!\")\n",
    "        return [1] * len(input_ids) + [0] * (max_seq_len - len(input_ids))\n",
    "\n",
    "    def _get_token_type_id(self, input_ids, max_seq_len):\n",
    "        \"\"\"Segments: 0 for the first sequence, 1 for the second.\"\"\"\n",
    "        if len(input_ids) > max_seq_len:\n",
    "            raise IndexError(\"Token length more than max seq length!\")\n",
    "        segments = []\n",
    "        current_segment_id = 0\n",
    "        for input_id in input_ids:\n",
    "            segments.append(current_segment_id)\n",
    "            if input_id == SEP:\n",
    "                current_segment_id = 1\n",
    "        return segments + [0] * (max_seq_len - len(input_ids))        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, item_idx):\n",
    "        example = {\n",
    "            \"id\": self.examples[item_idx][\"id\"],\n",
    "            \"sent_id\": self.examples[item_idx][\"sent_id\"],\n",
    "            \"event_type\": self.examples[item_idx][\"event_type\"],\n",
    "            \"trigger\": self.examples[item_idx][\"trigger\"],\n",
    "            \"text\": self.examples[item_idx][\"text\"],\n",
    "            \"input_ids\": torch.tensor(self.examples[item_idx][\"input_ids\"]).long(),\n",
    "            \"attention_masks\": torch.tensor(self.examples[item_idx][\"attention_masks\"]),\n",
    "            \"token_type_ids\": torch.tensor(self.examples[item_idx][\"token_type_ids\"]),\n",
    "            \"seq_lens\": self.examples[item_idx][\"seq_lens\"],\n",
    "        }\n",
    "        return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e83d75c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_enerm_dataset = BaiduEnermDataset(dataset_path=duee_fin_test_preprocess_path)\n",
    "test_trigger_dataset = BaiduTriggerDataset(dataset_path=duee_fin_test_preprocess_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68d4256b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_enerm_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d250f8a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_trigger_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "126fbe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed = 42):\n",
    "    \"\"\"Set the seed for generating random numbers on all GPUs.\n",
    "\n",
    "    It's safe to call this function if CUDA is not available; in that case, it is silently ignored.\n",
    "\n",
    "    Args:\n",
    "        seed (int, optional): random numbers on all GPUs. Defaults to 42.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a5b9fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:2\n",
      "\n",
      "Tesla V100-PCIE-32GB\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "CUDA Device Count: 3\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    \n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', torch.cuda.memory_allocated(0)/1024**3, 'GB')\n",
    "    print('Cached:   ', torch.cuda.memory_reserved(0)/1024**3, 'GB')\n",
    "    \n",
    "    print('CUDA Device Count:', n_gpu)\n",
    "    \n",
    "set_seed(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5e7449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_by_lines(path, data):\n",
    "    \"\"\"write the data\"\"\"\n",
    "    with open(path, \"w\") as outfile:\n",
    "        [outfile.write(d + \"\\n\") for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb751c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_enerm(model, test_dataloader):\n",
    "    model.eval()\n",
    "    step = 0\n",
    "    eval_acc = 0.0\n",
    "    eval_f1 = 0.0\n",
    "    eval_loss = 0.0\n",
    "    results = []\n",
    "    test_iterator = tqdm(test_dataloader)\n",
    "    for batch in test_iterator:\n",
    "        _, logits = model(\n",
    "            input_ids=batch['input_ids'].to(device),\n",
    "            attention_mask=batch['attention_masks'].to(device),\n",
    "            token_type_ids=batch['token_type_ids'].to(device)\n",
    "        )\n",
    "\n",
    "        probs = torch.sigmoid(logits).data.cpu()\n",
    "        probs_ids = (probs > 0.5).numpy()\n",
    "        probs = probs.numpy()\n",
    "        for id_, sent_id, text, label_probs, p_id in zip(batch['id'], batch['sent_id'], batch['text'], probs.tolist(), probs_ids.tolist()):\n",
    "            true_indices = np.argwhere(p_id).flatten()\n",
    "            labels = [id2enumlabel[true_index] for true_index in true_indices]\n",
    "            results.append({\"id\": id_, \"sent_id\": sent_id, \"text\": text, \"pred\":{\"probs\": label_probs, \"label\": labels}})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78e822ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_trigger(model, test_dataloader):\n",
    "    model.eval()\n",
    "    step = 0\n",
    "    eval_acc = 0.0\n",
    "    eval_f1 = 0.0\n",
    "    eval_loss = 0.0\n",
    "    results = []\n",
    "    test_iterator = tqdm(test_dataloader)\n",
    "    for batch in test_iterator:\n",
    "        loss, logits = model(\n",
    "            input_ids=batch['input_ids'].to(device),\n",
    "            attention_mask=batch['attention_masks'].to(device),\n",
    "            token_type_ids=batch['token_type_ids'].to(device)\n",
    "        )\n",
    "\n",
    "        probs = torch.sigmoid(logits).data.cpu()\n",
    "        probs_ids = (probs > 0.5).numpy()\n",
    "        probs = probs.numpy()\n",
    "        for id_, sent_id, text, p_list, p_ids, seq_len in zip(batch['id'], batch['sent_id'], batch['text'], probs.tolist(), probs_ids.tolist(), batch['seq_lens']):\n",
    "            prob_multi, label_multi = [], []\n",
    "            for index, pid in enumerate(p_ids[1: seq_len - 1]):\n",
    "                true_indices = np.argwhere(pid).flatten()\n",
    "                prob_multi.append(p_list[index])\n",
    "                label_multi.append([id2triggerlabel[true_index] for true_index in true_indices])\n",
    "            results.append({\"id\": id_, \"sent_id\":sent_id, \"text\": text, \"pred\": {\"probs\": prob_multi, \"labels\": label_multi}})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c44ffadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_role(model, test_dataloader):\n",
    "    model.eval()\n",
    "    step = 0\n",
    "    eval_acc = 0.0\n",
    "    eval_f1 = 0.0\n",
    "    eval_loss = 0.0\n",
    "    results = []\n",
    "    test_iterator = tqdm(test_dataloader)\n",
    "    for batch in test_iterator:\n",
    "        loss, logits = model(\n",
    "            input_ids=batch['input_ids'].to(device),\n",
    "            attention_mask=batch['attention_masks'].to(device),\n",
    "            token_type_ids=batch['token_type_ids'].to(device)\n",
    "        )\n",
    "\n",
    "        probs = torch.sigmoid(logits).data.cpu()\n",
    "        probs_ids = (probs > 0.5).numpy()\n",
    "        probs = probs.numpy()\n",
    "        for id_, sent_id, text, event_type, trigger, p_list, p_ids, seq_len in zip(batch['id'], batch['sent_id'], batch['text'], batch['event_type'], batch['trigger'], probs.tolist(), probs_ids.tolist(), batch['seq_lens']):\n",
    "            prob_multi, label_multi = [], []\n",
    "            for index, pid in enumerate(p_ids[1: seq_len - 1]):\n",
    "                true_indices = np.argwhere(pid).flatten()\n",
    "                prob_multi.append(p_list[index])\n",
    "                label_multi.append([id2rolelabel[true_index] for true_index in true_indices])\n",
    "            results.append({\"id\": id_, \"sent_id\":sent_id, \"event_type\": event_type, \"trigger\": trigger, \"text\": text, \"pred\": {\"probs\": prob_multi, \"labels\": label_multi}})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37388d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_enerm_sampler = SequentialSampler(test_enerm_dataset)\n",
    "test_enerm_dataloader = DataLoader(test_enerm_dataset, sampler=test_enerm_sampler, batch_size = 512)\n",
    "\n",
    "test_trigger_sampler = SequentialSampler(test_trigger_dataset)\n",
    "test_trigger_dataloader = DataLoader(test_trigger_dataset, sampler=test_trigger_sampler, batch_size = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5e5e932",
   "metadata": {},
   "outputs": [],
   "source": [
    "enum_model = torch.load(enerm_model_path).to(device)\n",
    "tigger_model = torch.load(tigger_model_path).to(device)\n",
    "role_model = torch.load(role_model_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a7e770f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "685134d2a4684a229fb3c4ea4035202f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=59), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentences_enerm = test_enerm(enum_model, test_enerm_dataloader)\n",
    "sentences_enerm = [json.dumps(sent_enerm, ensure_ascii=False) for sent_enerm in sentences_enerm]\n",
    "write_by_lines('./predict/DuEE_fin/enerm/test_pred.json', sentences_enerm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87f89af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5cb8eb48d2a462ba12f0f440b285f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=59), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentences_tigger = test_trigger(tigger_model, test_trigger_dataloader)\n",
    "sentences_tigger = [json.dumps(sent_tigger, ensure_ascii=False) for sent_tigger in sentences_tigger]\n",
    "write_by_lines('./predict/DuEE_fin/trigger/test_pred.json', sentences_tigger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e0ac0ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aea479d5b97243eb811cab315d4df8c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=30000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_role_dataset = BaiduRoleDataset(dataset_path=duee_fin_test_preprocess_path, trigger_file_path='./predict/DuEE_fin/trigger/test_pred.json')\n",
    "\n",
    "test_role_sampler = SequentialSampler(test_role_dataset)\n",
    "test_role_dataloader = DataLoader(test_role_dataset, sampler=test_role_sampler, batch_size = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b51ab620",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-6127fb081fe5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_role_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-a1e0338c05ea>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item_idx)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         example = {\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0;34m\"id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0;34m\"sent_id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sent_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;34m\"event_type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"event_type\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "test_role_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8cfb008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9a2f016931d45e28cfd98318e79dea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentences_role = test_role(role_model, test_role_dataloader)\n",
    "sentences_role = [json.dumps(sent_role, ensure_ascii=False) for sent_role in sentences_role]\n",
    "write_by_lines('./predict/DuEE_fin/role/test_pred.json', sentences_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1052a74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trigger predict 30000 load from ./predict/DuEE_fin/trigger/test_pred.json\n",
      "role predict 0 load from ./predict/DuEE_fin/role/test_pred.json\n",
      "enum predict 30000 load from ./predict/DuEE_fin/enerm/test_pred.json\n",
      "schema 13 load from ./dictionary/event_schema.json\n",
      "submit data 30000 save to ./submit/DuEE_fin/test_duee_fin_erine_multilabel-trick1-fix.json\n"
     ]
    }
   ],
   "source": [
    "### multilabel classfication with header trick use \n",
    "\n",
    "!python duee_fin_postprocess_trick1.py --trigger_file ./predict/DuEE_fin/trigger/test_pred.json \\\n",
    "    --role_file ./predict/DuEE_fin/role/test_pred.json \\\n",
    "    --enum_file ./predict/DuEE_fin/enerm/test_pred.json --schema_file ./dictionary/event_schema.json \\\n",
    "    --save_path ./submit/DuEE_fin/test_duee_fin_erine_multilabel-trick1-fix.json --multilabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b19e23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
