{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b99e3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from seqeval.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel, AutoModelForTokenClassification\n",
    "\n",
    "from crf_layer import CRFLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "474ba276",
   "metadata": {},
   "outputs": [],
   "source": [
    "enum_role = \"环节\"\n",
    "max_seq_len = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bedd926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(dataset):\n",
    "    \"\"\"data_process\"\"\"\n",
    "\n",
    "    def label_data(data, start, l, _type):\n",
    "        \"\"\"label_data\"\"\"\n",
    "        for i in range(start, start + l):\n",
    "            suffix = \"B-\" if i == start else \"I-\"\n",
    "            data[i] = \"{}{}\".format(suffix, _type)\n",
    "        return data\n",
    "\n",
    "    output = []\n",
    "    for d_json in dataset:\n",
    "        _id = d_json[\"id\"]\n",
    "        text_a = [\n",
    "            \"，\" if t == \" \" or t == \"\\n\" or t == \"\\t\" else t\n",
    "            for t in list(d_json[\"text\"].lower())\n",
    "        ]\n",
    "        for event in d_json.get(\"event_list\", []):\n",
    "            event_type = event['event_type']\n",
    "            labels = [\"O\"] * len(text_a)\n",
    "            for arg in event[\"arguments\"]:\n",
    "                role_type = arg[\"role\"]\n",
    "                if role_type == enum_role:\n",
    "                    continue\n",
    "                argument = arg[\"argument\"]\n",
    "                start = arg[\"argument_start_index\"]\n",
    "                labels = label_data(labels, start, len(argument), role_type)\n",
    "            output.append({\n",
    "                \"tokens\": text_a, \"labels\": labels\n",
    "            })\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4af2aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(dict_path):\n",
    "    \"\"\"load_dict\"\"\"\n",
    "    vocab = {}\n",
    "    for line in open(dict_path, 'r', encoding='utf-8'):\n",
    "        value, key = line.strip('\\n').split('\\t')\n",
    "        vocab[key] = int(value)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09abf029",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_vocab = load_dict(dict_path='./dictionary/role_tag.dict')\n",
    "id2label = {val: key for key, val in label_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba347d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nghuyong/ernie-1.0 were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at nghuyong/ernie-1.0 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\"nghuyong/ernie-1.0\")\n",
    "config.num_labels = len(label_vocab)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"nghuyong/ernie-1.0\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87567be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"nghuyong/ernie-1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44aa16b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PADDING = tokenizer.vocab[tokenizer.pad_token]\n",
    "SEP = tokenizer.vocab[tokenizer.sep_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "870891b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaiduEventDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset_path, label_dict_path, ignore_index=-100):\n",
    "        self.label_vocab = load_dict(label_dict_path)\n",
    "        self.label_num = max(self.label_vocab.values()) + 1\n",
    "        self.examples = []\n",
    "        with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "            dataset = json.loads(f.read())\n",
    "            preprocess_dataset = data_process(dataset)\n",
    "            for d_json in preprocess_dataset:\n",
    "                tokens = d_json['tokens']\n",
    "                input_ids = tokenizer(tokens, is_split_into_words=True, add_special_tokens=True, max_length=max_seq_len, truncation=True)['input_ids']\n",
    "                tokens_input = input_ids + [PADDING] * (max_seq_len - len(input_ids))\n",
    "                attention_masks = self._get_attention_mask(input_ids, max_seq_len)\n",
    "                token_type_ids = self._get_token_type_id(input_ids, max_seq_len)\n",
    "                example = {\n",
    "                    \"input_ids\": tokens_input, \"attention_masks\": attention_masks,\n",
    "                    \"token_type_ids\": token_type_ids, \"seq_lens\": len(input_ids)\n",
    "                }\n",
    "                if 'labels' in d_json:\n",
    "                    labels = d_json['labels']\n",
    "                    labels = labels[:(max_seq_len - 2)]\n",
    "                    encoded_label = [\"O\"] + labels + [\"O\"]\n",
    "                    encoded_label = [self.label_vocab[x] for x in encoded_label] + [ignore_index] * (max_seq_len - 2 - len(labels))\n",
    "                    example.update({\"encoded_label\": encoded_label})\n",
    "                self.examples.append(example)\n",
    "\n",
    "    def _get_attention_mask(self, input_ids, max_seq_len):\n",
    "        \"\"\"Mask for padding.\"\"\"\n",
    "        if len(input_ids) > max_seq_len:\n",
    "            raise IndexError(\"Token length more than max seq length!\")\n",
    "        return [1] * len(input_ids) + [0] * (max_seq_len - len(input_ids))\n",
    "\n",
    "    def _get_token_type_id(self, input_ids, max_seq_len):\n",
    "        \"\"\"Segments: 0 for the first sequence, 1 for the second.\"\"\"\n",
    "        if len(input_ids) > max_seq_len:\n",
    "            raise IndexError(\"Token length more than max seq length!\")\n",
    "        segments = []\n",
    "        current_segment_id = 0\n",
    "        for input_id in input_ids:\n",
    "            segments.append(current_segment_id)\n",
    "            if input_id == SEP:\n",
    "                current_segment_id = 1\n",
    "        return segments + [0] * (max_seq_len - len(input_ids))        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, item_idx):\n",
    "        example = {\n",
    "            \"input_ids\": torch.tensor(self.examples[item_idx][\"input_ids\"]).long(),\n",
    "            \"attention_masks\": torch.tensor(self.examples[item_idx][\"attention_masks\"]),\n",
    "            \"token_type_ids\": torch.tensor(self.examples[item_idx][\"token_type_ids\"]),\n",
    "            \"seq_lens\": self.examples[item_idx][\"seq_lens\"]\n",
    "        }\n",
    "        if \"encoded_label\" in self.examples[item_idx]:\n",
    "            example.update({\"encoded_label\": torch.tensor(self.examples[item_idx][\"encoded_label\"], dtype=torch.long)})\n",
    "        return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21aa23c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed = 42):\n",
    "    \"\"\"Set the seed for generating random numbers on all GPUs.\n",
    "\n",
    "    It's safe to call this function if CUDA is not available; in that case, it is silently ignored.\n",
    "\n",
    "    Args:\n",
    "        seed (int, optional): random numbers on all GPUs. Defaults to 42.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5903c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BaiduEventDataset(dataset_path='./resources/duee_fin_train_preprocess.json', label_dict_path='./dictionary/role_tag.dict')\n",
    "dev_dataset = BaiduEventDataset(dataset_path='./resources/duee_fin_dev_preprocess.json', label_dict_path='./dictionary/role_tag.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9cb3ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    1,   250,   275,   281,    74,   211,  1452,    67,   586, 17963,\n",
       "           589,    42,   397,   701,   540,    30, 17963,   540,    42,   208,\n",
       "           540,    30, 17963,   208,    42,   284,   249, 17963, 17963,    74,\n",
       "          2124,  1336,  1947,   273,   599,    64,    59,   837,   793,   207,\n",
       "          2091,     6,  1236,   159,   207,  2091,   277,   656,    13,   284,\n",
       "           701,    42,   317,   317, 17963,     4,     4,     4,     4,    61,\n",
       "           362,    74,   341,   139,    60,   316,   102,   877,     4,   341,\n",
       "            60,  1545,  1675,   532,  1452,     4,   211,  1452,    67,   586,\n",
       "            78,  1671,  3409,     4,   284,   540,   540,   208,   208,   249,\n",
       "             4,   185,   966,   463,    74,   589,    42,   397,   701,   183,\n",
       "            77,   515,   136,   284,   139,    86,  1598,    34,   460,    53,\n",
       "           612,   351,     4,    53,   230,   293,    45,   837,   242,  2124,\n",
       "          1336,  1947,     5,   124,   264,     4,   349,  1699,  2124,  1336,\n",
       "          1947,   174,    63,    64,    59,   837,   793,   315,    38,    15,\n",
       "           207,  2091,    26,   112, 12043,     4,  1488,   268,    89,    53,\n",
       "           612,   139,     4,  2124,  1336,  1947,   300,   303,     9,    53,\n",
       "           230,   837,   793,   317,   589,   701,   540,    42,   284,   585,\n",
       "           211,   837,     4,   602,    53,   230,   238,   837,    89,     5,\n",
       "           249,   540,    42,   249,   317, 17963, 12048,  1236,   159,   207,\n",
       "          2091,   837,   793,   249,   249,   701,   540,   211,   837,     4,\n",
       "           602,  2124,  1336,  1947,   303,     9,    53,   230,   837,   793,\n",
       "           238,   179,     5,   284,   701,    42,   317,   317, 17963,     4,\n",
       "           602,    53,   230,   238,   837,    89,     5,   585,    42,   540,\n",
       "           208, 17963, 12043,     2,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " 'attention_masks': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'seq_lens': 234,\n",
       " 'encoded_label': tensor([ 120,  120,  120,  120,  120,   12,   13,   13,   13,  120,  120,  120,\n",
       "          120,  120,  120,  120,  120,  120,  120,  120,  120,  120,  120,  120,\n",
       "          120,  120,  120,  120,  120,  120,    0,    1,    1,  120,  120,  120,\n",
       "          120,    6,    7,  120,  120,  120,  120,  120,  120,  120,  120,  120,\n",
       "          120,   16,   17,   17,   17,   17,   17,  120,  120,  120,  120,  120,\n",
       "          120,  120,  120,  120,  120,  120,  120,  120,  120,  120,  120,  120,\n",
       "          120,  120,  120,  120,  120,  120,  120,  120,  120,  120,  120,  120,\n",
       "          120,  120,  120,  120,  120,  120,  120,  120,  120,  120,  120,  120,\n",
       "          120,  120,  120,  120,  120,    2,    3,    3,    3,    3,    3,  120,\n",
       "          120,  120,  120,  120,  120,  120,  120,  120,  120,  120,  120,  120,\n",
       "          120,  120,  120,  120,  120,  120,  120,  120,  120,  120,  120,  120,\n",
       "          120,  120,  120,  120,  120,  120,  120,  120,  120,  120,  120,  120,\n",
       "          120,  120,   10,   11,   11,   11,   11,   11,  120,  120,  120,  120,\n",
       "          120,  120,  120,  120,  120,  120,  120,  120,  120,  120,  120,  120,\n",
       "          120,  120,  120,  120,  120,  120,  120,  120,  120,  120,  120,  120,\n",
       "          120,  120,  120,  120,  120,  120,  120,  120,  120,  120,  120,  120,\n",
       "          120,    8,    9,    9,    9,    9,  120,  120,  120,  120,  120,  120,\n",
       "          120,  120,  120,  120,  120,  120,  120,  120,  120,  120,  120,  120,\n",
       "          120,  120,  120,  120,  120,  120,  120,  120,  120,  120,  120,   14,\n",
       "           15,   15,   15,   15,  120,  120, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa8d34f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:2\n",
      "\n",
      "Tesla V100-PCIE-32GB\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "CUDA Device Count: 3\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    \n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', torch.cuda.memory_allocated(0)/1024**3, 'GB')\n",
    "    print('Cached:   ', torch.cuda.memory_reserved(0)/1024**3, 'GB')\n",
    "    \n",
    "    print('CUDA Device Count:', n_gpu)\n",
    "    \n",
    "set_seed(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "569b24e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, eval_dataloader):\n",
    "    model.eval()\n",
    "    step = 0\n",
    "    eval_acc = 0.0\n",
    "    eval_f1 = 0.0\n",
    "    eval_precision = 0.0\n",
    "    eval_recall = 0.0\n",
    "    eval_loss = 0.0\n",
    "    for batch in eval_dataloader:\n",
    "        outputs = model(\n",
    "            input_ids=batch['input_ids'].to(device),\n",
    "            # attention_mask=batch['attention_masks'].to(device),\n",
    "            token_type_ids=batch['token_type_ids'].to(device),\n",
    "            labels=batch['encoded_label'].to(device)\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        eval_loss += loss.item()\n",
    "        pred_list = torch.argmax(logits, dim=-1)\n",
    "        pred_Y, true_Y = [], []\n",
    "        for t_list, p_list, seq_len in zip(batch['encoded_label'].cpu().tolist(), pred_list.cpu().tolist(), batch['seq_lens']):\n",
    "            pred_Y.append([id2label.get(pid, \"O\") for pid in p_list[1: seq_len - 1]])\n",
    "            true_Y.append([id2label.get(tid, \"O\") for tid in t_list[1: seq_len - 1]])\n",
    "        eval_acc += accuracy_score(pred_Y, true_Y)\n",
    "        eval_precision += precision_score(pred_Y, true_Y, zero_division=1)\n",
    "        eval_recall += recall_score(pred_Y, true_Y, zero_division=1)\n",
    "        eval_f1 += f1_score(pred_Y, true_Y)\n",
    "        step += 1\n",
    "    model.train()\n",
    "    return eval_loss/step, eval_acc/step, eval_precision/step, eval_recall/step, eval_f1/step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57b5dce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### train model\n",
    "\n",
    "def train(model, ds_train, ds_dev = None, n_epochs = 100, learning_rate = 2e-5, weight_decay = 0.01, batch_size = 1, eval_per_epoch = 1):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    train_sampler = RandomSampler(ds_train)\n",
    "    train_dataloader = DataLoader(ds_train, sampler=train_sampler, batch_size=batch_size)\n",
    "    \n",
    "    eval_sampler = SequentialSampler(ds_dev)\n",
    "    eval_dataloader = DataLoader(ds_dev, sampler=eval_sampler, batch_size=batch_size)\n",
    "    \n",
    "    decay_params = [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ]\n",
    "\n",
    "#     if n_gpu > 1:\n",
    "#         model = torch.nn.DataParallel(model)\n",
    "\n",
    "    optimizer_grouped_parameters = [{\n",
    "        \"params\": model.parameters(),\n",
    "        \"lr\": learning_rate, \n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"apply_decay_param_fun\": lambda x: x in decay_params\n",
    "    }]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters)\n",
    "#     scheduler = ReduceLROnPlateau(optimizer, \"min\")\n",
    "    \n",
    "    f1 = 0.0\n",
    "    acc = 0.0\n",
    "    precision = 0.0\n",
    "    recall = 0.0\n",
    "    tr_loss = 0.0\n",
    "    global_step = 0\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    postfix = {}\n",
    "    for epoch in range(0, n_epochs):\n",
    "        eval_flag = False\n",
    "        train_iterator = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{n_epochs}\")\n",
    "        for batch in train_iterator:\n",
    "            outputs = model(\n",
    "                input_ids=batch['input_ids'].to(device),\n",
    "                # attention_mask=batch['attention_masks'].to(device),\n",
    "                token_type_ids=batch['token_type_ids'].to(device),\n",
    "                labels=batch['encoded_label'].to(device)\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "#             if n_gpu > 1:\n",
    "#                 loss = loss.mean()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # scheduler.step(loss)\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            pred_list = torch.argmax(logits, dim=-1)\n",
    "            pred_Y, true_Y = [], []\n",
    "            for t_list, p_list, seq_len in zip(batch['encoded_label'].cpu().tolist(), pred_list.cpu().tolist(), batch['seq_lens']):\n",
    "                pred_Y.append([id2label.get(pid, \"O\") for pid in p_list[1: seq_len - 1]])\n",
    "                true_Y.append([id2label.get(tid, \"O\") for tid in t_list[1: seq_len - 1]])\n",
    "            acc += accuracy_score(pred_Y, true_Y)\n",
    "            precision += precision_score(pred_Y, true_Y, zero_division=1)\n",
    "            recall += recall_score(pred_Y, true_Y, zero_division=1)\n",
    "            f1 += f1_score(pred_Y, true_Y)\n",
    "            model.zero_grad()\n",
    "\n",
    "            postfix.update({\"Avg loss\": f\"{tr_loss / (global_step + 1):.2f}\", \"Avg acc score\": f\"{acc / (global_step + 1):.2f}\", \"Avg precision score\": f\"{precision / (global_step + 1):.2f}\", \"Avg recall score\": f\"{recall / (global_step + 1):.2f}\", \"Avg f1 score\": f\"{f1 / (global_step + 1):.2f}\"})\n",
    "            if (\n",
    "                not eval_flag\n",
    "                and (global_step + 1) % len(train_dataloader) == 0\n",
    "                and (epoch % eval_per_epoch) == 0\n",
    "            ):\n",
    "                if ds_dev is not None:\n",
    "                    eval_loss, eval_acc, eval_precision, eval_recall, eval_f1 = evaluate(model, eval_dataloader)\n",
    "                postfix.update({\"Avg eval loss\": f\"{eval_loss:.2f}\", \"Avg eval acc\": f\"{eval_acc:.2f}\", \"Avg eval precision\": f\"{eval_precision:.2f}\", \"Avg eval recall\": f\"{eval_recall:.2f}\", \"Avg eval f1\": f\"{eval_f1:.2f}\"})\n",
    "                eval_flag = True\n",
    "            train_iterator.set_postfix(postfix)\n",
    "            global_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213f4b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95846cfc4a234ad2be837c12354dee20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 1/20', max=590, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(model, train_dataset, ds_dev=dev_dataset, n_epochs=20, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62fe2a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.cpu(), './models/DuEE_fin/roberta-chinese-large/role.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "458f733c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c25bbf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "069d4b83295243b0bedfebb442b10c5b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_c4fd9ccefe95434db2e7df930aaac118",
       "style": "IPY_MODEL_e1c3e571d2b54106bf20fac3e33b5012",
       "value": " 130/274 [02:12&lt;02:26,  1.02s/it, Avg loss=1.23, Avg acc score=0.92, Avg f1 score=0.92]"
      }
     },
     "60f889e1229a4154aa318d4c46d16fe2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "80336ec9357f4abd8c4c3ba264963949": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_8e3273f660b642669d155147b757f335",
        "IPY_MODEL_069d4b83295243b0bedfebb442b10c5b"
       ],
       "layout": "IPY_MODEL_f399e7b310cd4d1fb83bf06a20fc5d56"
      }
     },
     "8e3273f660b642669d155147b757f335": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "description": "Epoch 1/20:  47%",
       "layout": "IPY_MODEL_60f889e1229a4154aa318d4c46d16fe2",
       "max": 274,
       "style": "IPY_MODEL_c2f754305c5341929c539cca7dd9a9d2",
       "value": 130
      }
     },
     "c2f754305c5341929c539cca7dd9a9d2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "c4fd9ccefe95434db2e7df930aaac118": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e1c3e571d2b54106bf20fac3e33b5012": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "f399e7b310cd4d1fb83bf06a20fc5d56": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
