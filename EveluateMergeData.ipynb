{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e5446d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel, AutoModelForSequenceClassification\n",
    "\n",
    "from crf_layer import CRFLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77f33187",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_model = \"hfl/chinese-roberta-wwm-ext-large\"\n",
    "\n",
    "shema_path = './dictionary/event_schema.json'\n",
    "enerm_dict_path = './dictionary/enum_tag.dict'\n",
    "trigger_dict_path = './dictionary/trigger_tag.dict'\n",
    "role_dict_path = './dictionary/role_tag.dict'\n",
    "\n",
    "enerm_model_path = './models/DuEE_fin/roberta-chinese-large/enum.bin'\n",
    "tigger_model_path = './models/DuEE_fin/roberta-chinese-large/trigger.bin'\n",
    "role_model_path = './models/DuEE_fin/roberta-chinese-large/role.bin'\n",
    "\n",
    "duee_fin_dev_path = './resources/duee_fin_dev.json'\n",
    "duee_fin_dev_preprocess_path = './resources/duee_fin_dev_preprocess.json'\n",
    "\n",
    "enum_role = \"环节\"\n",
    "enum_event_type = \"公司上市\"\n",
    "max_seq_len = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b2db8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(dataset, model=\"trigger\"):\n",
    "    \"\"\"data_process\"\"\"\n",
    "    \n",
    "    def label_data(data, start, l, _type):\n",
    "        \"\"\"label_data\"\"\"\n",
    "        for i in range(start, start + l):\n",
    "            suffix = \"B-\" if i == start else \"I-\"\n",
    "            data[i] = \"{}{}\".format(suffix, _type)\n",
    "        return data\n",
    "\n",
    "    output = []\n",
    "    for d_json in dataset:\n",
    "        _id = d_json[\"id\"]\n",
    "        text_a = [\n",
    "            \"，\" if t == \" \" or t == \"\\n\" or t == \"\\t\" else t\n",
    "            for t in list(d_json[\"text\"].lower())\n",
    "        ]\n",
    "        if model == \"trigger\":\n",
    "            labels = [\"O\"] * len(text_a)\n",
    "#             if len(d_json.get(\"event_list\", [])) == 0:\n",
    "#                 continue\n",
    "            for event in d_json.get(\"event_list\", []):\n",
    "                event_type = event[\"event_type\"]\n",
    "                start = event[\"trigger_start_index\"]\n",
    "                trigger = event[\"trigger\"]\n",
    "                labels = label_data(labels, start, len(trigger), event_type)\n",
    "            output.append({\n",
    "                \"id\": d_json[\"id\"],\n",
    "                \"sent_id\": d_json[\"sent_id\"],\n",
    "                \"text\": d_json[\"text\"],\n",
    "                \"tokens\": text_a,\n",
    "                \"labels\": labels\n",
    "            })\n",
    "        elif model == \"role\":\n",
    "            labels = [\"O\"] * len(text_a)\n",
    "            for event in d_json.get(\"event_list\", []):\n",
    "                for arg in event[\"arguments\"]:\n",
    "                    role_type = arg[\"role\"]\n",
    "                    if role_type == enum_role:\n",
    "                        continue\n",
    "                    argument = arg[\"argument\"]\n",
    "                    start = arg[\"argument_start_index\"]\n",
    "                    labels = label_data(labels, start, len(argument), role_type)\n",
    "            output.append({\n",
    "                \"id\": d_json[\"id\"],\n",
    "                \"sent_id\": d_json[\"sent_id\"],\n",
    "                \"text\": d_json[\"text\"],\n",
    "                \"tokens\": text_a,\n",
    "                \"labels\": labels\n",
    "            })\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85b282d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enum_data_process(dataset):\n",
    "    \"\"\"enum_data_process\"\"\"\n",
    "    output = []\n",
    "    for d_json in dataset:\n",
    "        text = d_json[\"text\"].lower().replace(\"\\t\", \" \")\n",
    "#         if len(d_json.get(\"event_list\", [])) == 0:\n",
    "#             continue\n",
    "        label = 'ABS'\n",
    "        for event in d_json.get(\"event_list\", []):\n",
    "            if event[\"event_type\"] != \"公司上市\":\n",
    "                continue\n",
    "            for argument in event[\"arguments\"]:\n",
    "                role_type = argument[\"role\"]\n",
    "                if role_type == enum_role:\n",
    "                    label = argument[\"argument\"]\n",
    "        output.append({\n",
    "            \"id\": d_json[\"id\"],\n",
    "            \"sent_id\": d_json[\"sent_id\"],\n",
    "            \"text\": text,\n",
    "            \"label\": label\n",
    "        })\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab43c840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(dict_path):\n",
    "    \"\"\"load_dict\"\"\"\n",
    "    vocab = {}\n",
    "    for line in open(dict_path, 'r', encoding='utf-8'):\n",
    "        value, key = line.strip('\\n').split('\\t')\n",
    "        vocab[key] = int(value)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec0d3b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_enum_vocab = load_dict(dict_path=enerm_dict_path)\n",
    "id2enumlabel = {val: key for key, val in label_enum_vocab.items()}\n",
    "label_trigger_vocab = load_dict(dict_path=trigger_dict_path)\n",
    "id2triggerlabel = {val: key for key, val in label_trigger_vocab.items()}\n",
    "label_role_vocab = load_dict(dict_path=role_dict_path)\n",
    "id2rolelabel = {val: key for key, val in label_role_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfbd1a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01b5ccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PADDING = tokenizer.vocab[tokenizer.pad_token]\n",
    "SEP = tokenizer.vocab[tokenizer.sep_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b609aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaiduEnermDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset_path, label_dict_path):\n",
    "        self.label_vocab = load_dict(label_dict_path)\n",
    "        self.label_num = max(self.label_vocab.values()) + 1\n",
    "        self.examples = []\n",
    "        with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "            dataset = json.loads(f.read())\n",
    "            preprocess_dataset = enum_data_process(dataset)\n",
    "            for d_json in preprocess_dataset:\n",
    "                text = d_json['text']\n",
    "                input_ids = tokenizer(text, is_split_into_words=False, add_special_tokens=True, max_length=max_seq_len, truncation=True)['input_ids']\n",
    "                tokens_input = input_ids + [PADDING] * (max_seq_len - len(input_ids))\n",
    "                attention_masks = self._get_attention_mask(input_ids, max_seq_len)\n",
    "                token_type_ids = self._get_token_type_id(input_ids, max_seq_len)\n",
    "                example = {\n",
    "                    \"input_ids\": tokens_input, \"attention_masks\": attention_masks, \"token_type_ids\": token_type_ids,\n",
    "                    \"seq_lens\": len(input_ids)\n",
    "                }\n",
    "                example.update(d_json)\n",
    "                if 'label' in d_json:\n",
    "                    label = d_json['label']\n",
    "                    example.update({\"encoded_label\": self.label_vocab.get(label, -1)})\n",
    "                self.examples.append(example)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, item_idx):\n",
    "        example = {\n",
    "            \"id\": self.examples[item_idx][\"id\"],\n",
    "            \"sent_id\": self.examples[item_idx][\"sent_id\"],\n",
    "            \"text\": self.examples[item_idx][\"text\"],\n",
    "            \"input_ids\": torch.tensor(self.examples[item_idx][\"input_ids\"]).long(),\n",
    "            \"attention_masks\": torch.tensor(self.examples[item_idx][\"attention_masks\"]),\n",
    "            \"token_type_ids\": torch.tensor(self.examples[item_idx][\"token_type_ids\"]),\n",
    "            \"seq_lens\": self.examples[item_idx][\"seq_lens\"]\n",
    "        }\n",
    "        if \"encoded_label\" in self.examples[item_idx]:\n",
    "            example.update({\"encoded_label\": torch.tensor(self.examples[item_idx][\"encoded_label\"], dtype=torch.long)})\n",
    "        return example\n",
    "\n",
    "    def _get_attention_mask(self, input_ids, max_seq_len):\n",
    "        \"\"\"Mask for padding.\"\"\"\n",
    "        if len(input_ids) > max_seq_len:\n",
    "            raise IndexError(\"Token length more than max seq length!\")\n",
    "        return [1] * len(input_ids) + [0] * (max_seq_len - len(input_ids))\n",
    "\n",
    "    def _get_token_type_id(self, input_ids, max_seq_len):\n",
    "        \"\"\"Segments: 0 for the first sequence, 1 for the second.\"\"\"\n",
    "        if len(input_ids) > max_seq_len:\n",
    "            raise IndexError(\"Token length more than max seq length!\")\n",
    "        segments = []\n",
    "        current_segment_id = 0\n",
    "        for input_id in input_ids:\n",
    "            segments.append(current_segment_id)\n",
    "            if input_id == SEP:\n",
    "                current_segment_id = 1\n",
    "        return segments + [0] * (max_seq_len - len(input_ids)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7ff1c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaiduEventDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset_path, label_dict_path, model=\"trigger\", ignore_index=-100):\n",
    "        self.label_vocab = load_dict(label_dict_path)\n",
    "        self.label_num = max(self.label_vocab.values()) + 1\n",
    "        self.examples = []\n",
    "        with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "            dataset = json.loads(f.read())\n",
    "            preprocess_dataset = data_process(dataset, model=model)\n",
    "            for d_json in preprocess_dataset:\n",
    "                tokens = d_json['tokens']\n",
    "                input_ids = tokenizer(tokens, is_split_into_words=True, add_special_tokens=True, max_length=max_seq_len, truncation=True)['input_ids']\n",
    "                tokens_input = input_ids + [PADDING] * (max_seq_len - len(input_ids))\n",
    "                attention_masks = self._get_attention_mask(input_ids, max_seq_len)\n",
    "                token_type_ids = self._get_token_type_id(input_ids, max_seq_len)\n",
    "                example = {\n",
    "                    \"input_ids\": tokens_input, \"attention_masks\": attention_masks, \"token_type_ids\": token_type_ids,\n",
    "                    \"seq_lens\": len(input_ids)\n",
    "                }\n",
    "                example.update(d_json)\n",
    "                if 'labels' in d_json:\n",
    "                    labels = d_json['labels']\n",
    "                    labels = labels[:(max_seq_len - 2)]\n",
    "                    encoded_label = [\"O\"] + labels + [\"O\"]\n",
    "                    encoded_label = [self.label_vocab[x] for x in encoded_label] + [ignore_index] * (max_seq_len - 2 - len(labels))\n",
    "                    example.update({\"encoded_label\": encoded_label})\n",
    "                self.examples.append(example)\n",
    "\n",
    "    def _get_attention_mask(self, input_ids, max_seq_len):\n",
    "        \"\"\"Mask for padding.\"\"\"\n",
    "        if len(input_ids) > max_seq_len:\n",
    "            raise IndexError(\"Token length more than max seq length!\")\n",
    "        return [1] * len(input_ids) + [0] * (max_seq_len - len(input_ids))\n",
    "\n",
    "    def _get_token_type_id(self, input_ids, max_seq_len):\n",
    "        \"\"\"Segments: 0 for the first sequence, 1 for the second.\"\"\"\n",
    "        if len(input_ids) > max_seq_len:\n",
    "            raise IndexError(\"Token length more than max seq length!\")\n",
    "        segments = []\n",
    "        current_segment_id = 0\n",
    "        for input_id in input_ids:\n",
    "            segments.append(current_segment_id)\n",
    "            if input_id == SEP:\n",
    "                current_segment_id = 1\n",
    "        return segments + [0] * (max_seq_len - len(input_ids))        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, item_idx):\n",
    "        example = {\n",
    "            \"id\": self.examples[item_idx][\"id\"],\n",
    "            \"sent_id\": self.examples[item_idx][\"sent_id\"],\n",
    "            \"text\": self.examples[item_idx][\"text\"],\n",
    "            \"input_ids\": torch.tensor(self.examples[item_idx][\"input_ids\"]).long(),\n",
    "            \"attention_masks\": torch.tensor(self.examples[item_idx][\"attention_masks\"]),\n",
    "            \"token_type_ids\": torch.tensor(self.examples[item_idx][\"token_type_ids\"]),\n",
    "            \"seq_lens\": self.examples[item_idx][\"seq_lens\"]\n",
    "        }\n",
    "        if \"encoded_label\" in self.examples[item_idx]:\n",
    "            example.update({\"encoded_label\": torch.tensor(self.examples[item_idx][\"encoded_label\"], dtype=torch.long)})\n",
    "        return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd9bf5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_enerm_dataset = BaiduEnermDataset(dataset_path=duee_fin_dev_preprocess_path, label_dict_path=enerm_dict_path)\n",
    "dev_trigger_dataset = BaiduEventDataset(dataset_path=duee_fin_dev_preprocess_path, label_dict_path=trigger_dict_path, model=\"trigger\")\n",
    "dev_role_dataset = BaiduEventDataset(dataset_path=duee_fin_dev_preprocess_path, label_dict_path=role_dict_path, model=\"role\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3e91ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4647"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_enerm_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45593b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4647"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_trigger_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "567b9a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4647"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_role_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0599b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed = 42):\n",
    "    \"\"\"Set the seed for generating random numbers on all GPUs.\n",
    "\n",
    "    It's safe to call this function if CUDA is not available; in that case, it is silently ignored.\n",
    "\n",
    "    Args:\n",
    "        seed (int, optional): random numbers on all GPUs. Defaults to 42.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f796fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n",
      "\n",
      "Tesla V100-PCIE-32GB\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "CUDA Device Count: 3\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    \n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', torch.cuda.memory_allocated(0)/1024**3, 'GB')\n",
    "    print('Cached:   ', torch.cuda.memory_reserved(0)/1024**3, 'GB')\n",
    "    \n",
    "    print('CUDA Device Count:', n_gpu)\n",
    "    \n",
    "set_seed(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32467dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_enerm(model, test_dataloader):\n",
    "    from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
    "    \n",
    "    model.eval()\n",
    "    step = 0\n",
    "    eval_acc = 0.0\n",
    "    eval_f1 = 0.0\n",
    "    eval_precision = 0.0\n",
    "    eval_recall = 0.0\n",
    "    results = []\n",
    "    test_iterator = tqdm(test_dataloader)\n",
    "    for batch in test_iterator:\n",
    "        outputs = model(\n",
    "            input_ids=batch['input_ids'].to(device),\n",
    "            attention_mask=batch['attention_masks'].to(device),\n",
    "            token_type_ids=batch['token_type_ids'].to(device)\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "\n",
    "        probs = F.softmax(logits, dim=1).cpu()\n",
    "        probs_ids = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "        true_label = batch.get(\"encoded_label\", None).cpu().numpy()\n",
    "        pred_label = copy.deepcopy(probs_ids)\n",
    "        ignore_indices = np.argwhere(true_label == -1)\n",
    "        pred_label[ignore_indices] = -1\n",
    "        probs = probs.numpy()\n",
    "        eval_acc += accuracy_score(true_label.flatten(), pred_label.flatten())\n",
    "        eval_precision += precision_score(true_label.flatten(), pred_label.flatten(), average=\"macro\", zero_division=1)\n",
    "        eval_recall += recall_score(true_label.flatten(), pred_label.flatten(), average=\"macro\", zero_division=1)\n",
    "        eval_f1 += f1_score(true_label.flatten(), pred_label.flatten(), average=\"macro\")\n",
    "        for id_, sent_id, text, prob_one, p_id in zip(batch['id'], batch['sent_id'], batch['text'], probs.tolist(), probs_ids.tolist()):\n",
    "            label_probs = {}\n",
    "            for idx, p in enumerate(prob_one):\n",
    "                label_probs[id2enumlabel[idx]] = p\n",
    "            results.append({\"id\": id_, \"sent_id\": sent_id, \"text\": text, \"pred\":{\"probs\": label_probs, \"label\": id2enumlabel[p_id]}})\n",
    "        step += 1\n",
    "    print({\"Avg eval acc\": f\"{eval_acc/step:.2f}\", \"Avg eval precision\": f\"{eval_precision/step:.2f}\", \"Avg eval recall\": f\"{eval_recall/step:.2f}\", \"Avg eval f1\": f\"{eval_f1/step:.2f}\"})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81aa096e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d1a6a639e084721a23e92708a6fc50e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'Avg eval acc': '1.00', 'Avg eval precision': '0.91', 'Avg eval recall': '0.87', 'Avg eval f1': '0.83'}\n"
     ]
    }
   ],
   "source": [
    "enum_model = torch.load(enerm_model_path).to(device)\n",
    "\n",
    "test_enerm_sampler = SequentialSampler(dev_enerm_dataset)\n",
    "test_enerm_dataloader = DataLoader(dev_enerm_dataset, sampler=test_enerm_sampler, batch_size = 512)\n",
    "    \n",
    "sentences_enum_data = test_enerm(enum_model, test_enerm_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "268c4b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_trigger(model, test_dataloader):\n",
    "    from seqeval.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
    "\n",
    "    model.eval()\n",
    "    step = 0\n",
    "    eval_acc = 0.0\n",
    "    eval_f1 = 0.0\n",
    "    eval_precision = 0.0\n",
    "    eval_recall = 0.0\n",
    "    results = []\n",
    "    test_iterator = tqdm(test_dataloader)\n",
    "    for batch in test_iterator:\n",
    "        outputs = model(\n",
    "            input_ids=batch['input_ids'].to(device),\n",
    "            attention_mask=batch['attention_masks'].to(device),\n",
    "            token_type_ids=batch['token_type_ids'].to(device)\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "\n",
    "        probs = F.softmax(logits, dim=1).cpu()\n",
    "        probs_ids = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "        probs = probs.numpy()\n",
    "        pred_Y, true_Y = [], []\n",
    "        for t_list, p_list, seq_len in zip(batch['encoded_label'].cpu().tolist(), probs_ids.tolist(), batch['seq_lens']):\n",
    "            if not all([id2triggerlabel.get(tid, \"O\") == \"O\" for tid in t_list[1: seq_len - 1]]):\n",
    "                pred_Y.append([id2triggerlabel.get(pid, \"O\") for pid in p_list[1: seq_len - 1]])\n",
    "                true_Y.append([id2triggerlabel.get(tid, \"O\") for tid in t_list[1: seq_len - 1]])\n",
    "        eval_acc += accuracy_score(pred_Y, true_Y)\n",
    "        eval_precision += precision_score(pred_Y, true_Y, zero_division=1)\n",
    "        eval_recall += recall_score(pred_Y, true_Y, zero_division=1)\n",
    "        eval_f1 += f1_score(pred_Y, true_Y)\n",
    "        for id_, sent_id, text, p_list, p_ids, seq_len in zip(batch['id'], batch['sent_id'], batch['text'], probs.tolist(), probs_ids.tolist(), batch['seq_lens']):\n",
    "            prob_one = [p_list[index][pid] for index, pid in enumerate(p_ids[1: seq_len - 1])]\n",
    "            label_one = [id2triggerlabel[pid] for pid in p_ids[1: seq_len - 1]]\n",
    "            results.append({\"id\": id_, \"sent_id\":sent_id, \"text\": text, \"pred\": {\"probs\": prob_one, \"labels\": label_one}})\n",
    "        step += 1\n",
    "    print({\"Avg eval acc\": f\"{eval_acc/step:.2f}\", \"Avg eval precision\": f\"{eval_precision/step:.2f}\", \"Avg eval recall\": f\"{eval_recall/step:.2f}\", \"Avg eval f1\": f\"{eval_f1/step:.2f}\"})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a60a5b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11c55e25cc684fe2aef59d5950d7fb65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'Avg eval acc': '1.00', 'Avg eval precision': '0.89', 'Avg eval recall': '0.86', 'Avg eval f1': '0.87'}\n"
     ]
    }
   ],
   "source": [
    "tigger_model = torch.load(tigger_model_path).to(device)\n",
    "\n",
    "test_trigger_sampler = SequentialSampler(dev_trigger_dataset)\n",
    "test_trigger_dataloader = DataLoader(dev_trigger_dataset, sampler=test_trigger_sampler, batch_size = 512)\n",
    "    \n",
    "sentences_tigger_data = test_trigger(tigger_model, test_trigger_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4700f5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_role(model, test_dataloader):\n",
    "    from seqeval.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
    "\n",
    "    model.eval()\n",
    "    step = 0\n",
    "    eval_acc = 0.0\n",
    "    eval_f1 = 0.0\n",
    "    eval_precision = 0.0\n",
    "    eval_recall = 0.0\n",
    "    results = []\n",
    "    test_iterator = tqdm(test_dataloader)\n",
    "    for batch in test_iterator:\n",
    "        outputs = model(\n",
    "            input_ids=batch['input_ids'].to(device),\n",
    "            attention_mask=batch['attention_masks'].to(device),\n",
    "            token_type_ids=batch['token_type_ids'].to(device)\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "\n",
    "        probs = F.softmax(logits, dim=1).cpu()\n",
    "        probs_ids = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "        probs = probs.numpy()\n",
    "        pred_Y, true_Y = [], []\n",
    "        for t_list, p_list, seq_len in zip(batch['encoded_label'].cpu().tolist(), probs_ids.tolist(), batch['seq_lens']):\n",
    "            if not all([id2rolelabel.get(tid, \"O\") == \"O\" for tid in t_list[1: seq_len - 1]]):\n",
    "                pred_Y.append([id2rolelabel.get(pid, \"O\") for pid in p_list[1: seq_len - 1]])\n",
    "                true_Y.append([id2rolelabel.get(tid, \"O\") for tid in t_list[1: seq_len - 1]])\n",
    "        eval_acc += accuracy_score(pred_Y, true_Y)\n",
    "        eval_precision += precision_score(pred_Y, true_Y, zero_division=1)\n",
    "        eval_recall += recall_score(pred_Y, true_Y, zero_division=1)\n",
    "        eval_f1 += f1_score(pred_Y, true_Y)\n",
    "        for id_, sent_id, text, p_list, p_ids, seq_len in zip(batch['id'], batch['sent_id'], batch['text'], probs.tolist(), probs_ids.tolist(), batch['seq_lens']):\n",
    "            prob_one = [p_list[index][pid] for index, pid in enumerate(p_ids[1: seq_len - 1])]\n",
    "            label_one = [id2rolelabel[pid] for pid in p_ids[1: seq_len - 1]]\n",
    "            results.append({\"id\": id_, \"sent_id\": sent_id, \"text\": text, \"pred\":{\"probs\": prob_one, \"labels\": label_one}})\n",
    "        step += 1\n",
    "    print({\"Avg eval acc\": f\"{eval_acc/step:.2f}\", \"Avg eval precision\": f\"{eval_precision/step:.2f}\", \"Avg eval recall\": f\"{eval_recall/step:.2f}\", \"Avg eval f1\": f\"{eval_f1/step:.2f}\"})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4e3e3e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daec42b0d831492f9ce9c6df8ea81efc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'Avg eval acc': '0.92', 'Avg eval precision': '0.65', 'Avg eval recall': '0.72', 'Avg eval f1': '0.68'}\n"
     ]
    }
   ],
   "source": [
    "role_model = torch.load(role_model_path).to(device)\n",
    "\n",
    "test_role_sampler = SequentialSampler(dev_role_dataset)\n",
    "test_role_dataloader = DataLoader(dev_role_dataset, sampler=test_role_sampler, batch_size = 512)\n",
    "    \n",
    "sentences_role_data = test_role(role_model, test_role_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "472ac616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_by_lines, extract_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea3a6ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_normalization(doc):\n",
    "    \"\"\"event_merge\"\"\"\n",
    "    for event in doc.get(\"event_list\", []):\n",
    "        argument_list = []\n",
    "        argument_set = set()\n",
    "        for arg in event[\"arguments\"]:\n",
    "            arg_str = \"{}-{}\".format(arg[\"role\"], arg[\"argument\"])\n",
    "            if arg_str not in argument_set:\n",
    "                argument_list.append(arg)\n",
    "            argument_set.add(arg_str)\n",
    "        event[\"arguments\"] = argument_list\n",
    "\n",
    "    event_list = sorted(\n",
    "        doc.get(\"event_list\", []),\n",
    "        key=lambda x: len(x[\"arguments\"]),\n",
    "        reverse=True)\n",
    "    new_event_list = []\n",
    "    for event in event_list:\n",
    "        event_type = event[\"event_type\"]\n",
    "        event_argument_set = set()\n",
    "        for arg in event[\"arguments\"]:\n",
    "            event_argument_set.add(\"{}-{}\".format(arg[\"role\"], arg[\"argument\"]))\n",
    "        flag = True\n",
    "        for new_event in new_event_list:\n",
    "            if event_type != new_event[\"event_type\"]:\n",
    "                continue\n",
    "            new_event_argument_set = set()\n",
    "            for arg in new_event[\"arguments\"]:\n",
    "                new_event_argument_set.add(\"{}-{}\".format(arg[\"role\"], arg[\n",
    "                    \"argument\"]))\n",
    "            if len(event_argument_set & new_event_argument_set) == len(\n",
    "                    new_event_argument_set):\n",
    "                flag = False\n",
    "        if flag:\n",
    "            new_event_list.append(event)\n",
    "    doc[\"event_list\"] = new_event_list\n",
    "    return doc\n",
    "\n",
    "def predict_data_process(trigger_data, role_data, enum_data, schema_file):\n",
    "    \"\"\"predict_data_process\"\"\"\n",
    "    pred_ret = []\n",
    "    schema_data = read_by_lines(schema_file)\n",
    "    print(\"trigger predict {} load.\".format(len(trigger_data)))\n",
    "    print(\"role predict {} load\".format(len(role_data)))\n",
    "    print(\"enum predict {} load\".format(len(enum_data)))\n",
    "    print(\"schema {} load from {}\".format(len(schema_data), schema_file))\n",
    "\n",
    "    schema, sent_role_mapping, sent_enum_mapping = {}, {}, {}\n",
    "    for s in schema_data:\n",
    "        d_json = json.loads(s)\n",
    "        schema[d_json[\"event_type\"]] = [r[\"role\"] for r in d_json[\"role_list\"]]\n",
    "\n",
    "    # role depends on id and sent_id \n",
    "    for d_json in role_data:\n",
    "        r_ret = extract_result(d_json[\"text\"], d_json[\"pred\"][\"labels\"])\n",
    "        role_ret = {}\n",
    "        for r in r_ret:\n",
    "            role_type = r[\"type\"]\n",
    "            if role_type not in role_ret:\n",
    "                role_ret[role_type] = []\n",
    "            role_ret[role_type].append(\"\".join(r[\"text\"]))\n",
    "        _id = \"{}\\t{}\".format(d_json[\"id\"], d_json[\"sent_id\"])\n",
    "        sent_role_mapping[_id] = role_ret\n",
    "\n",
    "    # process the enum_role data\n",
    "    for d_json in enum_data:\n",
    "        _id = \"{}\\t{}\".format(d_json[\"id\"], d_json[\"sent_id\"])\n",
    "        label = d_json[\"pred\"][\"label\"]\n",
    "        sent_enum_mapping[_id] = label\n",
    "\n",
    "    # process trigger data\n",
    "    for d_json in trigger_data:\n",
    "        t_ret = extract_result(d_json[\"text\"], d_json[\"pred\"][\"labels\"])\n",
    "        pred_event_types = list(set([t[\"type\"] for t in t_ret]))\n",
    "        event_list = []\n",
    "        _id = \"{}\\t{}\".format(d_json[\"id\"], d_json[\"sent_id\"])\n",
    "        for event_type in pred_event_types:\n",
    "            role_list = schema[event_type]\n",
    "            arguments = []\n",
    "            for role_type, ags in sent_role_mapping[_id].items():\n",
    "                if role_type not in role_list:\n",
    "                    continue\n",
    "                for arg in ags:\n",
    "                    arguments.append({\"role\": role_type, \"argument\": arg})\n",
    "            # 特殊处理环节\n",
    "            if event_type == enum_event_type:\n",
    "                arguments.append({\n",
    "                    \"role\": enum_role,\n",
    "                    \"argument\": sent_enum_mapping[_id]\n",
    "                })\n",
    "            event = {\n",
    "                \"event_type\": event_type,\n",
    "                \"arguments\": arguments,\n",
    "                \"text\": d_json[\"text\"],\n",
    "                \"label\": d_json[\"pred\"][\"labels\"]\n",
    "            }\n",
    "            event_list.append(event)\n",
    "        pred_ret.append({\n",
    "            \"id\": d_json[\"id\"],\n",
    "            \"sent_id\": d_json[\"sent_id\"],\n",
    "            \"text\": d_json[\"text\"],\n",
    "            \"event_list\": event_list\n",
    "        })\n",
    "    doc_pred = {}\n",
    "    for d in pred_ret:\n",
    "        if d[\"id\"] not in doc_pred:\n",
    "            doc_pred[d[\"id\"]] = {\"id\": d[\"id\"], \"event_list\": []}\n",
    "        doc_pred[d[\"id\"]][\"event_list\"].extend(d[\"event_list\"])\n",
    "\n",
    "    # unfiy the all prediction results and save them\n",
    "    doc_pred = [\n",
    "        event_normalization(r)\n",
    "        for r in doc_pred.values()\n",
    "    ]\n",
    "    print(\"submit data {} save\".format(len(doc_pred)))\n",
    "    return doc_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09818e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trigger predict 4647 load.\n",
      "role predict 4647 load\n",
      "enum predict 4647 load\n",
      "schema 13 load from ./dictionary/event_schema.json\n",
      "submit data 1023 save\n"
     ]
    }
   ],
   "source": [
    "doc_pred = predict_data_process(sentences_tigger_data, sentences_role_data, sentences_enum_data, shema_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d38e5f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mergedata(predict_doc, true_merge_dataset_path):\n",
    "    true_data_list = []\n",
    "    with open(true_merge_dataset_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            json_data = json.loads(line)\n",
    "            true_data_list.append(json_data)\n",
    "    predict_mapping_dict = {}\n",
    "    for doc in predict_doc:\n",
    "        predict_mapping_dict[doc['id']] = doc\n",
    "    count_predict = 0\n",
    "    count_true = 0\n",
    "    count_correct = 0\n",
    "    for true_data in true_data_list:\n",
    "        if true_data['id'] not in predict_mapping_dict:\n",
    "            if 'event_list' in true_data:\n",
    "                print('error: ', true_data)\n",
    "        else:\n",
    "            predict_doc = predict_mapping_dict[true_data['id']]\n",
    "            for pred_event in predict_doc.get('event_list', []):\n",
    "                pred_event_type = pred_event['event_type']\n",
    "                count_predict += len(pred_event.get('arguments', []))\n",
    "            for true_event in true_data.get('event_list', []):\n",
    "                true_event_type = true_event['event_type']\n",
    "                count_true += len(true_event.get('arguments', []))\n",
    "                predict_arguments = []\n",
    "                for pred_event in predict_doc.get('event_list', []):\n",
    "                    pred_event_type = pred_event['event_type']\n",
    "                    if true_event_type == pred_event_type:\n",
    "                        for pre_arg in pred_event.get('arguments', []):\n",
    "                            if pre_arg not in predict_arguments:\n",
    "                                predict_arguments.append(pre_arg)\n",
    "                for true_argument in true_event.get('arguments', []):\n",
    "                    for predict_argument in predict_arguments:\n",
    "                        if predict_argument['role'] == true_argument['role'] and predict_argument['argument'] == true_argument['argument']:\n",
    "                            count_correct += 1\n",
    "    p = count_correct / max(1, count_predict)  # precision\n",
    "    r = count_correct / max(1, count_true)  # recall\n",
    "    f1 = 2 * r * p / max(1e-9, r + p) # f1 score\n",
    "    s = count_true  # support\n",
    "\n",
    "    print(\"{:>10}{:>10}{:>10}{:>10}\\n\".format(\"precision\", \"recall\", \"f1-score\", \"support\"))\n",
    "    formatter = \"{:>10.3f}{:>10.3f}{:>10.3f}{:>10d}\".format\n",
    "    print(formatter(p, r, f1, s))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29fb40e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " precision    recall  f1-score   support\n",
      "\n",
      "     0.463     0.668     0.547      7946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_mergedata(doc_pred, duee_fin_dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60d567c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
